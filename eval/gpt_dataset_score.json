[
    {
        "user_input":"What is the core idea of Janus?",
        "retrieved_contexts":[
            " Auto-RegressiveTransformer\nUnd.Encoder\nTextTokenizer\nGen.Encoder\nTextDe-Tokenizer\nTextTokenizer\nImageDecoder\nImage:X!LanguageInstruct:X\"LanguageInstruct:X\"Image:X!LanguageResponse:X#GeneratedImage:X!UnderstandingImageGeneration\nâ€¦â€¦\nâ€¦â€¦Figure 2|Architecture of our Janus. Different from previous approaches [ 77,85] that typically\nassume visual understanding and generation require the same visual encoder, our Janus de-\ncouples visual encoding for visual understanding and visual generation. â€œUnd. Encoderâ€ and\nâ€œGen. Encoderâ€ are abbreviations for â€œUnderstanding Encoderâ€ and â€œGeneration Encoderâ€,\nrespectively. Best viewed in color.\nand generation tasks, regardless of whether they are based on autoregressive (AR) models\n[77,85] or diffusion models [ 86,94]. For example, Chameleon [ 77] adopts a VQ Tokenizer to\nencode images for both multimodal understanding and generation. However, this practice\nmay lead to suboptimal outcomes, as the vision encoder might face a trade-off between the\ndemands of understanding and generation. In contrast, our Janus can explicitly decouple the\nvisual representations for understanding and generation, recognizing that different tasks may\nrequire varying levels of information.\n3. Janus: A Simple, Unified and Flexible Multimodal Framework\n3.1. Architecture\nThe architecture of Janus is shown in Figure 2. For pure text understanding, multimodal\nunderstanding, and visual generation, we apply independent encoding methods to convert the ",
            " The Creation Myth: Janus is often credited with the creation of the world, embodying the primal forces that govern existence. Interactions with Other Deities: Janus frequently interacts with gods like Jupiter and Mars, showcasing his central role in the pantheon. Ancient Texts: Poets and writers, such as Ovid, have explored Janus' mythology, emphasizing his importance in Roman thought. ",
            " Janus: Decoupling Visual Encoding for Unified\nMultimodal Understanding and Generation\nChengyue Wu1,2Xiaokang Chen1,âˆ—,â€ Zhiyu Wu1,3Yiyang Ma1,3Xingchao Liu1Zizheng Pan1\nWen Liu1Zhenda Xie1Xingkai Yu1Chong Ruan1Ping Luo2,âˆ—\n1DeepSeek-AI2The University of Hong Kong3Peking University\nâ€ : Project leadâˆ—: Corresponding authors\nProject Page: https:\/\/github.com\/deepseek-ai\/Janus\nAbstract\nIn this paper, we introduce Janus , an autoregressive framework that unifies multimodal under-\nstanding and generation. Prior research often relies on a single visual encoder for both tasks,\nsuch as Chameleon. However, due to the differing levels of information granularity required by\nmultimodal understanding and generation, this approach can lead to suboptimal performance,\nparticularly in multimodal understanding. To address this issue, we decouple visual encoding\ninto separate pathways, while still leveraging a single, unified transformer architecture for\nprocessing. The decoupling not only alleviates the conflict between the visual encoderâ€™s roles in\nunderstanding and generation, but also enhances the frameworkâ€™s flexibility. For instance, both\nthe multimodal understanding and generation components can independently select their most\nsuitable encoding methods. Experiments show that Janus surpasses previous unified model\nand matches or exceeds the performance of task-specific models. The simplicity, high flexibility,"
        ],
        "response":"The core idea of Janus is to decouple visual encoding into separate pathways for multimodal understanding and generation. This approach aims to alleviate the conflict arising from using a single visual encoder for both tasks and enhances the framework's flexibility. By doing so, Janus aims to improve performance in both multimodal understanding and generation tasks.",
        "reference":"The core idea of Janus is to decouple visual encoding for multimodal understanding and generation. This is achieved by introducing two independent visual encoding pathways, one for multimodal understanding and one for multimodal generation, unified by the same transformer architecture.",
        "faithfulness":0.8571428571,
        "answer_relevancy":0.7772068752,
        "context_precision":0.8333333333,
        "answer_correctness":0.5857208664
    },
    {
        "user_input":"How does Janus's architecture differ from previous unified multimodal models?",
        "retrieved_contexts":[
            " Auto-RegressiveTransformer\nUnd.Encoder\nTextTokenizer\nGen.Encoder\nTextDe-Tokenizer\nTextTokenizer\nImageDecoder\nImage:X!LanguageInstruct:X\"LanguageInstruct:X\"Image:X!LanguageResponse:X#GeneratedImage:X!UnderstandingImageGeneration\nâ€¦â€¦\nâ€¦â€¦Figure 2|Architecture of our Janus. Different from previous approaches [ 77,85] that typically\nassume visual understanding and generation require the same visual encoder, our Janus de-\ncouples visual encoding for visual understanding and visual generation. â€œUnd. Encoderâ€ and\nâ€œGen. Encoderâ€ are abbreviations for â€œUnderstanding Encoderâ€ and â€œGeneration Encoderâ€,\nrespectively. Best viewed in color.\nand generation tasks, regardless of whether they are based on autoregressive (AR) models\n[77,85] or diffusion models [ 86,94]. For example, Chameleon [ 77] adopts a VQ Tokenizer to\nencode images for both multimodal understanding and generation. However, this practice\nmay lead to suboptimal outcomes, as the vision encoder might face a trade-off between the\ndemands of understanding and generation. In contrast, our Janus can explicitly decouple the\nvisual representations for understanding and generation, recognizing that different tasks may\nrequire varying levels of information.\n3. Janus: A Simple, Unified and Flexible Multimodal Framework\n3.1. Architecture\nThe architecture of Janus is shown in Figure 2. For pure text understanding, multimodal\nunderstanding, and visual generation, we apply independent encoding methods to convert the ",
            " techniques specific to their domain. Moreover, it is possible for Janus to accommodate additional\ninput types in the future, such as point clouds, EEG signals, or audio data, where independent\nencoders can extract features and then use a unified transformer to process them.\nTo the best of our knowledge, we are the first to highlight the importance of decoupling\nvisual encoding within the unified multimodal understanding and generation framework.\nOur experimental results show that Janus surpasses existing unified models with comparable\nparameter sizes on both multimodal understanding and generation benchmarks, achieving\nstate-of-the-art results. Notably, Janus even outperforms some task-specific models which have\nsignificantly more parameters (Figure 1). Specifically, on multimodal understanding benchmarks\nMMBench [ 54], SEED-Bench [ 42], and POPE [ 48], Janus ( 1.3B) achieved scores of 69.4,63.7, and\n87.0, respectively, outperforming LLaVA-v 1.5(7B) [50] and Qwen-VL-Chat ( 7B) [3] . On visual\ngeneration benchmarks MSCOCO- 30K [11] and GenEval [ 30], Janus achieved an FID score of\n8.53 and an accuracy of 61%, surpassing text-to-image generative models such as DALL-E 2[66]\nand SDXL [ 62]. We believe that the strong performance, coupled with the high flexibility and\nextensibility of Janus, presents it as a strong candidate for next-generation unified multimodal\nmodels.\n2. Related Work\n2.1. Visual Generation ",
            " Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation. Janus is a novel autoregressive framework that unifies multimodal understanding and generation. It addresses the limitations of previous approaches by decoupling visual encoding into separate pathways, while still utilizing a single, unified transformer architecture for processing."
        ],
        "response":"Janus's architecture differs from previous unified multimodal models by decoupling visual encoding into separate pathways for understanding and generation. Previous models typically use the same visual encoder for both tasks, potentially leading to suboptimal performance. Janus, on the other hand, uses independent encoding methods which allows for more specialized and effective processing for each task.",
        "reference":"Previous models often use a single visual encoder for both understanding and generation tasks. In contrast, Janus decouples visual encoding. It uses the SigLIP encoder for multimodal understanding to extract high - dimensional semantic features and a VQ tokenizer for visual generation to convert images into discrete IDs, with separate adaptors for each task.",
        "faithfulness":0.8,
        "answer_relevancy":1.0,
        "context_precision":0.8333333333,
        "answer_correctness":0.6373804023
    },
    {
        "user_input":"What are the three training stages of Janus?",
        "retrieved_contexts":[
            " Three-Stage Training Procedure: The training process of Janus is divided into three stages: training adaptors and image head, unified pretraining, and supervised fine-tuning. ",
            " Gen. Encoder\nImageHead\nGen. AdaptorStageI:Training AdaptorsandImageHead\nLLM\nUnd.Adaptor\nğŸ”¥\nğŸ”¥\nğŸ”¥\nUnderstanding\nTextHead\nUnd. Encoder\nâ„\nâ„\nâ„ImageGenerationStageII: UnifiedPretrainingStageIII:SupervisedFine-tuning\nGen. Encoder\nImageHead\nGen. Adaptor\nLLM\nUnd.Adaptor\nğŸ”¥\nğŸ”¥\nğŸ”¥\nUnderstanding\nTextHead\nUnd. Encoder\nâ„\nâ„\nâ„ImageGeneration\nğŸ”¥\nğŸ”¥\nGen. Encoder\nImageHead\nGen. Adaptor\nLLM\nUnd.Adaptor\nğŸ”¥\nğŸ”¥\nğŸ”¥\nUnderstanding\nTextHead\nUnd. EncoderImageGeneration\nğŸ”¥\nğŸ”¥\nğŸ”¥\nâ„Figure 3|Our Janus adopts a three-stage training procedure. We use flame symbols\/snowflake\nsymbols in the diagram to indicate the module updates\/does not update its parameters.\nattention masks.\n3.2. Training Procedure\nThe training of Janus is divided into three stages, as illustrated in Figure 3. Details are provided\nin the below.\nStage I: Training Adaptors and Image Head. The main goal of this stage is to create a conceptual\nconnection between visual and linguistic elements within the embedding space, enabling the\nLLM to understand the entities shown in images and have preliminary visual generation\nability. We keep the visual encoders and the LLM frozen during this stage, allowing only the\ntrainable parameters within the understanding adaptor, generation adaptor and image head to\nbe updated.\nStage II: Unified Pretraining. In this stage, we perform unified pretraining with multimodal\ncorpus to enable Janus to learn both multimodal understanding and generation. We unfreeze the ",
            " corpus to enable Janus to learn both multimodal understanding and generation. We unfreeze the\nLLM and utilize all types of training data: pure text data, multimodal understanding data, and\nvisual generation data. Inspired by Pixart [ 9], we begin by conducting simple visual generation\ntraining using ImageNet- 1k to help the model grasp basic pixel dependencies. Subsequently, we\nenhance the modelâ€™s open-domain visual generation capability with general text-to-image data.\nStage III: Supervised Fine-tuning. During this stage, we fine-tune the pretrained model with\ninstruction tuning data to enhance its instruction-following and dialogue capabilities. We\nfine-tune all parameters except the generation encoder. We focus on supervising the answers\nwhile masking system and user prompts. To ensure Janusâ€™s proficiency in both multimodal\nunderstanding and generation, we donâ€™t fine-tune separate models for a certain task. Instead,\nwe use a blend of pure text dialogue data, multimodal understanding data and visual generation\ndata, ensuring versatility across various scenarios.\n3.3. Training Objective\nJanus is an autoregressive model, and we simply adopt the cross-entropy loss during training:\nL=âˆ’âˆ‘ï¸\nğ‘–=1logğ‘ƒğœƒ(ğ‘¥ğ‘–|ğ‘¥<ğ‘–) (1)\nHere,ğ‘ƒ(Â·|Â·) indicates the conditional probability modeled by the weights ğœƒof Janus. For\npure text understanding and multimodal understanding tasks, we compute the loss on the text\n5"
        ],
        "response":"The three training stages of Janus are:\n\n1. Training Adaptors and Image Head\n2. Unified Pretraining\n3. Supervised Fine-tuning",
        "reference":"The first stage is training adaptors and the image head, where only the understanding adaptor, generation adaptor, and image head are updated while keeping the visual encoders and LLM frozen. The second stage is unified pretraining with multimodal corpus, including using ImageNet - 1k for simple visual generation training first and then general text - to - image data. The third stage is supervised fine - tuning with instruction tuning data, fine - tuning all parameters except the generation encoder.",
        "faithfulness":1.0,
        "answer_relevancy":1.0,
        "context_precision":1.0,
        "answer_correctness":0.5144436266
    },
    {
        "user_input":"What is the training objective of Janus?",
        "retrieved_contexts":[
            " corpus to enable Janus to learn both multimodal understanding and generation. We unfreeze the\nLLM and utilize all types of training data: pure text data, multimodal understanding data, and\nvisual generation data. Inspired by Pixart [ 9], we begin by conducting simple visual generation\ntraining using ImageNet- 1k to help the model grasp basic pixel dependencies. Subsequently, we\nenhance the modelâ€™s open-domain visual generation capability with general text-to-image data.\nStage III: Supervised Fine-tuning. During this stage, we fine-tune the pretrained model with\ninstruction tuning data to enhance its instruction-following and dialogue capabilities. We\nfine-tune all parameters except the generation encoder. We focus on supervising the answers\nwhile masking system and user prompts. To ensure Janusâ€™s proficiency in both multimodal\nunderstanding and generation, we donâ€™t fine-tune separate models for a certain task. Instead,\nwe use a blend of pure text dialogue data, multimodal understanding data and visual generation\ndata, ensuring versatility across various scenarios.\n3.3. Training Objective\nJanus is an autoregressive model, and we simply adopt the cross-entropy loss during training:\nL=âˆ’âˆ‘ï¸\nğ‘–=1logğ‘ƒğœƒ(ğ‘¥ğ‘–|ğ‘¥<ğ‘–) (1)\nHere,ğ‘ƒ(Â·|Â·) indicates the conditional probability modeled by the weights ğœƒof Janus. For\npure text understanding and multimodal understanding tasks, we compute the loss on the text\n5 ",
            " with the training exercises, a series of experiments was run to determine the effectiveness of the Janus simulation for training students in the Officer Basic Course (OBC). Course instructors were interviewed to determine the training objectives expected to be met using the Janus simulation. These objectives were measured using both objective and ",
            " \"The goal of the Janus program is to enable dramatic improvements in unconstrained face recognitionâ€¦to develop novel representations to encode the shape, texture, and dynamics of a face for the purpose of ... â€¢ Objective accuracy metric: 0.85 TAR @ 0.001 FAR â€¢ Objective query time: sublinear Phase 3 - 36 months"
        ],
        "response":"The training objective of Janus is to use cross-entropy loss for its autoregressive model. This involves modeling the conditional probability of sequences during training. This approach helps Janus in tasks involving pure text and multimodal understanding.",
        "reference":"Janus is an autoregressive model and uses the cross - entropy loss during training. The loss is calculated as $\\mathcal{L}=-\\sum_{i = 1} \\log P_{\\theta}(x_{i}",
        "faithfulness":1.0,
        "answer_relevancy":0.944595899,
        "context_precision":0.9999999999,
        "answer_correctness":0.5591694464
    },
    {
        "user_input":"How does Janus perform inference for image generation?",
        "retrieved_contexts":[
            " Auto-RegressiveTransformer\nUnd.Encoder\nTextTokenizer\nGen.Encoder\nTextDe-Tokenizer\nTextTokenizer\nImageDecoder\nImage:X!LanguageInstruct:X\"LanguageInstruct:X\"Image:X!LanguageResponse:X#GeneratedImage:X!UnderstandingImageGeneration\nâ€¦â€¦\nâ€¦â€¦Figure 2|Architecture of our Janus. Different from previous approaches [ 77,85] that typically\nassume visual understanding and generation require the same visual encoder, our Janus de-\ncouples visual encoding for visual understanding and visual generation. â€œUnd. Encoderâ€ and\nâ€œGen. Encoderâ€ are abbreviations for â€œUnderstanding Encoderâ€ and â€œGeneration Encoderâ€,\nrespectively. Best viewed in color.\nand generation tasks, regardless of whether they are based on autoregressive (AR) models\n[77,85] or diffusion models [ 86,94]. For example, Chameleon [ 77] adopts a VQ Tokenizer to\nencode images for both multimodal understanding and generation. However, this practice\nmay lead to suboptimal outcomes, as the vision encoder might face a trade-off between the\ndemands of understanding and generation. In contrast, our Janus can explicitly decouple the\nvisual representations for understanding and generation, recognizing that different tasks may\nrequire varying levels of information.\n3. Janus: A Simple, Unified and Flexible Multimodal Framework\n3.1. Architecture\nThe architecture of Janus is shown in Figure 2. For pure text understanding, multimodal\nunderstanding, and visual generation, we apply independent encoding methods to convert the ",
            " any resolution, without performing positional embedding interpolation for ViTs. Tokens can be\nfurther compressed to save computational cost, for instance, using pixel shuffle operation [12].\nVisual Generation. (1) For visual generation, finer-grained encoders can be chosen in order\nto preserve more image details after encoding, such as MoVQGan [ 93]. (2) Loss functions\nspecifically designed for visual generation can be employed, such as diffusion loss [ 46]. (3) A\ncombination of AR (causal attention) and parallel (bidirectional attention) methods can be used\nin the visual generation process to reduce accumulated errors during visual generation [79].\nSupport for Additional Modalities. The straightforward architecture of Janus allows for easy\nintegration with additional encoders, accommodating various modalities such as 3D point\ncloud [ 53], tactile [ 88], and EEG [ 4]. This gives Janus the potential to become a more powerful\nmultimodal generalist model.\n4. Experiments\nIn this section, we present a series of comprehensive experiments designed to assess the perfor-\nmance of our method across a range of visual understanding and generation tasks. We begin by\ndetailing our experimental setup, which includes the model architecture, training datasets, and\nevaluation benchmarks. Next, we report the performance of Janus, followed by a comparison\nwith other state-of-the-art models on various benchmarks for multimodal understanding and ",
            " techniques specific to their domain. Moreover, it is possible for Janus to accommodate additional\ninput types in the future, such as point clouds, EEG signals, or audio data, where independent\nencoders can extract features and then use a unified transformer to process them.\nTo the best of our knowledge, we are the first to highlight the importance of decoupling\nvisual encoding within the unified multimodal understanding and generation framework.\nOur experimental results show that Janus surpasses existing unified models with comparable\nparameter sizes on both multimodal understanding and generation benchmarks, achieving\nstate-of-the-art results. Notably, Janus even outperforms some task-specific models which have\nsignificantly more parameters (Figure 1). Specifically, on multimodal understanding benchmarks\nMMBench [ 54], SEED-Bench [ 42], and POPE [ 48], Janus ( 1.3B) achieved scores of 69.4,63.7, and\n87.0, respectively, outperforming LLaVA-v 1.5(7B) [50] and Qwen-VL-Chat ( 7B) [3] . On visual\ngeneration benchmarks MSCOCO- 30K [11] and GenEval [ 30], Janus achieved an FID score of\n8.53 and an accuracy of 61%, surpassing text-to-image generative models such as DALL-E 2[66]\nand SDXL [ 62]. We believe that the strong performance, coupled with the high flexibility and\nextensibility of Janus, presents it as a strong candidate for next-generation unified multimodal\nmodels.\n2. Related Work\n2.1. Visual Generation"
        ],
        "response":"Janus performs image generation inference by employing a decoupled architecture for visual understanding and generation. It uses finer-grained encoders like MoVQGan to preserve image details, applies specific loss functions such as diffusion loss, and combines AR (causal attention) and parallel (bidirectional attention) methods to reduce errors during generation. This approach ensures that different tasks receive appropriate levels of information processing.",
        "reference":"For image generation, Janus utilizes classifier - free guidance (CFG). Specifically, for each token, the logitÂ Â is calculated asÂ , whereÂ Â is the conditional logit,Â Â is the unconditional logit, andÂ Â is the scale for the classifier - free guidance (defaultÂ Â for evaluation).",
        "faithfulness":1.0,
        "answer_relevancy":0.9948064116,
        "context_precision":0.0,
        "answer_correctness":0.1954347256
    },
    {
        "user_input":"What datasets are used in Janus's training?",
        "retrieved_contexts":[
            " corpus to enable Janus to learn both multimodal understanding and generation. We unfreeze the\nLLM and utilize all types of training data: pure text data, multimodal understanding data, and\nvisual generation data. Inspired by Pixart [ 9], we begin by conducting simple visual generation\ntraining using ImageNet- 1k to help the model grasp basic pixel dependencies. Subsequently, we\nenhance the modelâ€™s open-domain visual generation capability with general text-to-image data.\nStage III: Supervised Fine-tuning. During this stage, we fine-tune the pretrained model with\ninstruction tuning data to enhance its instruction-following and dialogue capabilities. We\nfine-tune all parameters except the generation encoder. We focus on supervising the answers\nwhile masking system and user prompts. To ensure Janusâ€™s proficiency in both multimodal\nunderstanding and generation, we donâ€™t fine-tune separate models for a certain task. Instead,\nwe use a blend of pure text dialogue data, multimodal understanding data and visual generation\ndata, ensuring versatility across various scenarios.\n3.3. Training Objective\nJanus is an autoregressive model, and we simply adopt the cross-entropy loss during training:\nL=âˆ’âˆ‘ï¸\nğ‘–=1logğ‘ƒğœƒ(ğ‘¥ğ‘–|ğ‘¥<ğ‘–) (1)\nHere,ğ‘ƒ(Â·|Â·) indicates the conditional probability modeled by the weights ğœƒof Janus. For\npure text understanding and multimodal understanding tasks, we compute the loss on the text\n5 ",
            " Table 1|Detailed hyperparameters of our Janus . Data ratio refers to the ratio of multimodal\nunderstanding data, pure text data, and visual generation data.\nHyperparameters Stage 1 Stage 2 Stage 3\nLearning rate 1.0 Ã—10âˆ’31Ã—10âˆ’42.0Ã—10âˆ’5\nLR scheduler Cosine Constant Constant\nWeight decay 0.0 0.0 0.1\nGradient clip 1.0 1.0 1.0\nOptimizer AdamW ( ğ›½1=0.9,ğ›½2=0.95)\nWarm-up steps 300 5, 000 0\nTraining steps 10, 000 180, 000 24, 000\nBatch size 256 512 256\nData Ratio 1 : 0 : 1 2 : 3 : 5 7 : 3 : 10\n4.1. Implementation Details\nIn our experiments, we utilize DeepSeek-LLM ( 1.3B) [5] with a maximum supported sequence\nlength of 4096 as the base language model. For the vision encoder used in understanding tasks,\nwe select SigLIP-Large-Patch 16-384[92]. The generation encoder has a codebook of size 16, 384\nand downsamples images by a factor of 16. Both the understanding adaptor and the generation\nadaptor are two-layer MLPs. The detailed hyperparameters for each stage are provided in\nTable 1. All images are resized to 384Ã—384pixels. For multimodal understanding data, we\nresize the long side of the image and pad the short side with the background color (RGB: 127,\n127,127) to reach 384. For visual generation data, the short side is resized to 384, and the long\nside is cropped to 384. We use sequence packing during training to improve training efficiency.\nWe mix all data types according to the specified ratios in a single training step. Our Janus is ",
            " This stage was extended in Janus-Pro, giving the model more time to model pixel dependencies and process visual data effectively. Connecting visuals and text: In this phase, Janus-Pro integrates text and visuals by training on high-quality, descriptive text-to-image datasets."
        ],
        "response":"Janus's training uses pure text data, multimodal understanding data, and visual generation data. Specifically, datasets include ImageNet-1k for visual generation and general text-to-image data for enhancing open-domain visual generation capabilities.",
        "reference":"In stage I, it uses 1.25 million image - text paired captions from ShareGPT4V for multimodal understanding and approximately 1.2 million samples from ImageNet - 1k for visual generation. In stage II, it uses text - only data from DeepSeek - LLM, interleaved image - text data from WikiHow and WIT, image caption data from multiple sources, table and chart data from DeepSeek - VL, and visual generation data from various datasets. In stage III, it uses data from different sources for text understanding, multimodal understanding, and visual generation, with specific formatting for instruction tuning.",
        "faithfulness":1.0,
        "answer_relevancy":0.9594810746,
        "context_precision":0.9999999999,
        "answer_correctness":0.4321634269
    },
    {
        "user_input":"How does Janus compare with previous unified models in terms of performance?",
        "retrieved_contexts":[
            " techniques specific to their domain. Moreover, it is possible for Janus to accommodate additional\ninput types in the future, such as point clouds, EEG signals, or audio data, where independent\nencoders can extract features and then use a unified transformer to process them.\nTo the best of our knowledge, we are the first to highlight the importance of decoupling\nvisual encoding within the unified multimodal understanding and generation framework.\nOur experimental results show that Janus surpasses existing unified models with comparable\nparameter sizes on both multimodal understanding and generation benchmarks, achieving\nstate-of-the-art results. Notably, Janus even outperforms some task-specific models which have\nsignificantly more parameters (Figure 1). Specifically, on multimodal understanding benchmarks\nMMBench [ 54], SEED-Bench [ 42], and POPE [ 48], Janus ( 1.3B) achieved scores of 69.4,63.7, and\n87.0, respectively, outperforming LLaVA-v 1.5(7B) [50] and Qwen-VL-Chat ( 7B) [3] . On visual\ngeneration benchmarks MSCOCO- 30K [11] and GenEval [ 30], Janus achieved an FID score of\n8.53 and an accuracy of 61%, surpassing text-to-image generative models such as DALL-E 2[66]\nand SDXL [ 62]. We believe that the strong performance, coupled with the high flexibility and\nextensibility of Janus, presents it as a strong candidate for next-generation unified multimodal\nmodels.\n2. Related Work\n2.1. Visual Generation ",
            " results among models of similar scale. Specifically, compared to the previous best unified model,\nShow-o [ 86], we achieve performance improvements of 41% (949â†’1338 ) and 30% (48.7â†’59.1)\non the MME and GQA datasets, respectively. This can be attributed to Janus decoupling the\nvisual encoding for multimodal understanding and generation, mitigating the conflict between\nthese two tasks. When compared to models with significantly larger sizes, Janus remains highly\ncompetitive. For instance, Janus outperforms LLaVA-v 1.5(7B) on several datasets, including\nPOPE, MMbench, SEED Bench, and MM-Vet.\nVisual Generation Performance. We report visual generation performance on GenEval, COCO-\n30K and MJHQ- 30K benchmarks. As shown in Table 3, our Janus obtains 61% overall accuracy\non GenEval, which outperforms the previous best unified model Show-o ( 53%) and some\npopular generation-only methods, e.g., SDXL ( 55%) and DALL-E 2(52%). This demonstrates\nthat our approach has better instruction-following capabilities. As shown in Table 4, Janus\nachieves FIDs of 8.53 and 10.10 on the COCO- 30K and MJHQ- 30K benchmarks, respectively,\nsurpassing unified models Show-o and LWM, and demonstrating competitive performance\ncompared to some well-known generation-only methods. This demonstrates that the images\ngenerated by Janus have good quality and highlights its potential in visual generation.\n4.5. Ablation Studies ",
            " Janus surpasses previous unified model and matches or exceeds the performance of task-specific models. The simplicity, high flexibility, and effectiveness of Janus make it a strong candidate for next-generation unified multimodal models. ... Please note that the use of this model is subject to the terms outlined in License section. Commercial ..."
        ],
        "response":"Janus surpasses previous unified models in performance, achieving state-of-the-art results on both multimodal understanding and generation benchmarks. It outperforms models with comparable parameter sizes and even some significantly larger task-specific models. Additionally, Janus demonstrates superior visual generation capabilities, surpassing previous top unified models like Show-o and popular generation-only methods such as DALL-E 2 and SDXL.",
        "reference":"Janus outperforms previous unified models on both multimodal understanding and generation benchmarks. On multimodal understanding benchmarks like MMBench, SEED - Bench, and POPE, Janus (1.3B) achieves higher scores than models such as LLaVA - v1.5 (7B) and Qwen - VL - Chat (7B). In visual generation, it has better results than some well - known models like DALL - E 2 and SDXL on benchmarks like MSCOCO - 30K and GenEval.",
        "faithfulness":1.0,
        "answer_relevancy":0.9481994744,
        "context_precision":1.0,
        "answer_correctness":0.7131446292
    },
    {
        "user_input":"What are the possible extensions of Janus in multimodal understanding?",
        "retrieved_contexts":[
            " In this work, we introduce Janus-Pro, an advanced version of the previous work Janus. Specifically, Janus-Pro incorporates (1) an optimized training strategy, (2) expanded training data, and (3) scaling to larger model size. With these improvements, Janus-Pro achieves significant advancements in both multimodal understanding and text-to-image instruction-following capabilities, while also ... ",
            " Janus: Decoupling Visual Encoding for Unified\nMultimodal Understanding and Generation\nChengyue Wu1,2Xiaokang Chen1,âˆ—,â€ Zhiyu Wu1,3Yiyang Ma1,3Xingchao Liu1Zizheng Pan1\nWen Liu1Zhenda Xie1Xingkai Yu1Chong Ruan1Ping Luo2,âˆ—\n1DeepSeek-AI2The University of Hong Kong3Peking University\nâ€ : Project leadâˆ—: Corresponding authors\nProject Page: https:\/\/github.com\/deepseek-ai\/Janus\nAbstract\nIn this paper, we introduce Janus , an autoregressive framework that unifies multimodal under-\nstanding and generation. Prior research often relies on a single visual encoder for both tasks,\nsuch as Chameleon. However, due to the differing levels of information granularity required by\nmultimodal understanding and generation, this approach can lead to suboptimal performance,\nparticularly in multimodal understanding. To address this issue, we decouple visual encoding\ninto separate pathways, while still leveraging a single, unified transformer architecture for\nprocessing. The decoupling not only alleviates the conflict between the visual encoderâ€™s roles in\nunderstanding and generation, but also enhances the frameworkâ€™s flexibility. For instance, both\nthe multimodal understanding and generation components can independently select their most\nsuitable encoding methods. Experiments show that Janus surpasses previous unified model\nand matches or exceeds the performance of task-specific models. The simplicity, high flexibility, ",
            " techniques specific to their domain. Moreover, it is possible for Janus to accommodate additional\ninput types in the future, such as point clouds, EEG signals, or audio data, where independent\nencoders can extract features and then use a unified transformer to process them.\nTo the best of our knowledge, we are the first to highlight the importance of decoupling\nvisual encoding within the unified multimodal understanding and generation framework.\nOur experimental results show that Janus surpasses existing unified models with comparable\nparameter sizes on both multimodal understanding and generation benchmarks, achieving\nstate-of-the-art results. Notably, Janus even outperforms some task-specific models which have\nsignificantly more parameters (Figure 1). Specifically, on multimodal understanding benchmarks\nMMBench [ 54], SEED-Bench [ 42], and POPE [ 48], Janus ( 1.3B) achieved scores of 69.4,63.7, and\n87.0, respectively, outperforming LLaVA-v 1.5(7B) [50] and Qwen-VL-Chat ( 7B) [3] . On visual\ngeneration benchmarks MSCOCO- 30K [11] and GenEval [ 30], Janus achieved an FID score of\n8.53 and an accuracy of 61%, surpassing text-to-image generative models such as DALL-E 2[66]\nand SDXL [ 62]. We believe that the strong performance, coupled with the high flexibility and\nextensibility of Janus, presents it as a strong candidate for next-generation unified multimodal\nmodels.\n2. Related Work\n2.1. Visual Generation"
        ],
        "response":"The possible extensions of Janus in multimodal understanding include accommodating additional input types such as point clouds, EEG signals, or audio data. Independent encoders can be used to extract features from these new modalities, then processed by a unified transformer. This flexibility allows Janus to potentially enhance its capabilities by integrating various types of data.",
        "reference":"In multimodal understanding, Janus can use stronger vision encoders like EVA - CLIP or InternViT. It can also use dynamic high - resolution techniques to handle high - resolution images and compress tokens using operations like pixel shuffle to save computational cost.",
        "faithfulness":0.875,
        "answer_relevancy":0.9964469973,
        "context_precision":0.0,
        "answer_correctness":0.2123522738
    },
    {
        "user_input":"What are the possible extensions of Janus in visual generation?",
        "retrieved_contexts":[
            " Auto-RegressiveTransformer\nUnd.Encoder\nTextTokenizer\nGen.Encoder\nTextDe-Tokenizer\nTextTokenizer\nImageDecoder\nImage:X!LanguageInstruct:X\"LanguageInstruct:X\"Image:X!LanguageResponse:X#GeneratedImage:X!UnderstandingImageGeneration\nâ€¦â€¦\nâ€¦â€¦Figure 2|Architecture of our Janus. Different from previous approaches [ 77,85] that typically\nassume visual understanding and generation require the same visual encoder, our Janus de-\ncouples visual encoding for visual understanding and visual generation. â€œUnd. Encoderâ€ and\nâ€œGen. Encoderâ€ are abbreviations for â€œUnderstanding Encoderâ€ and â€œGeneration Encoderâ€,\nrespectively. Best viewed in color.\nand generation tasks, regardless of whether they are based on autoregressive (AR) models\n[77,85] or diffusion models [ 86,94]. For example, Chameleon [ 77] adopts a VQ Tokenizer to\nencode images for both multimodal understanding and generation. However, this practice\nmay lead to suboptimal outcomes, as the vision encoder might face a trade-off between the\ndemands of understanding and generation. In contrast, our Janus can explicitly decouple the\nvisual representations for understanding and generation, recognizing that different tasks may\nrequire varying levels of information.\n3. Janus: A Simple, Unified and Flexible Multimodal Framework\n3.1. Architecture\nThe architecture of Janus is shown in Figure 2. For pure text understanding, multimodal\nunderstanding, and visual generation, we apply independent encoding methods to convert the ",
            " techniques specific to their domain. Moreover, it is possible for Janus to accommodate additional\ninput types in the future, such as point clouds, EEG signals, or audio data, where independent\nencoders can extract features and then use a unified transformer to process them.\nTo the best of our knowledge, we are the first to highlight the importance of decoupling\nvisual encoding within the unified multimodal understanding and generation framework.\nOur experimental results show that Janus surpasses existing unified models with comparable\nparameter sizes on both multimodal understanding and generation benchmarks, achieving\nstate-of-the-art results. Notably, Janus even outperforms some task-specific models which have\nsignificantly more parameters (Figure 1). Specifically, on multimodal understanding benchmarks\nMMBench [ 54], SEED-Bench [ 42], and POPE [ 48], Janus ( 1.3B) achieved scores of 69.4,63.7, and\n87.0, respectively, outperforming LLaVA-v 1.5(7B) [50] and Qwen-VL-Chat ( 7B) [3] . On visual\ngeneration benchmarks MSCOCO- 30K [11] and GenEval [ 30], Janus achieved an FID score of\n8.53 and an accuracy of 61%, surpassing text-to-image generative models such as DALL-E 2[66]\nand SDXL [ 62]. We believe that the strong performance, coupled with the high flexibility and\nextensibility of Janus, presents it as a strong candidate for next-generation unified multimodal\nmodels.\n2. Related Work\n2.1. Visual Generation ",
            " demands that understanding and generation place on the visual encoder. Extensive experi-\nments have demonstrated the effectiveness and leading performance of Janus. It is also worth\nnoting that Janus is flexible and easy to extend. In addition to having significant potential for\nimprovement in both multimodal understanding and generation, Janus is also easily extendable\nto incorporate more input modalities. The above advantages suggest that Janus may serve as an\ninspiration for the development of the next generation of multimodal general-purpose models.\n13"
        ],
        "response":"Janus can potentially be extended to accommodate additional input types such as point clouds, EEG signals, or audio data. This is facilitated by using independent encoders specific to each new modality. Its flexible architecture allows these extensions while maintaining strong multimodal understanding and generation performance.",
        "reference":"In visual generation, Janus can choose finer - grained encoders like MoVQGan to preserve more image details. It can employ loss functions specifically designed for visual generation, such as diffusion loss. Also, a combination of AR (causal attention) and parallel (bidirectional attention) methods can be used to reduce accumulated errors during visual generation.",
        "faithfulness":0.8181818182,
        "answer_relevancy":0.7973524528,
        "context_precision":0.0,
        "answer_correctness":0.1913056749
    },
    {
        "user_input":"Why is Janus considered a strong candidate for next - generation unified multimodal models?",
        "retrieved_contexts":[
            " demands that understanding and generation place on the visual encoder. Extensive experi-\nments have demonstrated the effectiveness and leading performance of Janus. It is also worth\nnoting that Janus is flexible and easy to extend. In addition to having significant potential for\nimprovement in both multimodal understanding and generation, Janus is also easily extendable\nto incorporate more input modalities. The above advantages suggest that Janus may serve as an\ninspiration for the development of the next generation of multimodal general-purpose models.\n13 ",
            " techniques specific to their domain. Moreover, it is possible for Janus to accommodate additional\ninput types in the future, such as point clouds, EEG signals, or audio data, where independent\nencoders can extract features and then use a unified transformer to process them.\nTo the best of our knowledge, we are the first to highlight the importance of decoupling\nvisual encoding within the unified multimodal understanding and generation framework.\nOur experimental results show that Janus surpasses existing unified models with comparable\nparameter sizes on both multimodal understanding and generation benchmarks, achieving\nstate-of-the-art results. Notably, Janus even outperforms some task-specific models which have\nsignificantly more parameters (Figure 1). Specifically, on multimodal understanding benchmarks\nMMBench [ 54], SEED-Bench [ 42], and POPE [ 48], Janus ( 1.3B) achieved scores of 69.4,63.7, and\n87.0, respectively, outperforming LLaVA-v 1.5(7B) [50] and Qwen-VL-Chat ( 7B) [3] . On visual\ngeneration benchmarks MSCOCO- 30K [11] and GenEval [ 30], Janus achieved an FID score of\n8.53 and an accuracy of 61%, surpassing text-to-image generative models such as DALL-E 2[66]\nand SDXL [ 62]. We believe that the strong performance, coupled with the high flexibility and\nextensibility of Janus, presents it as a strong candidate for next-generation unified multimodal\nmodels.\n2. Related Work\n2.1. Visual Generation ",
            " In this paper, we introduce Janus, an autoregressive framework that unifies multimodal understanding and generation. Prior research often relies on a single visual encoder for both tasks, such as Chameleon. However, due to the differing levels of information granularity required by multimodal understanding and generation, this approach can lead to suboptimal performance, particularly in ..."
        ],
        "response":"Janus is considered a strong candidate for next-generation unified multimodal models because it demonstrates leading performance in both multimodal understanding and generation, shows high flexibility and extensibility to incorporate additional input modalities, and outperforms existing models with similar or larger parameters on various benchmarks.",
        "reference":"Janus is considered a strong candidate because it has a simple, unified, and flexible architecture. It decouples visual encoding, which leads to high performance on both multimodal understanding and generation tasks, even outperforming some task - specific models. It is also easy to extend, with the potential to incorporate more input modalities, such as 3D point cloud, tactile, and EEG data.",
        "faithfulness":1.0,
        "answer_relevancy":1.0,
        "context_precision":1.0,
        "answer_correctness":0.6392928686
    },
    {
        "user_input":"What is the main innovation of Kimi k1.5?",
        "retrieved_contexts":[
            " KIMI K 1.5:\nSCALING REINFORCEMENT LEARNING WITH LLM S\nTECHNICAL REPORT OF KIMI K 1.5\nKimi Team\nABSTRACT\nLanguage model pretraining with next token prediction has proved effective for scaling compute but\nis limited to the amount of available training data. Scaling reinforcement learning (RL) unlocks a new\naxis for the continued improvement of artificial intelligence, with the promise that large language\nmodels (LLMs) can scale their training data by learning to explore with rewards. However, prior\npublished work has not produced competitive results. In light of this, we report on the training practice\nof Kimi k1.5, our latest multi-modal LLM trained with RL, including its RL training techniques,\nmulti-modal data recipes, and infrastructure optimization. Long context scaling and improved policy\noptimization methods are key ingredients of our approach, which establishes a simplistic, effective\nRL framework without relying on more complex techniques such as Monte Carlo tree search, value\nfunctions, and process reward models. Notably, our system achieves state-of-the-art reasoning\nperformance across multiple benchmarks and modalitiesâ€”e.g., 77.5 on AIME, 96.2 on MATH\n500, 94-th percentile on Codeforces, 74.9 on MathVistaâ€”matching OpenAIâ€™s o1. Moreover, we\npresent effective long2short methods that use long-CoT techniques to improve short-CoT models,\nyielding state-of-the-art short-CoT reasoning resultsâ€”e.g., 60.8 on AIME, 94.6 on MATH500, 47.3 ",
            " Kimi k1.5 TECHNICAL REPORT\n3.2 Main Results\nK1.5 long-CoT model The performance of the Kimi k1.5 long-CoT model is presented in Table 2. Through long-CoT\nsupervised fine-tuning (described in Section 2.2) and vision-text joint reinforcement learning (discussed in Section 2.3),\nthe modelâ€™s long-term reasoning capabilities are enhanced significantly. The test-time computation scaling further\nstrengthens its performance, enabling the model to achieve state-of-the-art results across a range of modalities. Our\nevaluation reveals marked improvements in the modelâ€™s capacity to reason, comprehend, and synthesize information\nover extended contexts, representing a advancement in multi-modal AI capabilities.\nK1.5 short-CoT model The performance of the Kimi k1.5 short-CoT model is presented in Table 3. This model\nintegrates several techniques, including traditional supervised fine-tuning (discussed in Section 2.5.2), reinforcement\nlearning (explored in Section 2.3), and long-to-short distillation (outlined in Section 2.4). The results demonstrate\nthat the k1.5 short-CoT model delivers competitive or superior performance compared to leading open-source and\nproprietary models across multiple tasks. These include text, vision, and reasoning challenges, with notable strengths in\nnatural language understanding, mathematics, coding, and logical reasoning.\nBenchmark (Metric)Language-only Model Vision-Language Model\nQwQ-32B OpenAI QVQ-72B OpenAI Kimi\nPreview o1-mini Preview o1 k1.5 ",
            " Kimi Team. Full Report. ğŸš€ Introducing Kimi k1.5 --- an o1-level multi-modal model. Sota short-CoT performance, outperforming GPT-4o and Claude Sonnet 3.5 on ğŸ“AIME, ğŸ“MATH-500, ğŸ’» LiveCodeBench by a large margin (up to +550%) Long-CoT performance matches o1 across multiple modalities (ğŸ‘€MathVista, ğŸ“AIME, ğŸ’»Codeforces, etc)"
        ],
        "response":"The main innovation of Kimi k1.5 is the use of a simplistic yet effective reinforcement learning (RL) framework for training a multi-modal large language model (LLM) without relying on complex techniques like Monte Carlo tree search and value functions. This approach enhances the model's long-term reasoning capabilities, enabling it to achieve state-of-the-art performance across various benchmarks and modalities.",
        "reference":"The main innovation is scaling reinforcement learning (RL) with LLMs by introducing long context scaling (scaling the RL context window to 128k) and improved policy optimization methods. It also presents long2short methods to improve short-CoT models, achieving state-of-the-art reasoning performance across multiple benchmarks and modalities without relying on complex techniques like Monte Carlo tree search.",
        "faithfulness":1.0,
        "answer_relevancy":1.0,
        "context_precision":0.9999999999,
        "answer_correctness":0.7320147774
    },
    {
        "user_input":"How does Kimi k1.5 perform in different benchmarks compared to other models?",
        "retrieved_contexts":[
            " KIMI K 1.5:\nSCALING REINFORCEMENT LEARNING WITH LLM S\nTECHNICAL REPORT OF KIMI K 1.5\nKimi Team\nABSTRACT\nLanguage model pretraining with next token prediction has proved effective for scaling compute but\nis limited to the amount of available training data. Scaling reinforcement learning (RL) unlocks a new\naxis for the continued improvement of artificial intelligence, with the promise that large language\nmodels (LLMs) can scale their training data by learning to explore with rewards. However, prior\npublished work has not produced competitive results. In light of this, we report on the training practice\nof Kimi k1.5, our latest multi-modal LLM trained with RL, including its RL training techniques,\nmulti-modal data recipes, and infrastructure optimization. Long context scaling and improved policy\noptimization methods are key ingredients of our approach, which establishes a simplistic, effective\nRL framework without relying on more complex techniques such as Monte Carlo tree search, value\nfunctions, and process reward models. Notably, our system achieves state-of-the-art reasoning\nperformance across multiple benchmarks and modalitiesâ€”e.g., 77.5 on AIME, 96.2 on MATH\n500, 94-th percentile on Codeforces, 74.9 on MathVistaâ€”matching OpenAIâ€™s o1. Moreover, we\npresent effective long2short methods that use long-CoT techniques to improve short-CoT models,\nyielding state-of-the-art short-CoT reasoning resultsâ€”e.g., 60.8 on AIME, 94.6 on MATH500, 47.3 ",
            " Kimi k1.5 TECHNICAL REPORT\n3.2 Main Results\nK1.5 long-CoT model The performance of the Kimi k1.5 long-CoT model is presented in Table 2. Through long-CoT\nsupervised fine-tuning (described in Section 2.2) and vision-text joint reinforcement learning (discussed in Section 2.3),\nthe modelâ€™s long-term reasoning capabilities are enhanced significantly. The test-time computation scaling further\nstrengthens its performance, enabling the model to achieve state-of-the-art results across a range of modalities. Our\nevaluation reveals marked improvements in the modelâ€™s capacity to reason, comprehend, and synthesize information\nover extended contexts, representing a advancement in multi-modal AI capabilities.\nK1.5 short-CoT model The performance of the Kimi k1.5 short-CoT model is presented in Table 3. This model\nintegrates several techniques, including traditional supervised fine-tuning (discussed in Section 2.5.2), reinforcement\nlearning (explored in Section 2.3), and long-to-short distillation (outlined in Section 2.4). The results demonstrate\nthat the k1.5 short-CoT model delivers competitive or superior performance compared to leading open-source and\nproprietary models across multiple tasks. These include text, vision, and reasoning challenges, with notable strengths in\nnatural language understanding, mathematics, coding, and logical reasoning.\nBenchmark (Metric)Language-only Model Vision-Language Model\nQwQ-32B OpenAI QVQ-72B OpenAI Kimi\nPreview o1-mini Preview o1 k1.5 ",
            " Kimi k1.5 TECHNICAL REPORT\nKimi k1.5 short-CoTOpenAI 4oClaude 3.5 Sonnet DeepSeek V3LLaMA-3.1 405B-Inst.\nGeneralCodeMath\n47.333.4\nLiveCodeBench v4 24.08-24.11 (Pass@1-COT)\n36.328.440.531.1Qwen2.5 72B-Inst.\n87.487.2\nMMLU (EM)\n88.388.688.585.3\n94.674.6\nMATH-500 (EM)\n78.373.890.280\n60.8\n9.3AIME 2024 (Pass@1)\n1623.339.223.3\n6869.1\nMMMU_val(Pass@1)\n66.464.5\n70.163.8\nMathVista_test(Pass@1)\n65.369.7\n87.284.3\nIF-Eval (Prompt Strict)\n86.58686.184.1\n91.787.9\nCLUEWSC (EM)\n85.484.790.991.4\n88.376\nC-Eval (EM)\n76.761.586.586.1Qwen2-VLVision\nFigure 2: Kimi k1.5 short-CoT results.\n1 Introduction\nLanguage model pretraining with next token prediction has been studied under the context of the scaling law, where\nproportionally scaling model parameters and data sizes leads to the continued improvement of intelligence. (Kaplan\net al. 2020; Hoffmann et al. 2022) However, this approach is limited to the amount of available high-quality training\ndata (Villalobos et al. 2024; Muennighoff et al. 2023). In this report, we present the training recipe of Kimi k1.5,\nour latest multi-modal LLM trained with reinforcement learning (RL). The goal is to explore a possible new axis\nfor continued scaling. Using RL with LLMs, the models learns to explore with rewards and thus is not limited to a\npre-existing static dataset.\nThere are a few key ingredients about the design and training of k1.5.\nâ€¢Long context scaling . We scale the context window of RL to 128k and observe continued improvement of"
        ],
        "response":"Kimi k1.5 performs exceptionally well across different benchmarks, achieving state-of-the-art results. It scores 77.5 on AIME, 96.2 on MATH 500, 94-th percentile on Codeforces, and 74.9 on MathVista, among others. These results show that Kimi k1.5 is competitive or superior compared to other leading models in multiple modalities and tasks.",
        "reference":"In long-CoT, it achieves 77.5 on AIME, 96.2 on MATH 500, 94-th percentile on Codeforces, 74.9 on MathVista, matching or outperforming models like OpenAIâ€™s o1. In short-CoT, it gets 60.8 on AIME, 94.6 on MATH500, 47.3 on LiveCodeBench, outperforming models such as GPT-4o and Claude Sonnet 3.5 by a large margin (up to +550%).",
        "faithfulness":0.9,
        "answer_relevancy":0.9804817048,
        "context_precision":1.0,
        "answer_correctness":0.5497443071
    },
    {
        "user_input":"What are the key ingredients of Kimi k1.5's design and training?",
        "retrieved_contexts":[
            " Key Ingredients of Kimi k1.5. Kimi k1.5's design and training revolve around several key ingredients: Long Context Scaling: The researchers scaled Kimi k1.5's context window for RL to 128k and found that performance continued to improve as the context length increased. To make training with such a large context window more efficient, they ... ",
            " Key Ingredients of Kimi k1.5. There are a few key ingredients about the design and training of k1.5. Long context scaling. We scale the context window of RL to 128k and observe continued improvement of performance with an increased context length. A key idea behind our approach is to use partial rollouts to improve training efficiencyâ€”i.e ... ",
            " Kimi k1.5 TECHNICAL REPORT\nKimi k1.5 short-CoTOpenAI 4oClaude 3.5 Sonnet DeepSeek V3LLaMA-3.1 405B-Inst.\nGeneralCodeMath\n47.333.4\nLiveCodeBench v4 24.08-24.11 (Pass@1-COT)\n36.328.440.531.1Qwen2.5 72B-Inst.\n87.487.2\nMMLU (EM)\n88.388.688.585.3\n94.674.6\nMATH-500 (EM)\n78.373.890.280\n60.8\n9.3AIME 2024 (Pass@1)\n1623.339.223.3\n6869.1\nMMMU_val(Pass@1)\n66.464.5\n70.163.8\nMathVista_test(Pass@1)\n65.369.7\n87.284.3\nIF-Eval (Prompt Strict)\n86.58686.184.1\n91.787.9\nCLUEWSC (EM)\n85.484.790.991.4\n88.376\nC-Eval (EM)\n76.761.586.586.1Qwen2-VLVision\nFigure 2: Kimi k1.5 short-CoT results.\n1 Introduction\nLanguage model pretraining with next token prediction has been studied under the context of the scaling law, where\nproportionally scaling model parameters and data sizes leads to the continued improvement of intelligence. (Kaplan\net al. 2020; Hoffmann et al. 2022) However, this approach is limited to the amount of available high-quality training\ndata (Villalobos et al. 2024; Muennighoff et al. 2023). In this report, we present the training recipe of Kimi k1.5,\nour latest multi-modal LLM trained with reinforcement learning (RL). The goal is to explore a possible new axis\nfor continued scaling. Using RL with LLMs, the models learns to explore with rewards and thus is not limited to a\npre-existing static dataset.\nThere are a few key ingredients about the design and training of k1.5.\nâ€¢Long context scaling . We scale the context window of RL to 128k and observe continued improvement of"
        ],
        "response":"The key ingredients of Kimi k1.5's design and training include long context scaling with a context window expanded to 128k, and the use of partial rollouts to improve training efficiency. This approach continues to improve performance as the context length increases. The training also involves reinforcement learning (RL) to allow the model to explore with rewards beyond static datasets.",
        "reference":"Key ingredients include long context scaling, improved policy optimization (deriving a formulation of RL with long-CoT and using a variant of online mirror descent), a simplistic framework, multimodal training (text and vision data), and effective long2short methods.",
        "faithfulness":1.0,
        "answer_relevancy":1.0,
        "context_precision":0.3333333333,
        "answer_correctness":0.3046427887
    },
    {
        "user_input":"How is the RL prompt set curated for Kimi k1.5?",
        "retrieved_contexts":[
            " Kimi k1.5 TECHNICAL REPORT\n2 Approach: Reinforcement Learning with LLMs\nThe development of Kimi k1.5 consists of several stages: pretraining, vanilla supervised fine-tuning (SFT), long-CoT\nsupervised fine-turning, and reinforcement learning (RL). This report focuses on RL, beginning with an overview of\nthe RL prompt set curation (Section 2.1) and long-CoT supervised finetuning (Section 2.2), followed by an in-depth\ndiscussion of RL training strategies in Section 2.3. Additional details on pretraining and vanilla supervised finetuning\ncan be found in Section 2.5.\n2.1 RL Prompt Set Curation\nThrough our preliminary experiments, we found that the quality and diversity of the RL prompt set play a critical role in\nensuring the effectiveness of reinforcement learning. A well-constructed prompt set not only guides the model toward\nrobust reasoning but also mitigates the risk of reward hacking and overfitting to superficial patterns. Specifically, three\nkey properties define a high-quality RL prompt set:\nâ€¢Diverse Coverage : Prompts should span a wide array of disciplines, such as STEM, coding, and general reasoning,\nto enhance the modelâ€™s adaptability and ensure broad applicability across different domains.\nâ€¢Balanced Difficulty : The prompt set should include a well-distributed range of easy, moderate, and difficult\nquestions to facilitate gradual learning and prevent overfitting to specific complexity levels. ",
            " These architectural improvements collectively enable Kimi K1.5 to achieve state-of-the-art reasoning capabilities across diverse benchmarks. RL Framework & Policy Optimization. Kimi K1.5's RL pipeline consists of four stages: RL Prompt Set Curation. A high-quality prompt dataset is essential for stable RL training. It employs: ",
            " å¤šé˜¶æ®µè®­ç»ƒï¼šKimi k1.5 çš„è®­ç»ƒåŒ…æ‹¬é¢„è®­ç»ƒã€æœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç­‰é˜¶æ®µï¼Œå…¶ä¸­ RL æ˜¯é‡ç‚¹ã€‚ RL Prompt Set Curationï¼šç²¾å¿ƒè®¾è®¡é«˜è´¨é‡çš„ RL æç¤ºé›†ï¼ŒåŒ…æ‹¬ï¼š å¤šæ ·æ€§ï¼šè¦†ç›– STEMã€ç¼–ç ã€é€šç”¨æ¨ç†ç­‰å¤šä¸ªé¢†åŸŸã€‚"
        ],
        "response":"The RL prompt set for Kimi k1.5 is curated to ensure quality and diversity, spanning various disciplines including STEM, coding, and general reasoning. It includes a balanced range of difficulty levels to facilitate gradual learning and avoid overfitting. These properties are critical for the effectiveness of reinforcement learning.",
        "reference":"The RL prompt set is curated to have diverse coverage (spanning STEM, coding, general reasoning), balanced difficulty (including easy, moderate, and difficult questions), and accurate evaluability. It uses automatic filters, a tagging system, a model-based difficulty assessment method, and a process to remove easy-to-hack prompts.",
        "faithfulness":0.9090909091,
        "answer_relevancy":0.9269155449,
        "context_precision":0.9999999999,
        "answer_correctness":0.6059134179
    },
    {
        "user_input":"What is the process of long-CoT supervised fine-tuning in Kimi k1.5?",
        "retrieved_contexts":[
            " The team overcame these issues in DeepSeek-R1 by supervised fine-tuning on a small number of long CoT examples prior to reinforcement learning. Similarly, the Kimi k1.5 team found that fine-tuning the model on long CoTs prior to reinforcement learning enabled it to devise its own problem-solving strategies. ",
            " Kimi k1.5 TECHNICAL REPORT\n3.2 Main Results\nK1.5 long-CoT model The performance of the Kimi k1.5 long-CoT model is presented in Table 2. Through long-CoT\nsupervised fine-tuning (described in Section 2.2) and vision-text joint reinforcement learning (discussed in Section 2.3),\nthe modelâ€™s long-term reasoning capabilities are enhanced significantly. The test-time computation scaling further\nstrengthens its performance, enabling the model to achieve state-of-the-art results across a range of modalities. Our\nevaluation reveals marked improvements in the modelâ€™s capacity to reason, comprehend, and synthesize information\nover extended contexts, representing a advancement in multi-modal AI capabilities.\nK1.5 short-CoT model The performance of the Kimi k1.5 short-CoT model is presented in Table 3. This model\nintegrates several techniques, including traditional supervised fine-tuning (discussed in Section 2.5.2), reinforcement\nlearning (explored in Section 2.3), and long-to-short distillation (outlined in Section 2.4). The results demonstrate\nthat the k1.5 short-CoT model delivers competitive or superior performance compared to leading open-source and\nproprietary models across multiple tasks. These include text, vision, and reasoning challenges, with notable strengths in\nnatural language understanding, mathematics, coding, and logical reasoning.\nBenchmark (Metric)Language-only Model Vision-Language Model\nQwQ-32B OpenAI QVQ-72B OpenAI Kimi\nPreview o1-mini Preview o1 k1.5 ",
            " Supervised Fine-Tuning (SFT) Vanilla SFT: After pretraining, ... Long-CoT SFT: This phase focuses on long-chain of thought (CoT) reasoning, ... Throughout the training process, Kimi k1.5 is evaluated against various benchmarks to assess its performance. The model undergoes iterative updates based on feedback from these evaluations, continuously ..."
        ],
        "response":"The process of long-CoT supervised fine-tuning in Kimi k1.5 involves fine-tuning the model on long-chain of thought (CoT) examples prior to reinforcement learning. This approach enhances its long-term reasoning capabilities and enables it to devise its own problem-solving strategies. The model is evaluated against various benchmarks throughout the training process for iterative improvements.",
        "reference":"A small yet high-quality long-CoT warmup dataset is constructed using prompt engineering. It contains verified reasoning paths for text and image inputs. Then, a lightweight SFT is performed on this dataset to prime the model to internalize reasoning strategies and improve its performance in generating detailed and coherent responses.",
        "faithfulness":1.0,
        "answer_relevancy":0.9636751775,
        "context_precision":0.0,
        "answer_correctness":0.3496630073
    },
    {
        "user_input":"What is the policy optimization algorithm used in Kimi k1.5's RL training?",
        "retrieved_contexts":[
            " The core of Kimi K1.5's training lies in RL training. ... allowing it to implicitly learn the search and planning algorithms during training. The RL algorithm used here is a variant of online ... ",
            " These architectural improvements collectively enable Kimi K1.5 to achieve state-of-the-art reasoning capabilities across diverse benchmarks. RL Framework & Policy Optimization. Kimi K1.5's RL pipeline consists of four stages: RL Prompt Set Curation. A high-quality prompt dataset is essential for stable RL training. It employs: ",
            " KIMI K 1.5:\nSCALING REINFORCEMENT LEARNING WITH LLM S\nTECHNICAL REPORT OF KIMI K 1.5\nKimi Team\nABSTRACT\nLanguage model pretraining with next token prediction has proved effective for scaling compute but\nis limited to the amount of available training data. Scaling reinforcement learning (RL) unlocks a new\naxis for the continued improvement of artificial intelligence, with the promise that large language\nmodels (LLMs) can scale their training data by learning to explore with rewards. However, prior\npublished work has not produced competitive results. In light of this, we report on the training practice\nof Kimi k1.5, our latest multi-modal LLM trained with RL, including its RL training techniques,\nmulti-modal data recipes, and infrastructure optimization. Long context scaling and improved policy\noptimization methods are key ingredients of our approach, which establishes a simplistic, effective\nRL framework without relying on more complex techniques such as Monte Carlo tree search, value\nfunctions, and process reward models. Notably, our system achieves state-of-the-art reasoning\nperformance across multiple benchmarks and modalitiesâ€”e.g., 77.5 on AIME, 96.2 on MATH\n500, 94-th percentile on Codeforces, 74.9 on MathVistaâ€”matching OpenAIâ€™s o1. Moreover, we\npresent effective long2short methods that use long-CoT techniques to improve short-CoT models,\nyielding state-of-the-art short-CoT reasoning resultsâ€”e.g., 60.8 on AIME, 94.6 on MATH500, 47.3"
        ],
        "response":"Kimi K1.5 uses an improved policy optimization method in its RL training.",
        "reference":"A variant of online policy mirror decent is used. At each iteration, it optimizes a relative entropy regularized policy optimization problem. The surrogate loss is calculated and gradients are taken using samples from the reference policy. It excludes the value network to encourage the model to explore diverse reasoning paths.",
        "faithfulness":1.0,
        "answer_relevancy":0.8041186907,
        "context_precision":0.9999999999,
        "answer_correctness":0.4266255511
    },
    {
        "user_input":"How does Kimi k1.5 address the overthinking phenomenon during RL training?",
        "retrieved_contexts":[
            " Kimi k1.5 TECHNICAL REPORT\n2 Approach: Reinforcement Learning with LLMs\nThe development of Kimi k1.5 consists of several stages: pretraining, vanilla supervised fine-tuning (SFT), long-CoT\nsupervised fine-turning, and reinforcement learning (RL). This report focuses on RL, beginning with an overview of\nthe RL prompt set curation (Section 2.1) and long-CoT supervised finetuning (Section 2.2), followed by an in-depth\ndiscussion of RL training strategies in Section 2.3. Additional details on pretraining and vanilla supervised finetuning\ncan be found in Section 2.5.\n2.1 RL Prompt Set Curation\nThrough our preliminary experiments, we found that the quality and diversity of the RL prompt set play a critical role in\nensuring the effectiveness of reinforcement learning. A well-constructed prompt set not only guides the model toward\nrobust reasoning but also mitigates the risk of reward hacking and overfitting to superficial patterns. Specifically, three\nkey properties define a high-quality RL prompt set:\nâ€¢Diverse Coverage : Prompts should span a wide array of disciplines, such as STEM, coding, and general reasoning,\nto enhance the modelâ€™s adaptability and ensure broad applicability across different domains.\nâ€¢Balanced Difficulty : The prompt set should include a well-distributed range of easy, moderate, and difficult\nquestions to facilitate gradual learning and prevent overfitting to specific complexity levels. ",
            " The development of Kimi k1.5 went through several stages: pre-training, supervised fine-tuning (SFT), long-CoT supervised fine-tuning, and reinforcement learning (RL). Kimi has always had an advantage in supporting long contexts, and this time they also introduced long-text RL during the reinforcement learning phase to enable models to provide ... ",
            " Kimi k1.5 TECHNICAL REPORT\n2.6.1 Large Scale Reinforcement Learning Training System for LLM\nIn the realm of artificial intelligence, reinforcement learning (RL) has emerged as a pivotal training methodology\nfor large language models (LLMs)(Ouyang et al. 2022)(Jaech et al. 2024), drawing inspiration from its success in\nmastering complex games like Go, StarCraft II, and Dota 2 through systems such as AlphaGo(Silver et al. 2017),\nAlphaStar(Vinyals et al. 2019), and OpenAI Dota Five (Berner et al. 2019). Following in this tradition, the Kimi\nk1.5 system adopts an iterative synchronous RL framework, meticulously designed to bolster the modelâ€™s reasoning\ncapabilities through persistent learning and adaptation. A key innovation in this system is the introduction of a Partial\nRollout technique, designed to optimize the handling of complex reasoning trajectories.\nThe RL training system as illustrated in Figure 3a operates through an iterative synchronous approach, with each\niteration encompassing a rollout phase and a training phase. During the rollout phase, rollout workers, coordinated\nby a central master, generate rollout trajectories by interacting with the model, producing sequences of responses to\nvarious inputs. These trajectories are then stored in a replay buffer, which ensures a diverse and unbiased dataset for\ntraining by disrupting temporal correlations. In the subsequent training phase, trainer workers access these experiences"
        ],
        "response":"Kimi k1.5 addresses the overthinking phenomenon during RL training by curating a high-quality, diverse, and balanced RL prompt set. This prompt set includes a wide array of disciplines and ranges in difficulty to ensure adaptability and prevent overfitting to specific complexity levels. Additionally, the Partial Rollout technique optimizes handling of complex reasoning trajectories.",
        "reference":"Kimi k1.5 introduces a length reward to restrain the growth of token length. Given sampled responses, it calculates a length reward based on the response length and correctness. It also gradually warms up the length penalty during training to alleviate potential slowdowns.",
        "faithfulness":0.625,
        "answer_relevancy":1.0,
        "context_precision":0.0,
        "answer_correctness":0.1862074656
    },
    {
        "user_input":"What are the sampling strategies used in Kimi k1.5's RL training?",
        "retrieved_contexts":[
            " Kimi k1.5 TECHNICAL REPORT\n2.6.1 Large Scale Reinforcement Learning Training System for LLM\nIn the realm of artificial intelligence, reinforcement learning (RL) has emerged as a pivotal training methodology\nfor large language models (LLMs)(Ouyang et al. 2022)(Jaech et al. 2024), drawing inspiration from its success in\nmastering complex games like Go, StarCraft II, and Dota 2 through systems such as AlphaGo(Silver et al. 2017),\nAlphaStar(Vinyals et al. 2019), and OpenAI Dota Five (Berner et al. 2019). Following in this tradition, the Kimi\nk1.5 system adopts an iterative synchronous RL framework, meticulously designed to bolster the modelâ€™s reasoning\ncapabilities through persistent learning and adaptation. A key innovation in this system is the introduction of a Partial\nRollout technique, designed to optimize the handling of complex reasoning trajectories.\nThe RL training system as illustrated in Figure 3a operates through an iterative synchronous approach, with each\niteration encompassing a rollout phase and a training phase. During the rollout phase, rollout workers, coordinated\nby a central master, generate rollout trajectories by interacting with the model, producing sequences of responses to\nvarious inputs. These trajectories are then stored in a replay buffer, which ensures a diverse and unbiased dataset for\ntraining by disrupting temporal correlations. In the subsequent training phase, trainer workers access these experiences ",
            " The development of Kimi k1.5 went through several stages: pre-training, supervised fine-tuning (SFT), long-CoT supervised fine-tuning, and reinforcement learning (RL). Kimi has always had an advantage in supporting long contexts, and this time they also introduced long-text RL during the reinforcement learning phase to enable models to provide ... ",
            " Kimi k1.5 TECHNICAL REPORT\n2 Approach: Reinforcement Learning with LLMs\nThe development of Kimi k1.5 consists of several stages: pretraining, vanilla supervised fine-tuning (SFT), long-CoT\nsupervised fine-turning, and reinforcement learning (RL). This report focuses on RL, beginning with an overview of\nthe RL prompt set curation (Section 2.1) and long-CoT supervised finetuning (Section 2.2), followed by an in-depth\ndiscussion of RL training strategies in Section 2.3. Additional details on pretraining and vanilla supervised finetuning\ncan be found in Section 2.5.\n2.1 RL Prompt Set Curation\nThrough our preliminary experiments, we found that the quality and diversity of the RL prompt set play a critical role in\nensuring the effectiveness of reinforcement learning. A well-constructed prompt set not only guides the model toward\nrobust reasoning but also mitigates the risk of reward hacking and overfitting to superficial patterns. Specifically, three\nkey properties define a high-quality RL prompt set:\nâ€¢Diverse Coverage : Prompts should span a wide array of disciplines, such as STEM, coding, and general reasoning,\nto enhance the modelâ€™s adaptability and ensure broad applicability across different domains.\nâ€¢Balanced Difficulty : The prompt set should include a well-distributed range of easy, moderate, and difficult\nquestions to facilitate gradual learning and prevent overfitting to specific complexity levels."
        ],
        "response":"The sampling strategies used in Kimi k1.5's RL training include Partial Rollout technique to optimize handling complex reasoning trajectories and ensuring diverse and unbiased datasets through a replay buffer. The prompt set curation emphasizes diverse coverage across various disciplines and balanced difficulty levels to enhance adaptability and prevent overfitting.",
        "reference":"Curriculum Sampling is used, where training starts with easier tasks and progresses to more challenging ones. Prioritized Sampling is also employed, which focuses on problems where the model underperforms by sampling proportional to 1 - success rate.",
        "faithfulness":0.7142857143,
        "answer_relevancy":0.9283682155,
        "context_precision":0.0,
        "answer_correctness":0.1916878972
    },
    {
        "user_input":"What are the long2short methods used in Kimi k1.5?",
        "retrieved_contexts":[
            " Abstract page for arXiv paper 2501.12599: Kimi k1.5: Scaling Reinforcement Learning with LLMs ... Moreover, we present effective long2short methods that use long-CoT techniques to improve short-CoT models, yielding state-of-the-art short-CoT reasoning results -- e.g., 60.8 on AIME, 94.6 on MATH500, 47.3 on LiveCodeBench -- outperforming ... ",
            " Contribute to MoonshotAI\/Kimi-k1.5 development by creating an account on GitHub. ... Moreover, we present effective long2short methods that use long-CoT techniques to improve short-CoT models, yielding state-of-the-art short-CoT reasoning results---e.g., 60.8 on AIME, 94.6 on MATH500, 47.3 on LiveCodeBench---outperforming existing short-CoT ... ",
            " Kimi k1.5 TECHNICAL REPORT\nLong2short RL After a standard RL training phase, we select a model that offers the best balance between perfor-\nmance and token efficiency to serve as the base model, and conduct a separate long2short RL training phase. In this\nsecond phase, we apply the length penalty introduced in Section 2.3.3, and significantly reduce the maximum rollout\nlength to further penalize responses that exceed the desired length while possibly correct.\n2.5 Other Training Details\n2.5.1 Pretraining\nThe Kimi k1.5 base model is trained on a diverse, high-quality multimodal corpus. The language data covers five\ndomains: English, Chinese, Code, Mathematics Reasoning, and Knowledge. Multimodal data, including Captioning,\nImage-text Interleaving, OCR, Knowledge, and QA datasets, enables our model to acquire vision-language capabilities.\nRigorous quality control ensures relevance, diversity, and balance in the overall pretrain dataset. Our pretraining\nproceeds in three stages: (1) Vision-language pretraining, where a strong language foundation is established, followed\nby gradual multimodal integration; (2) Cooldown, which consolidates capabilities using curated and synthetic data,\nparticularly for reasoning and knowledge-based tasks; and (3) Long-context activation, extending sequence processing\nto 131,072 tokens. More details regarding our pretraining efforts can be found in Appendix B.\n2.5.2 Vanilla Supervised Finetuning"
        ],
        "response":"The long2short methods in Kimi k1.5 utilize long-CoT (Chain of Thought) techniques to improve short-CoT models by applying a length penalty and reducing the maximum rollout length during a separate long2short reinforcement learning (RL) training phase.",
        "reference":"The long2short methods include model merging (averaging weights of long-cot and short-cot models), shortest rejection sampling (selecting the shortest correct response for fine-tuning), DPO (using long CoT model to generate samples for pairwise preference training), and long2short RL (applying length penalty and reducing rollout length after a standard RL training phase).",
        "faithfulness":0.7142857143,
        "answer_relevancy":0.905855853,
        "context_precision":0.0,
        "answer_correctness":0.5573129965
    },
    {
        "user_input":"What is the training process of Kimi k1.5 in terms of pretraining and vanilla supervised finetuning?",
        "retrieved_contexts":[
            " Extensive scaling experiments indicate that most of the base model performance comes from improvements in the\nquality and diversity of the pretraining data. Specific details regarding model architecture scaling experiments lie\nbeyond the scope of this report and will be addressed in future publications.\nB.4 Training Stages\nThe Kimi k1.5 model is trained in three stages: the vision-language pretraining stage, the vision-language cooldown\nstage, and the long-context activation stage. Each stage of the Kimi k1.5 modelâ€™s training focuses on a particular\ncapability enhancement.\nVision-language pretraining stage In this stage, the model is firstly trained solely on language data, establishing\na robust language model foundation. Then the model is gradually introduced to interleaved vision-language data,\nacquiring multimodal capabilities. The visual tower is initially trained in isolation without updating the language model\nparameters, then we unfreeze the language model layers, and ultimately increase the proportion of vision-text data\n23 ",
            " The development of Kimi k1.5 consists of several stages: pretraining, vanilla supervised fine-tuning (SFT), long-CoT supervised fine-turning, and reinforcement learning (RL). This report focuses on RL, beginning with an overview of the RL prompt set curation (Section 2.1 ) and long-CoT supervised finetuning (Section 2.2 ), followed by an in ... ",
            " Kimi k1.5 Training. The training process of Kimi k1.5 is a comprehensive and multi-stage approach designed to enhance its reasoning capabilities through reinforcement learning (RL) and multimodal integration. Here's a breakdown of the training process: 1. Pretraining Stage. Data Collection: ... Supervised Fine-Tuning (SFT) Vanilla SFT: ..."
        ],
        "response":"The training process of Kimi k1.5 involves a multi-stage approach. Initially, there is a vision-language pretraining stage where the model is trained on language data first and then on interleaved vision-language data. This is followed by vanilla supervised fine-tuning (SFT) used to enhance the model's performance post-pretraining.",
        "reference":"In pretraining, it is trained on a diverse multimodal corpus in three stages: vision-language pretraining, cooldown, and long-context activation. In vanilla supervised finetuning, a corpus covering multiple domains is created. For non-reasoning tasks, a seed dataset is constructed through human annotation and refined. For reasoning tasks, rejection sampling is used. The model is trained at different sequence lengths with learning rate decay.",
        "faithfulness":0.6666666667,
        "answer_relevancy":0.9149046975,
        "context_precision":0.3333333333,
        "answer_correctness":0.4275647998
    },
    {
        "user_input":"What are the main contributions of this research?",
        "retrieved_contexts":[
            " 'The main contribution of this study is'...: An analysis of statements of contribution in English published research articles and L2 manuscripts ",
            " 1.1. Contributions\nPost-Training: Large-Scale Reinforcement Learning on the Base Model\nâ€¢We directly apply reinforcement learning (RL) to the base model without relying on super-\nvised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore\nchain-of-thought (CoT) for solving complex problems, resulting in the development of\nDeepSeek-R1-Zero. DeepSeek-R1-Zero demonstrates capabilities such as self-verification,\nreflection, and generating long CoTs, marking a significant milestone for the research\ncommunity. Notably, it is the first open research to validate that reasoning capabilities of\nLLMs can be incentivized purely through RL, without the need for SFT. This breakthrough\npaves the way for future advancements in this area.\nâ€¢We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL\nstages aimed at discovering improved reasoning patterns and aligning with human pref-\nerences, as well as two SFT stages that serve as the seed for the modelâ€™s reasoning and\nnon-reasoning capabilities. We believe the pipeline will benefit the industry by creating\nbetter models.\nDistillation: Smaller Models Can Be Powerful Too\nâ€¢We demonstrate that the reasoning patterns of larger models can be distilled into smaller\nmodels, resulting in better performance compared to the reasoning patterns discovered\nthrough RL on small models. The open source DeepSeek-R1, as well as its API, will benefit ",
            " Types of Research Contribution. The contribution of a thesis can be categorized into several types, each serving a distinct purpose in advancing knowledge. Below are the key types of research contributions: 1. Theoretical Contribution. This type of contribution involves the development or refinement of theories, models, or conceptual frameworks."
        ],
        "response":"The main contributions of this study are the application of reinforcement learning directly to the base model without supervised fine-tuning, resulting in the innovative development of DeepSeek-R1-Zero. This approach allows for advanced reasoning capabilities like self-verification and reflection. Additionally, the study introduces a pipeline incorporating RL and SFT stages to improve reasoning patterns and align with human preferences, and demonstrates that larger model reasoning patterns can be distilled into smaller models for better performance.",
        "reference":"Directly applied RL to the base model without preliminary SFT, developing DeepSeek-R1-Zero which exhibits self-verification, reflection, etc.Introduced the training pipeline for DeepSeek-R1, involving two RL and two SFT stages.Introduced the training pipeline for DeepSeek-R1, involving two RL and two SFT stages.Demonstrated the distillation of reasoning patterns from larger to smaller models and open - sourced relevant models and API.",
        "faithfulness":1.0,
        "answer_relevancy":0.7093021546,
        "context_precision":0.5,
        "answer_correctness":0.6732596611
    },
    {
        "user_input":"How does DeepSeek-R1-Zero improve reasoning capabilities?",
        "retrieved_contexts":[
            " We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it ... ",
            " The Self-Evolution Process of DeepSeek-R1-Zero demonstrates how Reinforcement Learning can improve the model's reasoning capabilities autonomously. The chart shows how the model's reasoning capabilities for handling complex reasoning tasks evolve. Source: DeepSeek-R1 paper Enhancing Reasoning and General Capabilities in DeepSeek-R1 ",
            " DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via\nReinforcement Learning\nDeepSeek-AI\nresearch@deepseek.com\nAbstract\nWe introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1.\nDeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without super-\nvised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities.\nThrough RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing\nreasoning behaviors. However, it encounters challenges such as poor readability, and language\nmixing. To address these issues and further enhance reasoning performance, we introduce\nDeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-\nR1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the\nresearch community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models\n(1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.\nAIME 2024\n(Pass@1)Codeforces\n(Percentile)GPQA Diamond\n(Pass@1)MATH-500\n(Pass@1)MMLU\n(Pass@1)SWE-bench Verified\n(Resolved)020406080100Accuracy \/ Percentile (%)79.896.3\n71.597.3\n90.8\n49.279.296.6\n75.796.4\n91.8\n48.972.690.6\n62.194.3\n87.4\n36.863.693.4\n60.090.0\n85.2\n41.6\n39.258.7 59.190.2\n88.5\n42.0DeepSeek-R1 OpenAI-o1-1217 DeepSeek-R1-32B OpenAI-o1-mini DeepSeek-V3\nFigure 1|Benchmark performance of DeepSeek-R1."
        ],
        "response":"DeepSeek-R1-Zero improves reasoning capabilities through large-scale reinforcement learning (RL) without supervised fine-tuning. This training method allows the model to autonomously develop powerful reasoning behaviors. The self-evolution process enhances its ability to handle complex reasoning tasks.",
        "reference":"By using Group Relative Policy Optimization (GRPO) as the RL algorithm, a rule - based reward system (accuracy and format rewards), and a simple training template. Through RL training, it shows a self - evolution process with increasing thinking time and emerging sophisticated reasoning behaviors, like reflection and exploring alternative problem - solving approaches.",
        "faithfulness":1.0,
        "answer_relevancy":0.9902796502,
        "context_precision":0.0,
        "answer_correctness":0.4895959915
    },
    {
        "user_input":"What is the difference between DeepSeek-R1-Zero and DeepSeek-R1?",
        "retrieved_contexts":[
            " What are the main differences between DeepSeek-R1 and DeepSeek-R1-Zero? DeepSeek-R1 builds on the foundation of DeepSeek-R1-Zero, which serves as the base model. The two differ in their training approaches and overall performance. 1. Training method. DeepSeek-R1-Zero: Trained entirely with reinforcement learning (RL) and no supervised fine ... ",
            " DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via\nReinforcement Learning\nDeepSeek-AI\nresearch@deepseek.com\nAbstract\nWe introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1.\nDeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without super-\nvised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities.\nThrough RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing\nreasoning behaviors. However, it encounters challenges such as poor readability, and language\nmixing. To address these issues and further enhance reasoning performance, we introduce\nDeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-\nR1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the\nresearch community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models\n(1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.\nAIME 2024\n(Pass@1)Codeforces\n(Percentile)GPQA Diamond\n(Pass@1)MATH-500\n(Pass@1)MMLU\n(Pass@1)SWE-bench Verified\n(Resolved)020406080100Accuracy \/ Percentile (%)79.896.3\n71.597.3\n90.8\n49.279.296.6\n75.796.4\n91.8\n48.972.690.6\n62.194.3\n87.4\n36.863.693.4\n60.090.0\n85.2\n41.6\n39.258.7 59.190.2\n88.5\n42.0DeepSeek-R1 OpenAI-o1-1217 DeepSeek-R1-32B OpenAI-o1-mini DeepSeek-V3\nFigure 1|Benchmark performance of DeepSeek-R1. ",
            " DeepSeek-R1-Zero: This model is a ... The differences between DeepSeek-R1 and DeepSeek-R1-Zero highlight the trade-offs between innovation and practicality. While R1-Zero pushes the boundaries of ..."
        ],
        "response":"DeepSeek-R1-Zero is trained entirely with reinforcement learning and no supervised fine-tuning, which results in strong reasoning capabilities but issues with readability and language mixing. DeepSeek-R1 addresses these issues by incorporating multi-stage training and cold-start data before reinforcement learning, enhancing both readability and overall performance.",
        "reference":"DeepSeek-R1-Zero applies RL directly to the base model without SFT data, while DeepSeek-R1 uses a small amount of cold - start data to fine - tune the model first. DeepSeek-R1-Zero has issues like poor readability and language mixing, which DeepSeek-R1 addresses through its multi - stage training pipeline.",
        "faithfulness":0.9,
        "answer_relevancy":0.912180961,
        "context_precision":1.0,
        "answer_correctness":0.8200719516
    },
    {
        "user_input":"What is the cold start in the training of DeepSeek-R1?",
        "retrieved_contexts":[
            " DeepSeek-R1: To address the limitations of R1-Zero, this version incorporates cold-start data. \"Cold start\" means using curated examples of high-quality reasoning to give the model a foundation before applying RL. After initial fine-tuning, RL is used to refine its capabilities further, resulting in: Clearer and more structured answers. ",
            " Inspired by the promising results of DeepSeek-R1-Zero, two natural questions arise: 1) Can\nreasoning performance be further improved or convergence accelerated by incorporating a small\namount of high-quality data as a cold start? 2) How can we train a user-friendly model that\nnot only produces clear and coherent Chains of Thought (CoT) but also demonstrates strong\ngeneral capabilities? To address these questions, we design a pipeline to train DeepSeek-R1. The\npipeline consists of four stages, outlined as follows.\n2.3.1. Cold Start\nUnlike DeepSeek-R1-Zero, to prevent the early unstable cold start phase of RL training from\nthe base model, for DeepSeek-R1 we construct and collect a small amount of long CoT data\nto fine-tune the model as the initial RL actor. To collect such data, we have explored several\napproaches: using few-shot prompting with a long CoT as an example, directly prompting\nmodels to generate detailed answers with reflection and verification, gathering DeepSeek-R1-\nZero outputs in a readable format, and refining the results through post-processing by human\nannotators.\nIn this work, we collect thousands of cold-start data to fine-tune the DeepSeek-V3-Base as\nthe starting point for RL. Compared to DeepSeek-R1-Zero, the advantages of cold start data\n9 ",
            " To overcome these challenges and further enhance reasoning performance, the authors introduced DeepSeek-R1, which employs a multi-stage training pipeline incorporating a cold start phase followed ..."
        ],
        "response":"The cold start in the training of DeepSeek-R1 involves using curated examples of high-quality reasoning (long Chains of Thought data) to fine-tune the model before applying reinforcement learning. This initial fine-tuning aims to prevent instability and give a solid foundation for further improvement. The curated data helps the model produce clearer, structured answers from the beginning.",
        "reference":"It constructs and collects a small amount of long Chain-of-Thought (CoT) data to fine - tune the DeepSeek-V3-Base as the initial RL actor. The cold - start data is designed to be more readable and shows better potential compared to DeepSeek-R1-Zero. Data collection methods include few - shot prompting, direct prompting for detailed answers, gathering DeepSeek-R1-Zero outputs, and post - processing by human annotators.",
        "faithfulness":1.0,
        "answer_relevancy":0.904032763,
        "context_precision":0.5,
        "answer_correctness":0.5059008128
    },
    {
        "user_input":"How is the reward modeled in DeepSeek-R1-Zero?",
        "retrieved_contexts":[
            " A conversation between User and Assistant. The user asks a question, and the Assistant solves it.\nThe assistant first thinks about the reasoning process in the mind and then provides the user\nwith the answer. The reasoning process and answer are enclosed within <think> <\/think> and\n<answer> <\/answer> tags, respectively, i.e., <think> reasoning process here <\/think>\n<answer> answer here <\/answer>. User: prompt. Assistant:\nTable 1|Template for DeepSeek-R1-Zero. prompt will be replaced with the specific reasoning\nquestion during training.\n2.2.2. Reward Modeling\nThe reward is the source of the training signal, which decides the optimization direction of RL.\nTo train DeepSeek-R1-Zero, we adopt a rule-based reward system that mainly consists of two\ntypes of rewards:\nâ€¢Accuracy rewards : The accuracy reward model evaluates whether the response is correct.\nFor example, in the case of math problems with deterministic results, the model is required\nto provide the final answer in a specified format (e.g., within a box), enabling reliable\nrule-based verification of correctness. Similarly, for LeetCode problems, a compiler can be\nused to generate feedback based on predefined test cases.\nâ€¢Format rewards : In addition to the accuracy reward model, we employ a format reward\nmodel that enforces the model to put its thinking process between â€˜<think>â€™ and â€˜<\/think>â€™\ntags.\nWe do not apply the outcome or process neural reward model in developing DeepSeek-R1-Zero, ",
            " While training DeepSeek-R1-Zero, a rule based reward system is employed, which majorly consists of 2 types - Accuracy rewards â€” This model evaluates whether or not the response is correct. These ... ",
            " DeepSeek-R1-Zero = RL Model; DeepSeek-R1 = Improved RL Model; Note: These take up 670+GB of disk space when downloaded, and a cluster of GPUs to run. ... They constrain the policy model's outputs to be close to the reward model's outputs using a KL divergence constraint in their loss, so that the model doesn't drift too much during the RL ..."
        ],
        "response":"The reward in DeepSeek-R1-Zero is modeled using a rule-based reward system that consists of two main types: accuracy rewards and format rewards. Accuracy rewards evaluate the correctness of the response, while format rewards ensure the model's thinking process is enclosed within specific tags. This approach provides a reliable training signal for optimization.",
        "reference":"It uses a rule - based reward system. The accuracy reward model evaluates whether the response is correct, for example, verifying math problem answers in a specified format or using a compiler for LeetCode problems. The format reward model enforces the model to put its thinking process between â€˜<think>â€™ and â€˜<\/think>â€™ tags. Neural reward models are not used due to potential reward hacking and high resource requirements.",
        "faithfulness":0.6666666667,
        "answer_relevancy":0.9381812395,
        "context_precision":1.0,
        "answer_correctness":0.5383790939
    },
    {
        "user_input":"What benchmarks are used to evaluate the models?",
        "retrieved_contexts":[
            " Machine learning benchmarks are baselines, assessment measures, and standardized datasets that are used to evaluate and compare the effectiveness of machine learning models. ",
            " These optimizations improve RL efficiency in code execution, providing a consistent and reliable environment for\nevaluating RL-generated code, essential for iterative training and model improvement.\n3 Experiments\n3.1 Evaluation\nSince k1.5 is a multimodal model, we conducted comprehensive evaluation across various benchmarks for different\nmodalities. The detailed evaluation setup can be found in Appendix C. Our benchmarks primarily consist of the\nfollowing three categories:\nâ€¢Text Benchmark : MMLU (Hendrycks et al. 2020), IF-Eval (J. Zhou et al. 2023), CLUEWSC (L. Xu et al. 2020),\nC-EV AL (Y . Huang et al. 2023)\nâ€¢Reasoning Benchmark : HumanEval-Mul, LiveCodeBench (Jain et al. 2024), Codeforces, AIME 2024, MATH-\n500 (Lightman et al. 2023)\nâ€¢Vision Benchmark : MMMU (Yue, Ni, et al. 2024), MATH-Vision (K. Wang et al. 2024), MathVista (Lu et al.\n2023)\n2https:\/\/github.com\/DMOJ\/judge-server\n11 ",
            " IF-Eval (J. Zhou et al. 2023) is a benchmark for evaluating large language modelsâ€™ ability to follow verifiable\ninstructions. There are 500+ prompts with instructions such as \"write an article with more than 800 words\", etc. Due to\na version shift, the number of IFEval reported in Table 3 derived from an intermediate model. We will update the scores\nbased on the final model.\nCLUEWSC (L. Xu et al. 2020) is a coreference resolution task in CLUE benchmark, requiring models to determine if\na pronoun and a noun phrase in a sentence co-refer, with data from Chinese fiction books.\nC-EV AL (Y . Huang et al. 2023) is a comprehensive Chinese evaluation suite for assessing advanced knowledge and\nreasoning abilities of foundation models. It includes 13,948 multiple-choice questions across 52 disciplines and four\ndifficulty levels.\nC.2 Reasoning Benchmark\nHumanEval-Mul is a subset of Multipl-E (Cassano, Gouwar, D. Nguyen, S. D. Nguyen, et al. 2022). MultiPL-E\nextends the HumanEval benchmark and MBPP benchmark to 18 languages that encompass a range of programming\n24"
        ],
        "response":"The benchmarks used to evaluate models include text benchmarks like MMLU, IF-Eval, CLUEWSC, and C-EVAL; reasoning benchmarks like HumanEval-Mul, LiveCodeBench, Codeforces, AIME 2024, and MATH-500; and vision benchmarks like MMMU, MATH-Vision, and MathVista.",
        "reference":"MMLU, MMLU - Redux, MMLU - Pro, C - Eval, CMMLU, IFEval, FRAMES, GPQA Diamond, SimpleQA, C - SimpleQA, SWE - Bench Verified, Aider 1, LiveCodeBench, Codeforces, Chinese National High School Mathematics Olympiad (CNMO 2024), American Invitational Mathematics Examination 2024 (AIME 2024), and open - ended generation tasks like AlpacaEval 2.0 and Arena - Hard.",
        "faithfulness":1.0,
        "answer_relevancy":0.9914201447,
        "context_precision":0.0,
        "answer_correctness":0.3307960056
    },
    {
        "user_input":"What are the baselines for model evaluation?",
        "retrieved_contexts":[
            " This chapter describes model validation, a crucial part of machine learning whether it is to select the best model or to assess performance of a given model. We start by detailing the main performance metrics for different tasks (classification, regression), and how they may be interpreted, including in the face of class imbalance, varying prevalence, or asymmetric cost-benefit trade-offs ... ",
            " Evaluating machine learning models is a important step in ensuring their effectiveness and reliability in real-world applications. Using appropriate metrics such as accuracy, precision, recall, F1 score for classification and regression-specific measures like MAE, MSE, RMSE and MAPE can assess model performance for different tasks. ",
            " Lantz, B. Machine Learning with R: Learn Techniques for Building and Improving Machine Learning Models, from Data Preparation to Model Tuning, Evaluation, and Working with Big Data (Packt ..."
        ],
        "response":"Baselines for model evaluation include metrics such as accuracy, precision, recall, F1 score for classification, and MAE, MSE, RMSE, and MAPE for regression. These metrics help assess the performance and effectiveness of machine learning models.",
        "reference":"DeepSeek-V3, Claude - Sonnet - 3.5 - 1022, GPT - 4o - 0513, OpenAI - o1 - mini, OpenAI - o1 - 1217. For distilled models, QwQ - 32B - Preview is also used as a baseline.",
        "faithfulness":0.9,
        "answer_relevancy":0.703956457,
        "context_precision":0.0,
        "answer_correctness":0.1550868356
    },
    {
        "user_input":"What is the performance of DeepSeek-R1 on math tasks?",
        "retrieved_contexts":[
            " DeepSeek-V3, demonstrating its capability in handling fact-based queries. A similar trend\nis observed where OpenAI-o1 surpasses GPT-4o on this benchmark. However, DeepSeek-R1\nperforms worse than DeepSeek-V3 on the Chinese SimpleQA benchmark, primarily due to its\ntendency to refuse answering certain queries after safety RL. Without safety RL, DeepSeek-R1\ncould achieve an accuracy of over 70%.\nDeepSeek-R1 also delivers impressive results on IF-Eval, a benchmark designed to assess a\nmodelâ€™s ability to follow format instructions. These improvements can be linked to the inclusion\nof instruction-following data during the final stages of supervised fine-tuning (SFT) and RL\ntraining. Furthermore, remarkable performance is observed on AlpacaEval2.0 and ArenaHard,\nindicating DeepSeek-R1â€™s strengths in writing tasks and open-domain question answering. Its\nsignificant outperformance of DeepSeek-V3 underscores the generalization benefits of large-scale\nRL, which not only boosts reasoning capabilities but also improves performance across diverse\ndomains. Moreover, the summary lengths generated by DeepSeek-R1 are concise, with an\naverage of 689 tokens on ArenaHard and 2,218 characters on AlpacaEval 2.0. This indicates that\nDeepSeek-R1 avoids introducing length bias during GPT-based evaluations, further solidifying\nits robustness across multiple tasks.\nOn math tasks, DeepSeek-R1 demonstrates performance on par with OpenAI-o1-1217, ",
            " Evaluation Results. The performance of DeepSeek-R1 is notable on various benchmarks: Reasoning Tasks: DeepSeek-R1 achieves 79.8% Pass@1 on AIME 2024 and 97.3% on MATH-500, performing on par with ... ",
            " DeepSeek-R1 represents a significant stride in open-source AI reasoning, with performance rivalling (and sometimes surpassing) OpenAI's models on math and code tasks. Its reinforcement-learning-first approach , combined with supervised refinements , yields rich, traceable chain-of-thought outputs that appeal to regulated and research-focused ..."
        ],
        "response":"DeepSeek-R1 performs impressively on math tasks, achieving a 97.3% score on the MATH-500 benchmark. Its performance is on par with OpenAI's models, indicating strong capabilities in this domain.",
        "reference":"On AIME 2024, it has a Pass@1 score of 79.8%, slightly surpassing OpenAI - o1 - 1217. On MATH - 500, it attains a score of 97.3%, performing on par with OpenAI - o1 - 1217 and significantly outperforming other models.",
        "faithfulness":1.0,
        "answer_relevancy":0.8994812165,
        "context_precision":0.5,
        "answer_correctness":0.512302669
    },
    {
        "user_input":"Why is distillation of reasoning capabilities important?",
        "retrieved_contexts":[
            " Reasoning is one crucial capability in Large Language Models (LLMs), allowing them to perform complex tasks such as solving math problems and multi-step planning. While reasoning capability can emerge in larger models, smaller ones usually have to rely on distillation to transfer this capability from a larger model. ",
            " Techniques like CoT, RL, and distillation have shown remarkable effectiveness in enhancing reasoning capabilities. DeepSeek-R1 and HuatuoGPT-o1 highlight the potential of these methods, and ongoing research aims to address existing limitations to further improve the reasoning abilities of LLMs, particularly through further development of PRMs ... ",
            " The training methodology for SLMs first emphasises distilling self-evaluation capability to lay the foundation for reducing the impact of errors in CoTs on SLMs, followed by incorporating comprehensive reasoning capability through diverse CoTs distillation. Distillation. This study illustrates an important method, called distillationâ€¦"
        ],
        "response":"Distillation of reasoning capabilities is important because it allows smaller models to acquire the complex reasoning skills found in larger models. This enables them to perform tasks such as solving math problems and multi-step planning, which are crucial for various applications. Distillation thus helps enhance the overall performance and practicality of smaller models.",
        "reference":"It can empower smaller models with reasoning capabilities. Distilling from more powerful models like DeepSeek-R1 into smaller models can achieve better performance than RL training on small models alone. For example, DeepSeek-R1 - Distill - Qwen - 7B outperforms non - reasoning models like GPT - 4o - 0513 across the board.",
        "faithfulness":0.6666666667,
        "answer_relevancy":0.8745232706,
        "context_precision":0.9999999999,
        "answer_correctness":0.4586989762
    }
]
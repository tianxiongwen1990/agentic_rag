[
  {
    "user_input": "What is the core idea of Janus?",
    "retrieved_contexts": [
      " Auto-RegressiveTransformer\nUnd.Encoder\nTextTokenizer\nGen.Encoder\nTextDe-Tokenizer\nTextTokenizer\nImageDecoder\nImage:X!LanguageInstruct:X\"LanguageInstruct:X\"Image:X!LanguageResponse:X#GeneratedImage:X!UnderstandingImageGeneration\n……\n……Figure 2|Architecture of our Janus. Different from previous approaches [ 77,85] that typically\nassume visual understanding and generation require the same visual encoder, our Janus de-\ncouples visual encoding for visual understanding and visual generation. “Und. Encoder” and\n“Gen. Encoder” are abbreviations for “Understanding Encoder” and “Generation Encoder”,\nrespectively. Best viewed in color.\nand generation tasks, regardless of whether they are based on autoregressive (AR) models\n[77,85] or diffusion models [ 86,94]. For example, Chameleon [ 77] adopts a VQ Tokenizer to\nencode images for both multimodal understanding and generation. However, this practice\nmay lead to suboptimal outcomes, as the vision encoder might face a trade-off between the\ndemands of understanding and generation. In contrast, our Janus can explicitly decouple the\nvisual representations for understanding and generation, recognizing that different tasks may\nrequire varying levels of information.\n3. Janus: A Simple, Unified and Flexible Multimodal Framework\n3.1. Architecture\nThe architecture of Janus is shown in Figure 2. For pure text understanding, multimodal\nunderstanding, and visual generation, we apply independent encoding methods to convert the ",
      " In ancient Roman religion and myth, Janus (/ ˈ dʒ eɪ n ə s / JAY-nəs; Latin: Ianvs [ˈi̯aːnʊs]) is the god of beginnings, gates, transitions, time, duality, doorways, [2] passages, frames, and endings. He is usually depicted as having two faces. The month of January is named for Janus (). [3] According to ancient Roman farmers' almanacs, Juno was mistaken as the tutelary deity of the ... ",
      " Janus: Decoupling Visual Encoding for Unified\nMultimodal Understanding and Generation\nChengyue Wu1,2Xiaokang Chen1,∗,†Zhiyu Wu1,3Yiyang Ma1,3Xingchao Liu1Zizheng Pan1\nWen Liu1Zhenda Xie1Xingkai Yu1Chong Ruan1Ping Luo2,∗\n1DeepSeek-AI2The University of Hong Kong3Peking University\n†: Project lead∗: Corresponding authors\nProject Page: https://github.com/deepseek-ai/Janus\nAbstract\nIn this paper, we introduce Janus , an autoregressive framework that unifies multimodal under-\nstanding and generation. Prior research often relies on a single visual encoder for both tasks,\nsuch as Chameleon. However, due to the differing levels of information granularity required by\nmultimodal understanding and generation, this approach can lead to suboptimal performance,\nparticularly in multimodal understanding. To address this issue, we decouple visual encoding\ninto separate pathways, while still leveraging a single, unified transformer architecture for\nprocessing. The decoupling not only alleviates the conflict between the visual encoder’s roles in\nunderstanding and generation, but also enhances the framework’s flexibility. For instance, both\nthe multimodal understanding and generation components can independently select their most\nsuitable encoding methods. Experiments show that Janus surpasses previous unified model\nand matches or exceeds the performance of task-specific models. The simplicity, high flexibility,"
    ],
    "response": "The core idea of Janus is to decouple visual encoding for multimodal understanding and generation into separate pathways. This approach addresses the issue of conflicting demands on a single visual encoder and enhances the framework's flexibility and performance. Janus still leverages a unified transformer architecture to process the encoded information.",
    "reference": "The core idea of Janus is to decouple visual encoding for multimodal understanding and generation. This is achieved by introducing two independent visual encoding pathways, one for multimodal understanding and one for multimodal generation, unified by the same transformer architecture."
  },
  {
    "user_input": "How does Janus's architecture differ from previous unified multimodal models?",
    "retrieved_contexts": [
      " Janus: Decoupling Visual Encoding for Unified\nMultimodal Understanding and Generation\nChengyue Wu1,2Xiaokang Chen1,∗,†Zhiyu Wu1,3Yiyang Ma1,3Xingchao Liu1Zizheng Pan1\nWen Liu1Zhenda Xie1Xingkai Yu1Chong Ruan1Ping Luo2,∗\n1DeepSeek-AI2The University of Hong Kong3Peking University\n†: Project lead∗: Corresponding authors\nProject Page: https://github.com/deepseek-ai/Janus\nAbstract\nIn this paper, we introduce Janus , an autoregressive framework that unifies multimodal under-\nstanding and generation. Prior research often relies on a single visual encoder for both tasks,\nsuch as Chameleon. However, due to the differing levels of information granularity required by\nmultimodal understanding and generation, this approach can lead to suboptimal performance,\nparticularly in multimodal understanding. To address this issue, we decouple visual encoding\ninto separate pathways, while still leveraging a single, unified transformer architecture for\nprocessing. The decoupling not only alleviates the conflict between the visual encoder’s roles in\nunderstanding and generation, but also enhances the framework’s flexibility. For instance, both\nthe multimodal understanding and generation components can independently select their most\nsuitable encoding methods. Experiments show that Janus surpasses previous unified model\nand matches or exceeds the performance of task-specific models. The simplicity, high flexibility, ",
      " Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation. Janus is a novel autoregressive framework that unifies multimodal understanding and generation. It addresses the limitations of previous approaches by decoupling visual encoding into separate pathways, while still utilizing a single, unified transformer architecture for processing. ",
      " Introduction to Janus-Pro. Janus-Pro is a unified multimodal model, meaning it supports both understanding and generation tasks within a single framework. Unlike previous approaches that use a shared encoder for both tasks (which causes interference), It decouples visual encoding into separate components for each task."
    ],
    "response": "Janus's architecture differs from previous unified multimodal models by decoupling visual encoding into separate pathways for understanding and generation tasks. This is in contrast to prior approaches that use a single visual encoder for both tasks. The decoupling enhances performance and flexibility by allowing each task to select its most suitable encoding methods.",
    "reference": "Previous models often use a single visual encoder for both understanding and generation tasks. In contrast, Janus decouples visual encoding. It uses the SigLIP encoder for multimodal understanding to extract high - dimensional semantic features and a VQ tokenizer for visual generation to convert images into discrete IDs, with separate adaptors for each task."
  },
  {
    "user_input": "What are the three training stages of Janus?",
    "retrieved_contexts": [
      " Gen. Encoder\nImageHead\nGen. AdaptorStageI:Training AdaptorsandImageHead\nLLM\nUnd.Adaptor\n🔥\n🔥\n🔥\nUnderstanding\nTextHead\nUnd. Encoder\n❄\n❄\n❄ImageGenerationStageII: UnifiedPretrainingStageIII:SupervisedFine-tuning\nGen. Encoder\nImageHead\nGen. Adaptor\nLLM\nUnd.Adaptor\n🔥\n🔥\n🔥\nUnderstanding\nTextHead\nUnd. Encoder\n❄\n❄\n❄ImageGeneration\n🔥\n🔥\nGen. Encoder\nImageHead\nGen. Adaptor\nLLM\nUnd.Adaptor\n🔥\n🔥\n🔥\nUnderstanding\nTextHead\nUnd. EncoderImageGeneration\n🔥\n🔥\n🔥\n❄Figure 3|Our Janus adopts a three-stage training procedure. We use flame symbols/snowflake\nsymbols in the diagram to indicate the module updates/does not update its parameters.\nattention masks.\n3.2. Training Procedure\nThe training of Janus is divided into three stages, as illustrated in Figure 3. Details are provided\nin the below.\nStage I: Training Adaptors and Image Head. The main goal of this stage is to create a conceptual\nconnection between visual and linguistic elements within the embedding space, enabling the\nLLM to understand the entities shown in images and have preliminary visual generation\nability. We keep the visual encoders and the LLM frozen during this stage, allowing only the\ntrainable parameters within the understanding adaptor, generation adaptor and image head to\nbe updated.\nStage II: Unified Pretraining. In this stage, we perform unified pretraining with multimodal\ncorpus to enable Janus to learn both multimodal understanding and generation. We unfreeze the ",
      " Janus-Pro's training is divided into three stages, each designed to refine its capabilities: Learning visual basics: The model begins by training on datasets like ImageNet, focusing on recognizing objects, text, and visual patterns. This stage was extended in Janus-Pro, giving the model more time to model pixel dependencies and process visual ... ",
      " Source: google Introduction. Multimodal artificial intelligence, which integrates vision and language tasks, has advanced rapidly. Janus-Pro is a cutting-edge framework that builds on its predecessor, Janus, through key innovations like decoupled visual encoders for understanding and generation, an optimized three-stage training pipeline, and a robust architecture leveraging synthetic data."
    ],
    "response": "The three training stages of Janus are:\n1. Training Adaptors and Image Head.\n2. Unified Pretraining.\n3. Supervised Fine-tuning.",
    "reference": "The first stage is training adaptors and the image head, where only the understanding adaptor, generation adaptor, and image head are updated while keeping the visual encoders and LLM frozen. The second stage is unified pretraining with multimodal corpus, including using ImageNet - 1k for simple visual generation training first and then general text - to - image data. The third stage is supervised fine - tuning with instruction tuning data, fine - tuning all parameters except the generation encoder."
  },
  {
    "user_input": "What is the training objective of Janus?",
    "retrieved_contexts": [
      " corpus to enable Janus to learn both multimodal understanding and generation. We unfreeze the\nLLM and utilize all types of training data: pure text data, multimodal understanding data, and\nvisual generation data. Inspired by Pixart [ 9], we begin by conducting simple visual generation\ntraining using ImageNet- 1k to help the model grasp basic pixel dependencies. Subsequently, we\nenhance the model’s open-domain visual generation capability with general text-to-image data.\nStage III: Supervised Fine-tuning. During this stage, we fine-tune the pretrained model with\ninstruction tuning data to enhance its instruction-following and dialogue capabilities. We\nfine-tune all parameters except the generation encoder. We focus on supervising the answers\nwhile masking system and user prompts. To ensure Janus’s proficiency in both multimodal\nunderstanding and generation, we don’t fine-tune separate models for a certain task. Instead,\nwe use a blend of pure text dialogue data, multimodal understanding data and visual generation\ndata, ensuring versatility across various scenarios.\n3.3. Training Objective\nJanus is an autoregressive model, and we simply adopt the cross-entropy loss during training:\nL=−∑︁\n𝑖=1log𝑃𝜃(𝑥𝑖|𝑥<𝑖) (1)\nHere,𝑃(·|·) indicates the conditional probability modeled by the weights 𝜃of Janus. For\npure text understanding and multimodal understanding tasks, we compute the loss on the text\n5 ",
      " Identifying and Measuring Training Objectives in a Janus Battle Simulation Center Mary T. Dzindolet, Linda G Pierce, and Debora R. Gleason View all authors and affiliations ",
      " Gen. Encoder\nImageHead\nGen. AdaptorStageI:Training AdaptorsandImageHead\nLLM\nUnd.Adaptor\n🔥\n🔥\n🔥\nUnderstanding\nTextHead\nUnd. Encoder\n❄\n❄\n❄ImageGenerationStageII: UnifiedPretrainingStageIII:SupervisedFine-tuning\nGen. Encoder\nImageHead\nGen. Adaptor\nLLM\nUnd.Adaptor\n🔥\n🔥\n🔥\nUnderstanding\nTextHead\nUnd. Encoder\n❄\n❄\n❄ImageGeneration\n🔥\n🔥\nGen. Encoder\nImageHead\nGen. Adaptor\nLLM\nUnd.Adaptor\n🔥\n🔥\n🔥\nUnderstanding\nTextHead\nUnd. EncoderImageGeneration\n🔥\n🔥\n🔥\n❄Figure 3|Our Janus adopts a three-stage training procedure. We use flame symbols/snowflake\nsymbols in the diagram to indicate the module updates/does not update its parameters.\nattention masks.\n3.2. Training Procedure\nThe training of Janus is divided into three stages, as illustrated in Figure 3. Details are provided\nin the below.\nStage I: Training Adaptors and Image Head. The main goal of this stage is to create a conceptual\nconnection between visual and linguistic elements within the embedding space, enabling the\nLLM to understand the entities shown in images and have preliminary visual generation\nability. We keep the visual encoders and the LLM frozen during this stage, allowing only the\ntrainable parameters within the understanding adaptor, generation adaptor and image head to\nbe updated.\nStage II: Unified Pretraining. In this stage, we perform unified pretraining with multimodal\ncorpus to enable Janus to learn both multimodal understanding and generation. We unfreeze the"
    ],
    "response": "The training objective of Janus is to use a cross-entropy loss during training. This enables the model to predict the next element in a sequence given the previous elements, ensuring proficiency in both multimodal understanding and generation.",
    "reference": "Janus is an autoregressive model and uses the cross - entropy loss during training. The loss is calculated as $\\mathcal{L}=-\\sum_{i = 1} \\log P_{\\theta}(x_{i}"
  },
  {
    "user_input": "How does Janus perform inference for image generation?",
    "retrieved_contexts": [
      " Auto-RegressiveTransformer\nUnd.Encoder\nTextTokenizer\nGen.Encoder\nTextDe-Tokenizer\nTextTokenizer\nImageDecoder\nImage:X!LanguageInstruct:X\"LanguageInstruct:X\"Image:X!LanguageResponse:X#GeneratedImage:X!UnderstandingImageGeneration\n……\n……Figure 2|Architecture of our Janus. Different from previous approaches [ 77,85] that typically\nassume visual understanding and generation require the same visual encoder, our Janus de-\ncouples visual encoding for visual understanding and visual generation. “Und. Encoder” and\n“Gen. Encoder” are abbreviations for “Understanding Encoder” and “Generation Encoder”,\nrespectively. Best viewed in color.\nand generation tasks, regardless of whether they are based on autoregressive (AR) models\n[77,85] or diffusion models [ 86,94]. For example, Chameleon [ 77] adopts a VQ Tokenizer to\nencode images for both multimodal understanding and generation. However, this practice\nmay lead to suboptimal outcomes, as the vision encoder might face a trade-off between the\ndemands of understanding and generation. In contrast, our Janus can explicitly decouple the\nvisual representations for understanding and generation, recognizing that different tasks may\nrequire varying levels of information.\n3. Janus: A Simple, Unified and Flexible Multimodal Framework\n3.1. Architecture\nThe architecture of Janus is shown in Figure 2. For pure text understanding, multimodal\nunderstanding, and visual generation, we apply independent encoding methods to convert the ",
      " For multimodal understanding, it uses the SigLIP-L as the vision encoder, which supports 384 x 384 image input. For image generation, Janus-Pro uses the tokenizer from here with a downsample rate of 16. 3. Quick Start Please refer to Github Repository. 4. License This code repository is licensed under the MIT License. ",
      " corpus to enable Janus to learn both multimodal understanding and generation. We unfreeze the\nLLM and utilize all types of training data: pure text data, multimodal understanding data, and\nvisual generation data. Inspired by Pixart [ 9], we begin by conducting simple visual generation\ntraining using ImageNet- 1k to help the model grasp basic pixel dependencies. Subsequently, we\nenhance the model’s open-domain visual generation capability with general text-to-image data.\nStage III: Supervised Fine-tuning. During this stage, we fine-tune the pretrained model with\ninstruction tuning data to enhance its instruction-following and dialogue capabilities. We\nfine-tune all parameters except the generation encoder. We focus on supervising the answers\nwhile masking system and user prompts. To ensure Janus’s proficiency in both multimodal\nunderstanding and generation, we don’t fine-tune separate models for a certain task. Instead,\nwe use a blend of pure text dialogue data, multimodal understanding data and visual generation\ndata, ensuring versatility across various scenarios.\n3.3. Training Objective\nJanus is an autoregressive model, and we simply adopt the cross-entropy loss during training:\nL=−∑︁\n𝑖=1log𝑃𝜃(𝑥𝑖|𝑥<𝑖) (1)\nHere,𝑃(·|·) indicates the conditional probability modeled by the weights 𝜃of Janus. For\npure text understanding and multimodal understanding tasks, we compute the loss on the text\n5"
    ],
    "response": "Janus performs inference for image generation by using a tokenizer with a downsample rate of 16 and a dedicated generation encoder to decouple visual encoding for generation from understanding. This approach allows for optimized visual representations specific to each task. Furthermore, Janus employs an autoregressive model trained with cross-entropy loss.",
    "reference": "For image generation, Janus utilizes classifier - free guidance (CFG). Specifically, for each token, the logit  is calculated as , where  is the conditional logit,  is the unconditional logit, and  is the scale for the classifier - free guidance (default  for evaluation)."
  },
  {
    "user_input": "What datasets are used in Janus's training?",
    "retrieved_contexts": [
      " Table 1|Detailed hyperparameters of our Janus . Data ratio refers to the ratio of multimodal\nunderstanding data, pure text data, and visual generation data.\nHyperparameters Stage 1 Stage 2 Stage 3\nLearning rate 1.0 ×10−31×10−42.0×10−5\nLR scheduler Cosine Constant Constant\nWeight decay 0.0 0.0 0.1\nGradient clip 1.0 1.0 1.0\nOptimizer AdamW ( 𝛽1=0.9,𝛽2=0.95)\nWarm-up steps 300 5, 000 0\nTraining steps 10, 000 180, 000 24, 000\nBatch size 256 512 256\nData Ratio 1 : 0 : 1 2 : 3 : 5 7 : 3 : 10\n4.1. Implementation Details\nIn our experiments, we utilize DeepSeek-LLM ( 1.3B) [5] with a maximum supported sequence\nlength of 4096 as the base language model. For the vision encoder used in understanding tasks,\nwe select SigLIP-Large-Patch 16-384[92]. The generation encoder has a codebook of size 16, 384\nand downsamples images by a factor of 16. Both the understanding adaptor and the generation\nadaptor are two-layer MLPs. The detailed hyperparameters for each stage are provided in\nTable 1. All images are resized to 384×384pixels. For multimodal understanding data, we\nresize the long side of the image and pad the short side with the background color (RGB: 127,\n127,127) to reach 384. For visual generation data, the short side is resized to 384, and the long\nside is cropped to 384. We use sequence packing during training to improve training efficiency.\nWe mix all data types according to the specified ratios in a single training step. Our Janus is ",
      " corpus to enable Janus to learn both multimodal understanding and generation. We unfreeze the\nLLM and utilize all types of training data: pure text data, multimodal understanding data, and\nvisual generation data. Inspired by Pixart [ 9], we begin by conducting simple visual generation\ntraining using ImageNet- 1k to help the model grasp basic pixel dependencies. Subsequently, we\nenhance the model’s open-domain visual generation capability with general text-to-image data.\nStage III: Supervised Fine-tuning. During this stage, we fine-tune the pretrained model with\ninstruction tuning data to enhance its instruction-following and dialogue capabilities. We\nfine-tune all parameters except the generation encoder. We focus on supervising the answers\nwhile masking system and user prompts. To ensure Janus’s proficiency in both multimodal\nunderstanding and generation, we don’t fine-tune separate models for a certain task. Instead,\nwe use a blend of pure text dialogue data, multimodal understanding data and visual generation\ndata, ensuring versatility across various scenarios.\n3.3. Training Objective\nJanus is an autoregressive model, and we simply adopt the cross-entropy loss during training:\nL=−∑︁\n𝑖=1log𝑃𝜃(𝑥𝑖|𝑥<𝑖) (1)\nHere,𝑃(·|·) indicates the conditional probability modeled by the weights 𝜃of Janus. For\npure text understanding and multimodal understanding tasks, we compute the loss on the text\n5 ",
      " Janus-Pro is DeepSeek's newest multimodal AI model, designed to handle tasks involving both text and images. It introduces several improvements over the original Janus model, including better training strategies, larger datasets, and scaled model sizes (available in 1B and 7B parameter versions). Janus vs. Janus Pro-7b."
    ],
    "response": "Janus's training utilizes pure text data, multimodal understanding data, and visual generation data.",
    "reference": "In stage I, it uses 1.25 million image - text paired captions from ShareGPT4V for multimodal understanding and approximately 1.2 million samples from ImageNet - 1k for visual generation. In stage II, it uses text - only data from DeepSeek - LLM, interleaved image - text data from WikiHow and WIT, image caption data from multiple sources, table and chart data from DeepSeek - VL, and visual generation data from various datasets. In stage III, it uses data from different sources for text understanding, multimodal understanding, and visual generation, with specific formatting for instruction tuning."
  },
  {
    "user_input": "How does Janus compare with previous unified models in terms of performance?",
    "retrieved_contexts": [
      " techniques specific to their domain. Moreover, it is possible for Janus to accommodate additional\ninput types in the future, such as point clouds, EEG signals, or audio data, where independent\nencoders can extract features and then use a unified transformer to process them.\nTo the best of our knowledge, we are the first to highlight the importance of decoupling\nvisual encoding within the unified multimodal understanding and generation framework.\nOur experimental results show that Janus surpasses existing unified models with comparable\nparameter sizes on both multimodal understanding and generation benchmarks, achieving\nstate-of-the-art results. Notably, Janus even outperforms some task-specific models which have\nsignificantly more parameters (Figure 1). Specifically, on multimodal understanding benchmarks\nMMBench [ 54], SEED-Bench [ 42], and POPE [ 48], Janus ( 1.3B) achieved scores of 69.4,63.7, and\n87.0, respectively, outperforming LLaVA-v 1.5(7B) [50] and Qwen-VL-Chat ( 7B) [3] . On visual\ngeneration benchmarks MSCOCO- 30K [11] and GenEval [ 30], Janus achieved an FID score of\n8.53 and an accuracy of 61%, surpassing text-to-image generative models such as DALL-E 2[66]\nand SDXL [ 62]. We believe that the strong performance, coupled with the high flexibility and\nextensibility of Janus, presents it as a strong candidate for next-generation unified multimodal\nmodels.\n2. Related Work\n2.1. Visual Generation ",
      " results among models of similar scale. Specifically, compared to the previous best unified model,\nShow-o [ 86], we achieve performance improvements of 41% (949→1338 ) and 30% (48.7→59.1)\non the MME and GQA datasets, respectively. This can be attributed to Janus decoupling the\nvisual encoding for multimodal understanding and generation, mitigating the conflict between\nthese two tasks. When compared to models with significantly larger sizes, Janus remains highly\ncompetitive. For instance, Janus outperforms LLaVA-v 1.5(7B) on several datasets, including\nPOPE, MMbench, SEED Bench, and MM-Vet.\nVisual Generation Performance. We report visual generation performance on GenEval, COCO-\n30K and MJHQ- 30K benchmarks. As shown in Table 3, our Janus obtains 61% overall accuracy\non GenEval, which outperforms the previous best unified model Show-o ( 53%) and some\npopular generation-only methods, e.g., SDXL ( 55%) and DALL-E 2(52%). This demonstrates\nthat our approach has better instruction-following capabilities. As shown in Table 4, Janus\nachieves FIDs of 8.53 and 10.10 on the COCO- 30K and MJHQ- 30K benchmarks, respectively,\nsurpassing unified models Show-o and LWM, and demonstrating competitive performance\ncompared to some well-known generation-only methods. This demonstrates that the images\ngenerated by Janus have good quality and highlights its potential in visual generation.\n4.5. Ablation Studies ",
      " Janus: Decoupling Visual Encoding for Unified\nMultimodal Understanding and Generation\nChengyue Wu1,2Xiaokang Chen1,∗,†Zhiyu Wu1,3Yiyang Ma1,3Xingchao Liu1Zizheng Pan1\nWen Liu1Zhenda Xie1Xingkai Yu1Chong Ruan1Ping Luo2,∗\n1DeepSeek-AI2The University of Hong Kong3Peking University\n†: Project lead∗: Corresponding authors\nProject Page: https://github.com/deepseek-ai/Janus\nAbstract\nIn this paper, we introduce Janus , an autoregressive framework that unifies multimodal under-\nstanding and generation. Prior research often relies on a single visual encoder for both tasks,\nsuch as Chameleon. However, due to the differing levels of information granularity required by\nmultimodal understanding and generation, this approach can lead to suboptimal performance,\nparticularly in multimodal understanding. To address this issue, we decouple visual encoding\ninto separate pathways, while still leveraging a single, unified transformer architecture for\nprocessing. The decoupling not only alleviates the conflict between the visual encoder’s roles in\nunderstanding and generation, but also enhances the framework’s flexibility. For instance, both\nthe multimodal understanding and generation components can independently select their most\nsuitable encoding methods. Experiments show that Janus surpasses previous unified model\nand matches or exceeds the performance of task-specific models. The simplicity, high flexibility,"
    ],
    "response": "Janus surpasses previous unified models in terms of performance on both multimodal understanding and generation benchmarks. It achieves state-of-the-art results, outperforming some larger models and generation-specific methods. Janus' decoupling of visual encoding enhances its performance significantly.",
    "reference": "Janus outperforms previous unified models on both multimodal understanding and generation benchmarks. On multimodal understanding benchmarks like MMBench, SEED - Bench, and POPE, Janus (1.3B) achieves higher scores than models such as LLaVA - v1.5 (7B) and Qwen - VL - Chat (7B). In visual generation, it has better results than some well - known models like DALL - E 2 and SDXL on benchmarks like MSCOCO - 30K and GenEval."
  },
  {
    "user_input": "What are the possible extensions of Janus in multimodal understanding?",
    "retrieved_contexts": [
      " Janus: Decoupling Visual Encoding for Unified\nMultimodal Understanding and Generation\nChengyue Wu1,2Xiaokang Chen1,∗,†Zhiyu Wu1,3Yiyang Ma1,3Xingchao Liu1Zizheng Pan1\nWen Liu1Zhenda Xie1Xingkai Yu1Chong Ruan1Ping Luo2,∗\n1DeepSeek-AI2The University of Hong Kong3Peking University\n†: Project lead∗: Corresponding authors\nProject Page: https://github.com/deepseek-ai/Janus\nAbstract\nIn this paper, we introduce Janus , an autoregressive framework that unifies multimodal under-\nstanding and generation. Prior research often relies on a single visual encoder for both tasks,\nsuch as Chameleon. However, due to the differing levels of information granularity required by\nmultimodal understanding and generation, this approach can lead to suboptimal performance,\nparticularly in multimodal understanding. To address this issue, we decouple visual encoding\ninto separate pathways, while still leveraging a single, unified transformer architecture for\nprocessing. The decoupling not only alleviates the conflict between the visual encoder’s roles in\nunderstanding and generation, but also enhances the framework’s flexibility. For instance, both\nthe multimodal understanding and generation components can independently select their most\nsuitable encoding methods. Experiments show that Janus surpasses previous unified model\nand matches or exceeds the performance of task-specific models. The simplicity, high flexibility, ",
      " techniques specific to their domain. Moreover, it is possible for Janus to accommodate additional\ninput types in the future, such as point clouds, EEG signals, or audio data, where independent\nencoders can extract features and then use a unified transformer to process them.\nTo the best of our knowledge, we are the first to highlight the importance of decoupling\nvisual encoding within the unified multimodal understanding and generation framework.\nOur experimental results show that Janus surpasses existing unified models with comparable\nparameter sizes on both multimodal understanding and generation benchmarks, achieving\nstate-of-the-art results. Notably, Janus even outperforms some task-specific models which have\nsignificantly more parameters (Figure 1). Specifically, on multimodal understanding benchmarks\nMMBench [ 54], SEED-Bench [ 42], and POPE [ 48], Janus ( 1.3B) achieved scores of 69.4,63.7, and\n87.0, respectively, outperforming LLaVA-v 1.5(7B) [50] and Qwen-VL-Chat ( 7B) [3] . On visual\ngeneration benchmarks MSCOCO- 30K [11] and GenEval [ 30], Janus achieved an FID score of\n8.53 and an accuracy of 61%, surpassing text-to-image generative models such as DALL-E 2[66]\nand SDXL [ 62]. We believe that the strong performance, coupled with the high flexibility and\nextensibility of Janus, presents it as a strong candidate for next-generation unified multimodal\nmodels.\n2. Related Work\n2.1. Visual Generation ",
      " demands that understanding and generation place on the visual encoder. Extensive experi-\nments have demonstrated the effectiveness and leading performance of Janus. It is also worth\nnoting that Janus is flexible and easy to extend. In addition to having significant potential for\nimprovement in both multimodal understanding and generation, Janus is also easily extendable\nto incorporate more input modalities. The above advantages suggest that Janus may serve as an\ninspiration for the development of the next generation of multimodal general-purpose models.\n13"
    ],
    "response": "Janus can be extended to accommodate additional input types such as point clouds, EEG signals, or audio data. Independent encoders can be added to extract features from these modalities, which can then be processed by the unified transformer architecture. This flexibility highlights Janus's potential in further advancing multimodal understanding capabilities.",
    "reference": "In multimodal understanding, Janus can use stronger vision encoders like EVA - CLIP or InternViT. It can also use dynamic high - resolution techniques to handle high - resolution images and compress tokens using operations like pixel shuffle to save computational cost."
  },
  {
    "user_input": "What are the possible extensions of Janus in visual generation?",
    "retrieved_contexts": [
      " techniques specific to their domain. Moreover, it is possible for Janus to accommodate additional\ninput types in the future, such as point clouds, EEG signals, or audio data, where independent\nencoders can extract features and then use a unified transformer to process them.\nTo the best of our knowledge, we are the first to highlight the importance of decoupling\nvisual encoding within the unified multimodal understanding and generation framework.\nOur experimental results show that Janus surpasses existing unified models with comparable\nparameter sizes on both multimodal understanding and generation benchmarks, achieving\nstate-of-the-art results. Notably, Janus even outperforms some task-specific models which have\nsignificantly more parameters (Figure 1). Specifically, on multimodal understanding benchmarks\nMMBench [ 54], SEED-Bench [ 42], and POPE [ 48], Janus ( 1.3B) achieved scores of 69.4,63.7, and\n87.0, respectively, outperforming LLaVA-v 1.5(7B) [50] and Qwen-VL-Chat ( 7B) [3] . On visual\ngeneration benchmarks MSCOCO- 30K [11] and GenEval [ 30], Janus achieved an FID score of\n8.53 and an accuracy of 61%, surpassing text-to-image generative models such as DALL-E 2[66]\nand SDXL [ 62]. We believe that the strong performance, coupled with the high flexibility and\nextensibility of Janus, presents it as a strong candidate for next-generation unified multimodal\nmodels.\n2. Related Work\n2.1. Visual Generation ",
      " demands that understanding and generation place on the visual encoder. Extensive experi-\nments have demonstrated the effectiveness and leading performance of Janus. It is also worth\nnoting that Janus is flexible and easy to extend. In addition to having significant potential for\nimprovement in both multimodal understanding and generation, Janus is also easily extendable\nto incorporate more input modalities. The above advantages suggest that Janus may serve as an\ninspiration for the development of the next generation of multimodal general-purpose models.\n13 ",
      " Auto-RegressiveTransformer\nUnd.Encoder\nTextTokenizer\nGen.Encoder\nTextDe-Tokenizer\nTextTokenizer\nImageDecoder\nImage:X!LanguageInstruct:X\"LanguageInstruct:X\"Image:X!LanguageResponse:X#GeneratedImage:X!UnderstandingImageGeneration\n……\n……Figure 2|Architecture of our Janus. Different from previous approaches [ 77,85] that typically\nassume visual understanding and generation require the same visual encoder, our Janus de-\ncouples visual encoding for visual understanding and visual generation. “Und. Encoder” and\n“Gen. Encoder” are abbreviations for “Understanding Encoder” and “Generation Encoder”,\nrespectively. Best viewed in color.\nand generation tasks, regardless of whether they are based on autoregressive (AR) models\n[77,85] or diffusion models [ 86,94]. For example, Chameleon [ 77] adopts a VQ Tokenizer to\nencode images for both multimodal understanding and generation. However, this practice\nmay lead to suboptimal outcomes, as the vision encoder might face a trade-off between the\ndemands of understanding and generation. In contrast, our Janus can explicitly decouple the\nvisual representations for understanding and generation, recognizing that different tasks may\nrequire varying levels of information.\n3. Janus: A Simple, Unified and Flexible Multimodal Framework\n3.1. Architecture\nThe architecture of Janus is shown in Figure 2. For pure text understanding, multimodal\nunderstanding, and visual generation, we apply independent encoding methods to convert the"
    ],
    "response": "The possible extensions of Janus in visual generation include accommodating additional input types such as point clouds, EEG signals, or audio data. Janus is highly flexible and easily extendable, suggesting it has significant potential for incorporating more input modalities. This versatility presents Janus as a strong candidate for next-generation multimodal models.",
    "reference": "In visual generation, Janus can choose finer - grained encoders like MoVQGan to preserve more image details. It can employ loss functions specifically designed for visual generation, such as diffusion loss. Also, a combination of AR (causal attention) and parallel (bidirectional attention) methods can be used to reduce accumulated errors during visual generation."
  },
  {
    "user_input": "Why is Janus considered a strong candidate for next - generation unified multimodal models?",
    "retrieved_contexts": [
      " The simplicity, high flexibility, and effectiveness of Janus make it a strong candidate for next-generation unified multimodal models. JanusFlow introduces a minimalist architecture that integrates autoregressive language models with rectified flow, a state-of-the-art method in generative modeling. Our key finding demonstrates that rectified ... ",
      " techniques specific to their domain. Moreover, it is possible for Janus to accommodate additional\ninput types in the future, such as point clouds, EEG signals, or audio data, where independent\nencoders can extract features and then use a unified transformer to process them.\nTo the best of our knowledge, we are the first to highlight the importance of decoupling\nvisual encoding within the unified multimodal understanding and generation framework.\nOur experimental results show that Janus surpasses existing unified models with comparable\nparameter sizes on both multimodal understanding and generation benchmarks, achieving\nstate-of-the-art results. Notably, Janus even outperforms some task-specific models which have\nsignificantly more parameters (Figure 1). Specifically, on multimodal understanding benchmarks\nMMBench [ 54], SEED-Bench [ 42], and POPE [ 48], Janus ( 1.3B) achieved scores of 69.4,63.7, and\n87.0, respectively, outperforming LLaVA-v 1.5(7B) [50] and Qwen-VL-Chat ( 7B) [3] . On visual\ngeneration benchmarks MSCOCO- 30K [11] and GenEval [ 30], Janus achieved an FID score of\n8.53 and an accuracy of 61%, surpassing text-to-image generative models such as DALL-E 2[66]\nand SDXL [ 62]. We believe that the strong performance, coupled with the high flexibility and\nextensibility of Janus, presents it as a strong candidate for next-generation unified multimodal\nmodels.\n2. Related Work\n2.1. Visual Generation ",
      " Janus-Pro surpasses previous unified model and matches or exceeds the performance of task-specific models. The simplicity, high flexibility, and effectiveness of Janus-Pro make it a strong candidate for next-generation unified multimodal models. Github Repository"
    ],
    "response": "Janus is considered a strong candidate for next-generation unified multimodal models due to its simplicity, high flexibility, and effectiveness. It integrates autoregressive language models with state-of-the-art generative modeling techniques, and outperforms existing unified models and some task-specific models on both understanding and generation benchmarks. Janus's capability to accommodate various input types and its decoupled visual pathway further enhance its performance and extensibility.",
    "reference": "Janus is considered a strong candidate because it has a simple, unified, and flexible architecture. It decouples visual encoding, which leads to high performance on both multimodal understanding and generation tasks, even outperforming some task - specific models. It is also easy to extend, with the potential to incorporate more input modalities, such as 3D point cloud, tactile, and EEG data."
  },
  {
    "user_input": "What is the main innovation of Kimi k1.5?",
    "retrieved_contexts": [
      " KIMI K 1.5:\nSCALING REINFORCEMENT LEARNING WITH LLM S\nTECHNICAL REPORT OF KIMI K 1.5\nKimi Team\nABSTRACT\nLanguage model pretraining with next token prediction has proved effective for scaling compute but\nis limited to the amount of available training data. Scaling reinforcement learning (RL) unlocks a new\naxis for the continued improvement of artificial intelligence, with the promise that large language\nmodels (LLMs) can scale their training data by learning to explore with rewards. However, prior\npublished work has not produced competitive results. In light of this, we report on the training practice\nof Kimi k1.5, our latest multi-modal LLM trained with RL, including its RL training techniques,\nmulti-modal data recipes, and infrastructure optimization. Long context scaling and improved policy\noptimization methods are key ingredients of our approach, which establishes a simplistic, effective\nRL framework without relying on more complex techniques such as Monte Carlo tree search, value\nfunctions, and process reward models. Notably, our system achieves state-of-the-art reasoning\nperformance across multiple benchmarks and modalities—e.g., 77.5 on AIME, 96.2 on MATH\n500, 94-th percentile on Codeforces, 74.9 on MathVista—matching OpenAI’s o1. Moreover, we\npresent effective long2short methods that use long-CoT techniques to improve short-CoT models,\nyielding state-of-the-art short-CoT reasoning results—e.g., 60.8 on AIME, 94.6 on MATH500, 47.3 ",
      " Kimi Team. Full Report. 🚀 Introducing Kimi k1.5 --- an o1-level multi-modal model. Sota short-CoT performance, outperforming GPT-4o and Claude Sonnet 3.5 on 📐AIME, 📐MATH-500, 💻 LiveCodeBench by a large margin (up to +550%) Long-CoT performance matches o1 across multiple modalities (👀MathVista, 📐AIME, 💻Codeforces, etc) ",
      " Kimi k1.5 TECHNICAL REPORT\n2.6.1 Large Scale Reinforcement Learning Training System for LLM\nIn the realm of artificial intelligence, reinforcement learning (RL) has emerged as a pivotal training methodology\nfor large language models (LLMs)(Ouyang et al. 2022)(Jaech et al. 2024), drawing inspiration from its success in\nmastering complex games like Go, StarCraft II, and Dota 2 through systems such as AlphaGo(Silver et al. 2017),\nAlphaStar(Vinyals et al. 2019), and OpenAI Dota Five (Berner et al. 2019). Following in this tradition, the Kimi\nk1.5 system adopts an iterative synchronous RL framework, meticulously designed to bolster the model’s reasoning\ncapabilities through persistent learning and adaptation. A key innovation in this system is the introduction of a Partial\nRollout technique, designed to optimize the handling of complex reasoning trajectories.\nThe RL training system as illustrated in Figure 3a operates through an iterative synchronous approach, with each\niteration encompassing a rollout phase and a training phase. During the rollout phase, rollout workers, coordinated\nby a central master, generate rollout trajectories by interacting with the model, producing sequences of responses to\nvarious inputs. These trajectories are then stored in a replay buffer, which ensures a diverse and unbiased dataset for\ntraining by disrupting temporal correlations. In the subsequent training phase, trainer workers access these experiences"
    ],
    "response": "The main innovation of Kimi k1.5 is its effective scaling of reinforcement learning (RL) training for large language models (LLMs). It introduces an iterative synchronous RL framework with a Partial Rollout technique to optimize handling complex reasoning trajectories. This approach enables state-of-the-art reasoning performance across multiple benchmarks and modalities without relying on more complex techniques.",
    "reference": "The main innovation is scaling reinforcement learning (RL) with LLMs by introducing long context scaling (scaling the RL context window to 128k) and improved policy optimization methods. It also presents long2short methods to improve short-CoT models, achieving state-of-the-art reasoning performance across multiple benchmarks and modalities without relying on complex techniques like Monte Carlo tree search."
  },
  {
    "user_input": "How does Kimi k1.5 perform in different benchmarks compared to other models?",
    "retrieved_contexts": [
      " Kimi k1.5 TECHNICAL REPORT\nKimi k1.5 short-CoTOpenAI 4oClaude 3.5 Sonnet DeepSeek V3LLaMA-3.1 405B-Inst.\nGeneralCodeMath\n47.333.4\nLiveCodeBench v4 24.08-24.11 (Pass@1-COT)\n36.328.440.531.1Qwen2.5 72B-Inst.\n87.487.2\nMMLU (EM)\n88.388.688.585.3\n94.674.6\nMATH-500 (EM)\n78.373.890.280\n60.8\n9.3AIME 2024 (Pass@1)\n1623.339.223.3\n6869.1\nMMMU_val(Pass@1)\n66.464.5\n70.163.8\nMathVista_test(Pass@1)\n65.369.7\n87.284.3\nIF-Eval (Prompt Strict)\n86.58686.184.1\n91.787.9\nCLUEWSC (EM)\n85.484.790.991.4\n88.376\nC-Eval (EM)\n76.761.586.586.1Qwen2-VLVision\nFigure 2: Kimi k1.5 short-CoT results.\n1 Introduction\nLanguage model pretraining with next token prediction has been studied under the context of the scaling law, where\nproportionally scaling model parameters and data sizes leads to the continued improvement of intelligence. (Kaplan\net al. 2020; Hoffmann et al. 2022) However, this approach is limited to the amount of available high-quality training\ndata (Villalobos et al. 2024; Muennighoff et al. 2023). In this report, we present the training recipe of Kimi k1.5,\nour latest multi-modal LLM trained with reinforcement learning (RL). The goal is to explore a possible new axis\nfor continued scaling. Using RL with LLMs, the models learns to explore with rewards and thus is not limited to a\npre-existing static dataset.\nThere are a few key ingredients about the design and training of k1.5.\n•Long context scaling . We scale the context window of RL to 128k and observe continued improvement of ",
      " Kimi k1.5 TECHNICAL REPORT\n3.2 Main Results\nK1.5 long-CoT model The performance of the Kimi k1.5 long-CoT model is presented in Table 2. Through long-CoT\nsupervised fine-tuning (described in Section 2.2) and vision-text joint reinforcement learning (discussed in Section 2.3),\nthe model’s long-term reasoning capabilities are enhanced significantly. The test-time computation scaling further\nstrengthens its performance, enabling the model to achieve state-of-the-art results across a range of modalities. Our\nevaluation reveals marked improvements in the model’s capacity to reason, comprehend, and synthesize information\nover extended contexts, representing a advancement in multi-modal AI capabilities.\nK1.5 short-CoT model The performance of the Kimi k1.5 short-CoT model is presented in Table 3. This model\nintegrates several techniques, including traditional supervised fine-tuning (discussed in Section 2.5.2), reinforcement\nlearning (explored in Section 2.3), and long-to-short distillation (outlined in Section 2.4). The results demonstrate\nthat the k1.5 short-CoT model delivers competitive or superior performance compared to leading open-source and\nproprietary models across multiple tasks. These include text, vision, and reasoning challenges, with notable strengths in\nnatural language understanding, mathematics, coding, and logical reasoning.\nBenchmark (Metric)Language-only Model Vision-Language Model\nQwQ-32B OpenAI QVQ-72B OpenAI Kimi\nPreview o1-mini Preview o1 k1.5 ",
      " KIMI K 1.5:\nSCALING REINFORCEMENT LEARNING WITH LLM S\nTECHNICAL REPORT OF KIMI K 1.5\nKimi Team\nABSTRACT\nLanguage model pretraining with next token prediction has proved effective for scaling compute but\nis limited to the amount of available training data. Scaling reinforcement learning (RL) unlocks a new\naxis for the continued improvement of artificial intelligence, with the promise that large language\nmodels (LLMs) can scale their training data by learning to explore with rewards. However, prior\npublished work has not produced competitive results. In light of this, we report on the training practice\nof Kimi k1.5, our latest multi-modal LLM trained with RL, including its RL training techniques,\nmulti-modal data recipes, and infrastructure optimization. Long context scaling and improved policy\noptimization methods are key ingredients of our approach, which establishes a simplistic, effective\nRL framework without relying on more complex techniques such as Monte Carlo tree search, value\nfunctions, and process reward models. Notably, our system achieves state-of-the-art reasoning\nperformance across multiple benchmarks and modalities—e.g., 77.5 on AIME, 96.2 on MATH\n500, 94-th percentile on Codeforces, 74.9 on MathVista—matching OpenAI’s o1. Moreover, we\npresent effective long2short methods that use long-CoT techniques to improve short-CoT models,\nyielding state-of-the-art short-CoT reasoning results—e.g., 60.8 on AIME, 94.6 on MATH500, 47.3"
    ],
    "response": "Kimi k1.5 performs competitively across various benchmarks, often achieving state-of-the-art results. For example, it scores 77.5 on AIME, 96.2 on MATH 500, and 94-th percentile on Codeforces, demonstrating strong performance in reasoning, mathematics, and coding. It surpasses or matches leading models like OpenAI's offerings in multiple tasks.",
    "reference": "In long-CoT, it achieves 77.5 on AIME, 96.2 on MATH 500, 94-th percentile on Codeforces, 74.9 on MathVista, matching or outperforming models like OpenAI’s o1. In short-CoT, it gets 60.8 on AIME, 94.6 on MATH500, 47.3 on LiveCodeBench, outperforming models such as GPT-4o and Claude Sonnet 3.5 by a large margin (up to +550%)."
  },
  {
    "user_input": "What are the key ingredients of Kimi k1.5's design and training?",
    "retrieved_contexts": [
      " Key Ingredients of Kimi k1.5 Kimi k1.5's design and training revolve around several key ingredients: Long Context Scaling: The researchers scaled Kimi k1.5's context window for RL to 128k and found that performance continued to improve as the context length increased. ",
      " Kimi k1.5 TECHNICAL REPORT\nLong2short RL After a standard RL training phase, we select a model that offers the best balance between perfor-\nmance and token efficiency to serve as the base model, and conduct a separate long2short RL training phase. In this\nsecond phase, we apply the length penalty introduced in Section 2.3.3, and significantly reduce the maximum rollout\nlength to further penalize responses that exceed the desired length while possibly correct.\n2.5 Other Training Details\n2.5.1 Pretraining\nThe Kimi k1.5 base model is trained on a diverse, high-quality multimodal corpus. The language data covers five\ndomains: English, Chinese, Code, Mathematics Reasoning, and Knowledge. Multimodal data, including Captioning,\nImage-text Interleaving, OCR, Knowledge, and QA datasets, enables our model to acquire vision-language capabilities.\nRigorous quality control ensures relevance, diversity, and balance in the overall pretrain dataset. Our pretraining\nproceeds in three stages: (1) Vision-language pretraining, where a strong language foundation is established, followed\nby gradual multimodal integration; (2) Cooldown, which consolidates capabilities using curated and synthetic data,\nparticularly for reasoning and knowledge-based tasks; and (3) Long-context activation, extending sequence processing\nto 131,072 tokens. More details regarding our pretraining efforts can be found in Appendix B.\n2.5.2 Vanilla Supervised Finetuning ",
      " Kimi k1.5 TECHNICAL REPORT\n2 Approach: Reinforcement Learning with LLMs\nThe development of Kimi k1.5 consists of several stages: pretraining, vanilla supervised fine-tuning (SFT), long-CoT\nsupervised fine-turning, and reinforcement learning (RL). This report focuses on RL, beginning with an overview of\nthe RL prompt set curation (Section 2.1) and long-CoT supervised finetuning (Section 2.2), followed by an in-depth\ndiscussion of RL training strategies in Section 2.3. Additional details on pretraining and vanilla supervised finetuning\ncan be found in Section 2.5.\n2.1 RL Prompt Set Curation\nThrough our preliminary experiments, we found that the quality and diversity of the RL prompt set play a critical role in\nensuring the effectiveness of reinforcement learning. A well-constructed prompt set not only guides the model toward\nrobust reasoning but also mitigates the risk of reward hacking and overfitting to superficial patterns. Specifically, three\nkey properties define a high-quality RL prompt set:\n•Diverse Coverage : Prompts should span a wide array of disciplines, such as STEM, coding, and general reasoning,\nto enhance the model’s adaptability and ensure broad applicability across different domains.\n•Balanced Difficulty : The prompt set should include a well-distributed range of easy, moderate, and difficult\nquestions to facilitate gradual learning and prevent overfitting to specific complexity levels."
    ],
    "response": "The key ingredients of Kimi k1.5's design and training include scaling the context window for reinforcement learning to 128k, training on a diverse multimodal corpus covering multiple domains, and rigorous quality control of the datasets. The training is conducted in several stages: pretraining, vanilla supervised fine-tuning, long-CoT supervised fine-tuning, and reinforcement learning. The pretraining features vision-language integration, while long-context activation extends the sequence processing to 131,072 tokens.",
    "reference": "Key ingredients include long context scaling, improved policy optimization (deriving a formulation of RL with long-CoT and using a variant of online mirror descent), a simplistic framework, multimodal training (text and vision data), and effective long2short methods."
  },
  {
    "user_input": "How is the RL prompt set curated for Kimi k1.5?",
    "retrieved_contexts": [
      " Kimi k1.5 TECHNICAL REPORT\n2 Approach: Reinforcement Learning with LLMs\nThe development of Kimi k1.5 consists of several stages: pretraining, vanilla supervised fine-tuning (SFT), long-CoT\nsupervised fine-turning, and reinforcement learning (RL). This report focuses on RL, beginning with an overview of\nthe RL prompt set curation (Section 2.1) and long-CoT supervised finetuning (Section 2.2), followed by an in-depth\ndiscussion of RL training strategies in Section 2.3. Additional details on pretraining and vanilla supervised finetuning\ncan be found in Section 2.5.\n2.1 RL Prompt Set Curation\nThrough our preliminary experiments, we found that the quality and diversity of the RL prompt set play a critical role in\nensuring the effectiveness of reinforcement learning. A well-constructed prompt set not only guides the model toward\nrobust reasoning but also mitigates the risk of reward hacking and overfitting to superficial patterns. Specifically, three\nkey properties define a high-quality RL prompt set:\n•Diverse Coverage : Prompts should span a wide array of disciplines, such as STEM, coding, and general reasoning,\nto enhance the model’s adaptability and ensure broad applicability across different domains.\n•Balanced Difficulty : The prompt set should include a well-distributed range of easy, moderate, and difficult\nquestions to facilitate gradual learning and prevent overfitting to specific complexity levels. ",
      " RL Prompt Set Curation. ... Prior to RL training, Kimi K1.5 undergoes a fine-tuning phase that optimizes its ability to generate long-chain reasoning sequences. This process involves structured prompt engineering, where the model is exposed to carefully curated reasoning paths that improve its ability to plan, evaluate, reflect, and refine ... ",
      " These architectural improvements collectively enable Kimi K1.5 to achieve state-of-the-art reasoning capabilities across diverse benchmarks. RL Framework & Policy Optimization. Kimi K1.5's RL pipeline consists of four stages: RL Prompt Set Curation. A high-quality prompt dataset is essential for stable RL training. It employs:"
    ],
    "response": "The RL prompt set for Kimi k1.5 is curated to ensure quality and diversity, covering a wide range of disciplines and including a balanced difficulty range. This is done to enhance the model's adaptability, guide robust reasoning, and avoid overfitting and reward hacking. The process involves structured prompt engineering and fine-tuning to optimize long-chain reasoning sequences.",
    "reference": "The RL prompt set is curated to have diverse coverage (spanning STEM, coding, general reasoning), balanced difficulty (including easy, moderate, and difficult questions), and accurate evaluability. It uses automatic filters, a tagging system, a model-based difficulty assessment method, and a process to remove easy-to-hack prompts."
  },
  {
    "user_input": "What is the process of long-CoT supervised fine-tuning in Kimi k1.5?",
    "retrieved_contexts": [
      " KIMI K 1.5:\nSCALING REINFORCEMENT LEARNING WITH LLM S\nTECHNICAL REPORT OF KIMI K 1.5\nKimi Team\nABSTRACT\nLanguage model pretraining with next token prediction has proved effective for scaling compute but\nis limited to the amount of available training data. Scaling reinforcement learning (RL) unlocks a new\naxis for the continued improvement of artificial intelligence, with the promise that large language\nmodels (LLMs) can scale their training data by learning to explore with rewards. However, prior\npublished work has not produced competitive results. In light of this, we report on the training practice\nof Kimi k1.5, our latest multi-modal LLM trained with RL, including its RL training techniques,\nmulti-modal data recipes, and infrastructure optimization. Long context scaling and improved policy\noptimization methods are key ingredients of our approach, which establishes a simplistic, effective\nRL framework without relying on more complex techniques such as Monte Carlo tree search, value\nfunctions, and process reward models. Notably, our system achieves state-of-the-art reasoning\nperformance across multiple benchmarks and modalities—e.g., 77.5 on AIME, 96.2 on MATH\n500, 94-th percentile on Codeforces, 74.9 on MathVista—matching OpenAI’s o1. Moreover, we\npresent effective long2short methods that use long-CoT techniques to improve short-CoT models,\nyielding state-of-the-art short-CoT reasoning results—e.g., 60.8 on AIME, 94.6 on MATH500, 47.3 ",
      " Kimi k1.5 TECHNICAL REPORT\n3.2 Main Results\nK1.5 long-CoT model The performance of the Kimi k1.5 long-CoT model is presented in Table 2. Through long-CoT\nsupervised fine-tuning (described in Section 2.2) and vision-text joint reinforcement learning (discussed in Section 2.3),\nthe model’s long-term reasoning capabilities are enhanced significantly. The test-time computation scaling further\nstrengthens its performance, enabling the model to achieve state-of-the-art results across a range of modalities. Our\nevaluation reveals marked improvements in the model’s capacity to reason, comprehend, and synthesize information\nover extended contexts, representing a advancement in multi-modal AI capabilities.\nK1.5 short-CoT model The performance of the Kimi k1.5 short-CoT model is presented in Table 3. This model\nintegrates several techniques, including traditional supervised fine-tuning (discussed in Section 2.5.2), reinforcement\nlearning (explored in Section 2.3), and long-to-short distillation (outlined in Section 2.4). The results demonstrate\nthat the k1.5 short-CoT model delivers competitive or superior performance compared to leading open-source and\nproprietary models across multiple tasks. These include text, vision, and reasoning challenges, with notable strengths in\nnatural language understanding, mathematics, coding, and logical reasoning.\nBenchmark (Metric)Language-only Model Vision-Language Model\nQwQ-32B OpenAI QVQ-72B OpenAI Kimi\nPreview o1-mini Preview o1 k1.5 ",
      " The team overcame these issues in DeepSeek-R1 by supervised fine-tuning on a small number of long CoT examples prior to reinforcement learning. Similarly, the Kimi k1.5 team found that fine-tuning the model on long CoTs prior to reinforcement learning enabled it to devise its own problem-solving strategies."
    ],
    "response": "The process of long-CoT supervised fine-tuning in Kimi k1.5 involves fine-tuning the model on a small number of long CoT examples prior to reinforcement learning. This enhances the model's long-term reasoning capabilities and helps it devise its own problem-solving strategies. This process, combined with vision-text joint reinforcement learning, significantly boosts the model's performance in reasoning over extended contexts.",
    "reference": "A small yet high-quality long-CoT warmup dataset is constructed using prompt engineering. It contains verified reasoning paths for text and image inputs. Then, a lightweight SFT is performed on this dataset to prime the model to internalize reasoning strategies and improve its performance in generating detailed and coherent responses."
  },
  {
    "user_input": "What is the policy optimization algorithm used in Kimi k1.5's RL training?",
    "retrieved_contexts": [
      " Scaling LLMs through next-token prediction is limited by high-quality data availability. Kimi K1.5 overcomes this with an optimized reinforcement learning framework, enhancing long-context reasoning, policy optimization, and multimodal integration. It outperforms GPT-4o, Claude Sonnet 3.5, and OpenAI's o1 across reasoning benchmarks. This article explores its architecture, training methods ... ",
      " Source: Kimik1.5 Techical report Key Components: 1. Policy Optimization A training mechanism that adjusts how the model makes decisions (its \"policy\"). Using online mirror descent means the ... ",
      " Kimi k1.5 TECHNICAL REPORT\n2.6.1 Large Scale Reinforcement Learning Training System for LLM\nIn the realm of artificial intelligence, reinforcement learning (RL) has emerged as a pivotal training methodology\nfor large language models (LLMs)(Ouyang et al. 2022)(Jaech et al. 2024), drawing inspiration from its success in\nmastering complex games like Go, StarCraft II, and Dota 2 through systems such as AlphaGo(Silver et al. 2017),\nAlphaStar(Vinyals et al. 2019), and OpenAI Dota Five (Berner et al. 2019). Following in this tradition, the Kimi\nk1.5 system adopts an iterative synchronous RL framework, meticulously designed to bolster the model’s reasoning\ncapabilities through persistent learning and adaptation. A key innovation in this system is the introduction of a Partial\nRollout technique, designed to optimize the handling of complex reasoning trajectories.\nThe RL training system as illustrated in Figure 3a operates through an iterative synchronous approach, with each\niteration encompassing a rollout phase and a training phase. During the rollout phase, rollout workers, coordinated\nby a central master, generate rollout trajectories by interacting with the model, producing sequences of responses to\nvarious inputs. These trajectories are then stored in a replay buffer, which ensures a diverse and unbiased dataset for\ntraining by disrupting temporal correlations. In the subsequent training phase, trainer workers access these experiences"
    ],
    "response": "The policy optimization algorithm used in Kimi k1.5's RL training is online mirror descent.",
    "reference": "A variant of online policy mirror decent is used. At each iteration, it optimizes a relative entropy regularized policy optimization problem. The surrogate loss is calculated and gradients are taken using samples from the reference policy. It excludes the value network to encourage the model to explore diverse reasoning paths."
  },
  {
    "user_input": "How does Kimi k1.5 address the overthinking phenomenon during RL training?",
    "retrieved_contexts": [
      " Kimi k1.5 TECHNICAL REPORT\n2 Approach: Reinforcement Learning with LLMs\nThe development of Kimi k1.5 consists of several stages: pretraining, vanilla supervised fine-tuning (SFT), long-CoT\nsupervised fine-turning, and reinforcement learning (RL). This report focuses on RL, beginning with an overview of\nthe RL prompt set curation (Section 2.1) and long-CoT supervised finetuning (Section 2.2), followed by an in-depth\ndiscussion of RL training strategies in Section 2.3. Additional details on pretraining and vanilla supervised finetuning\ncan be found in Section 2.5.\n2.1 RL Prompt Set Curation\nThrough our preliminary experiments, we found that the quality and diversity of the RL prompt set play a critical role in\nensuring the effectiveness of reinforcement learning. A well-constructed prompt set not only guides the model toward\nrobust reasoning but also mitigates the risk of reward hacking and overfitting to superficial patterns. Specifically, three\nkey properties define a high-quality RL prompt set:\n•Diverse Coverage : Prompts should span a wide array of disciplines, such as STEM, coding, and general reasoning,\nto enhance the model’s adaptability and ensure broad applicability across different domains.\n•Balanced Difficulty : The prompt set should include a well-distributed range of easy, moderate, and difficult\nquestions to facilitate gradual learning and prevent overfitting to specific complexity levels. ",
      " KIMI K 1.5:\nSCALING REINFORCEMENT LEARNING WITH LLM S\nTECHNICAL REPORT OF KIMI K 1.5\nKimi Team\nABSTRACT\nLanguage model pretraining with next token prediction has proved effective for scaling compute but\nis limited to the amount of available training data. Scaling reinforcement learning (RL) unlocks a new\naxis for the continued improvement of artificial intelligence, with the promise that large language\nmodels (LLMs) can scale their training data by learning to explore with rewards. However, prior\npublished work has not produced competitive results. In light of this, we report on the training practice\nof Kimi k1.5, our latest multi-modal LLM trained with RL, including its RL training techniques,\nmulti-modal data recipes, and infrastructure optimization. Long context scaling and improved policy\noptimization methods are key ingredients of our approach, which establishes a simplistic, effective\nRL framework without relying on more complex techniques such as Monte Carlo tree search, value\nfunctions, and process reward models. Notably, our system achieves state-of-the-art reasoning\nperformance across multiple benchmarks and modalities—e.g., 77.5 on AIME, 96.2 on MATH\n500, 94-th percentile on Codeforces, 74.9 on MathVista—matching OpenAI’s o1. Moreover, we\npresent effective long2short methods that use long-CoT techniques to improve short-CoT models,\nyielding state-of-the-art short-CoT reasoning results—e.g., 60.8 on AIME, 94.6 on MATH500, 47.3 ",
      " Kimi k1.5 TECHNICAL REPORT\n2.6.1 Large Scale Reinforcement Learning Training System for LLM\nIn the realm of artificial intelligence, reinforcement learning (RL) has emerged as a pivotal training methodology\nfor large language models (LLMs)(Ouyang et al. 2022)(Jaech et al. 2024), drawing inspiration from its success in\nmastering complex games like Go, StarCraft II, and Dota 2 through systems such as AlphaGo(Silver et al. 2017),\nAlphaStar(Vinyals et al. 2019), and OpenAI Dota Five (Berner et al. 2019). Following in this tradition, the Kimi\nk1.5 system adopts an iterative synchronous RL framework, meticulously designed to bolster the model’s reasoning\ncapabilities through persistent learning and adaptation. A key innovation in this system is the introduction of a Partial\nRollout technique, designed to optimize the handling of complex reasoning trajectories.\nThe RL training system as illustrated in Figure 3a operates through an iterative synchronous approach, with each\niteration encompassing a rollout phase and a training phase. During the rollout phase, rollout workers, coordinated\nby a central master, generate rollout trajectories by interacting with the model, producing sequences of responses to\nvarious inputs. These trajectories are then stored in a replay buffer, which ensures a diverse and unbiased dataset for\ntraining by disrupting temporal correlations. In the subsequent training phase, trainer workers access these experiences"
    ],
    "response": "Kimi k1.5 addresses the overthinking phenomenon during RL training by curating a high-quality prompt set with diverse coverage and balanced difficulty. This approach ensures the model can adapt across various domains and prevents overfitting to specific levels of complexity. Additionally, the iterative synchronous RL framework and Partial Rollout technique further enhance the model's reasoning capabilities while managing complex reasoning trajectories.",
    "reference": "Kimi k1.5 introduces a length reward to restrain the growth of token length. Given sampled responses, it calculates a length reward based on the response length and correctness. It also gradually warms up the length penalty during training to alleviate potential slowdowns."
  },
  {
    "user_input": "What are the sampling strategies used in Kimi k1.5's RL training?",
    "retrieved_contexts": [
      " This paper mainly introduces the long-CoT supervised fine-tuning and reinforcement learning (RL) used in Kimi k1.5. ... Sampling Strategies Curriculum Sampling: Starting with training simple tasks and gradually developing to more challenging tasks. During data collection, labels for registration and difficulty are included, allowing training to ... ",
      " Kimi k1.5 TECHNICAL REPORT\n2 Approach: Reinforcement Learning with LLMs\nThe development of Kimi k1.5 consists of several stages: pretraining, vanilla supervised fine-tuning (SFT), long-CoT\nsupervised fine-turning, and reinforcement learning (RL). This report focuses on RL, beginning with an overview of\nthe RL prompt set curation (Section 2.1) and long-CoT supervised finetuning (Section 2.2), followed by an in-depth\ndiscussion of RL training strategies in Section 2.3. Additional details on pretraining and vanilla supervised finetuning\ncan be found in Section 2.5.\n2.1 RL Prompt Set Curation\nThrough our preliminary experiments, we found that the quality and diversity of the RL prompt set play a critical role in\nensuring the effectiveness of reinforcement learning. A well-constructed prompt set not only guides the model toward\nrobust reasoning but also mitigates the risk of reward hacking and overfitting to superficial patterns. Specifically, three\nkey properties define a high-quality RL prompt set:\n•Diverse Coverage : Prompts should span a wide array of disciplines, such as STEM, coding, and general reasoning,\nto enhance the model’s adaptability and ensure broad applicability across different domains.\n•Balanced Difficulty : The prompt set should include a well-distributed range of easy, moderate, and difficult\nquestions to facilitate gradual learning and prevent overfitting to specific complexity levels. ",
      " Scaling LLMs through next-token prediction is limited by high-quality data availability. Kimi K1.5 overcomes this with an optimized reinforcement learning framework, enhancing long-context reasoning, policy optimization, and multimodal integration.It outperforms GPT-4o, Claude Sonnet 3.5, and OpenAI's o1 across reasoning benchmarks. This article explores its architecture, training methods ..."
    ],
    "response": "The sampling strategies used in Kimi k1.5's RL training are curriculum sampling and balanced difficulty. Curriculum sampling starts with training on simple tasks and gradually progresses to more challenging ones. The prompt set also includes a well-distributed range of easy, moderate, and difficult questions.",
    "reference": "Curriculum Sampling is used, where training starts with easier tasks and progresses to more challenging ones. Prioritized Sampling is also employed, which focuses on problems where the model underperforms by sampling proportional to 1 - success rate."
  },
  {
    "user_input": "What are the long2short methods used in Kimi k1.5?",
    "retrieved_contexts": [
      " KIMI K 1.5:\nSCALING REINFORCEMENT LEARNING WITH LLM S\nTECHNICAL REPORT OF KIMI K 1.5\nKimi Team\nABSTRACT\nLanguage model pretraining with next token prediction has proved effective for scaling compute but\nis limited to the amount of available training data. Scaling reinforcement learning (RL) unlocks a new\naxis for the continued improvement of artificial intelligence, with the promise that large language\nmodels (LLMs) can scale their training data by learning to explore with rewards. However, prior\npublished work has not produced competitive results. In light of this, we report on the training practice\nof Kimi k1.5, our latest multi-modal LLM trained with RL, including its RL training techniques,\nmulti-modal data recipes, and infrastructure optimization. Long context scaling and improved policy\noptimization methods are key ingredients of our approach, which establishes a simplistic, effective\nRL framework without relying on more complex techniques such as Monte Carlo tree search, value\nfunctions, and process reward models. Notably, our system achieves state-of-the-art reasoning\nperformance across multiple benchmarks and modalities—e.g., 77.5 on AIME, 96.2 on MATH\n500, 94-th percentile on Codeforces, 74.9 on MathVista—matching OpenAI’s o1. Moreover, we\npresent effective long2short methods that use long-CoT techniques to improve short-CoT models,\nyielding state-of-the-art short-CoT reasoning results—e.g., 60.8 on AIME, 94.6 on MATH500, 47.3 ",
      " Abstract page for arXiv paper 2501.12599: Kimi k1.5: Scaling Reinforcement Learning with LLMs ... Moreover, we present effective long2short methods that use long-CoT techniques to improve short-CoT models, yielding state-of-the-art short-CoT reasoning results -- e.g., 60.8 on AIME, 94.6 on MATH500, 47.3 on LiveCodeBench -- outperforming ... ",
      " Long-to-Short Transformation: Kimi k1.5 implements techniques that reduce computational requirements by dynamically transforming longer input sequences into more computationally manageable forms ..."
    ],
    "response": "The long2short methods used in Kimi k1.5 involve using long-Chains of Thought (CoT) techniques to improve short-CoT models. These methods dynamically transform longer input sequences into more computationally manageable forms. This helps achieve state-of-the-art short-CoT reasoning results.",
    "reference": "The long2short methods include model merging (averaging weights of long-cot and short-cot models), shortest rejection sampling (selecting the shortest correct response for fine-tuning), DPO (using long CoT model to generate samples for pairwise preference training), and long2short RL (applying length penalty and reducing rollout length after a standard RL training phase)."
  },
  {
    "user_input": "What is the training process of Kimi k1.5 in terms of pretraining and vanilla supervised finetuning?",
    "retrieved_contexts": [
      " Extensive scaling experiments indicate that most of the base model performance comes from improvements in the\nquality and diversity of the pretraining data. Specific details regarding model architecture scaling experiments lie\nbeyond the scope of this report and will be addressed in future publications.\nB.4 Training Stages\nThe Kimi k1.5 model is trained in three stages: the vision-language pretraining stage, the vision-language cooldown\nstage, and the long-context activation stage. Each stage of the Kimi k1.5 model’s training focuses on a particular\ncapability enhancement.\nVision-language pretraining stage In this stage, the model is firstly trained solely on language data, establishing\na robust language model foundation. Then the model is gradually introduced to interleaved vision-language data,\nacquiring multimodal capabilities. The visual tower is initially trained in isolation without updating the language model\nparameters, then we unfreeze the language model layers, and ultimately increase the proportion of vision-text data\n23 ",
      " Kimi k1.5 TECHNICAL REPORT\nLong2short RL After a standard RL training phase, we select a model that offers the best balance between perfor-\nmance and token efficiency to serve as the base model, and conduct a separate long2short RL training phase. In this\nsecond phase, we apply the length penalty introduced in Section 2.3.3, and significantly reduce the maximum rollout\nlength to further penalize responses that exceed the desired length while possibly correct.\n2.5 Other Training Details\n2.5.1 Pretraining\nThe Kimi k1.5 base model is trained on a diverse, high-quality multimodal corpus. The language data covers five\ndomains: English, Chinese, Code, Mathematics Reasoning, and Knowledge. Multimodal data, including Captioning,\nImage-text Interleaving, OCR, Knowledge, and QA datasets, enables our model to acquire vision-language capabilities.\nRigorous quality control ensures relevance, diversity, and balance in the overall pretrain dataset. Our pretraining\nproceeds in three stages: (1) Vision-language pretraining, where a strong language foundation is established, followed\nby gradual multimodal integration; (2) Cooldown, which consolidates capabilities using curated and synthetic data,\nparticularly for reasoning and knowledge-based tasks; and (3) Long-context activation, extending sequence processing\nto 131,072 tokens. More details regarding our pretraining efforts can be found in Appendix B.\n2.5.2 Vanilla Supervised Finetuning ",
      " Kimi k1.5 TECHNICAL REPORT\n2 Approach: Reinforcement Learning with LLMs\nThe development of Kimi k1.5 consists of several stages: pretraining, vanilla supervised fine-tuning (SFT), long-CoT\nsupervised fine-turning, and reinforcement learning (RL). This report focuses on RL, beginning with an overview of\nthe RL prompt set curation (Section 2.1) and long-CoT supervised finetuning (Section 2.2), followed by an in-depth\ndiscussion of RL training strategies in Section 2.3. Additional details on pretraining and vanilla supervised finetuning\ncan be found in Section 2.5.\n2.1 RL Prompt Set Curation\nThrough our preliminary experiments, we found that the quality and diversity of the RL prompt set play a critical role in\nensuring the effectiveness of reinforcement learning. A well-constructed prompt set not only guides the model toward\nrobust reasoning but also mitigates the risk of reward hacking and overfitting to superficial patterns. Specifically, three\nkey properties define a high-quality RL prompt set:\n•Diverse Coverage : Prompts should span a wide array of disciplines, such as STEM, coding, and general reasoning,\nto enhance the model’s adaptability and ensure broad applicability across different domains.\n•Balanced Difficulty : The prompt set should include a well-distributed range of easy, moderate, and difficult\nquestions to facilitate gradual learning and prevent overfitting to specific complexity levels."
    ],
    "response": "The training process of Kimi k1.5 includes three primary stages: vision-language pretraining, cooldown, and long-context activation. During vision-language pretraining, the model first focuses on establishing a strong language foundation, followed by the gradual integration of multimodal data. Vanilla supervised finetuning occurs after pretraining and involves further training the model to refine its performance on specific tasks.",
    "reference": "In pretraining, it is trained on a diverse multimodal corpus in three stages: vision-language pretraining, cooldown, and long-context activation. In vanilla supervised finetuning, a corpus covering multiple domains is created. For non-reasoning tasks, a seed dataset is constructed through human annotation and refined. For reasoning tasks, rejection sampling is used. The model is trained at different sequence lengths with learning rate decay."
  },
  {
    "user_input": "What are the main contributions of this research?",
    "retrieved_contexts": [
      " 1.1. Contributions\nPost-Training: Large-Scale Reinforcement Learning on the Base Model\n•We directly apply reinforcement learning (RL) to the base model without relying on super-\nvised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore\nchain-of-thought (CoT) for solving complex problems, resulting in the development of\nDeepSeek-R1-Zero. DeepSeek-R1-Zero demonstrates capabilities such as self-verification,\nreflection, and generating long CoTs, marking a significant milestone for the research\ncommunity. Notably, it is the first open research to validate that reasoning capabilities of\nLLMs can be incentivized purely through RL, without the need for SFT. This breakthrough\npaves the way for future advancements in this area.\n•We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL\nstages aimed at discovering improved reasoning patterns and aligning with human pref-\nerences, as well as two SFT stages that serve as the seed for the model’s reasoning and\nnon-reasoning capabilities. We believe the pipeline will benefit the industry by creating\nbetter models.\nDistillation: Smaller Models Can Be Powerful Too\n•We demonstrate that the reasoning patterns of larger models can be distilled into smaller\nmodels, resulting in better performance compared to the reasoning patterns discovered\nthrough RL on small models. The open source DeepSeek-R1, as well as its API, will benefit ",
      " 'The main contribution of this study is'...: An analysis of statements of contribution in English published research articles and L2 manuscripts ",
      " In academic research, particularly at the postgraduate level, the term \"research contribution\" signifies the unique value a study brings to its field. A well-defined research contribution is a critical component of a thesis, reflecting the originality and impact of the work. Whether theoretical, methodological, or practical, this contribution demonstrates the researcher's ability to ..."
    ],
    "response": "The main contributions of this research include the application of reinforcement learning directly to the base model without supervised fine-tuning, the development of a two-stage RL and SFT pipeline for creating improved models, and the demonstration that reasoning patterns of larger models can be effectively distilled into smaller models.",
    "reference": "Directly applied RL to the base model without preliminary SFT, developing DeepSeek-R1-Zero which exhibits self-verification, reflection, etc.Introduced the training pipeline for DeepSeek-R1, involving two RL and two SFT stages.Introduced the training pipeline for DeepSeek-R1, involving two RL and two SFT stages.Demonstrated the distillation of reasoning patterns from larger to smaller models and open - sourced relevant models and API."
  },
  {
    "user_input": "How does DeepSeek-R1-Zero improve reasoning capabilities?",
    "retrieved_contexts": [
      " Figure 3|The average response length of DeepSeek-R1-Zero on the training set during the RL\nprocess. DeepSeek-R1-Zero naturally learns to solve reasoning tasks with more thinking time.\nment throughout the training process. This improvement is not the result of external adjustments\nbut rather an intrinsic development within the model. DeepSeek-R1-Zero naturally acquires the\nability to solve increasingly complex reasoning tasks by leveraging extended test-time compu-\ntation. This computation ranges from generating hundreds to thousands of reasoning tokens,\nallowing the model to explore and refine its thought processes in greater depth.\nOne of the most remarkable aspects of this self-evolution is the emergence of sophisticated\nbehaviors as the test-time computation increases. Behaviors such as reflection—where the model\nrevisits and reevaluates its previous steps—and the exploration of alternative approaches to\nproblem-solving arise spontaneously. These behaviors are not explicitly programmed but instead\nemerge as a result of the model’s interaction with the reinforcement learning environment. This\nspontaneous development significantly enhances DeepSeek-R1-Zero’s reasoning capabilities,\nenabling it to tackle more challenging tasks with greater efficiency and accuracy.\nAha Moment of DeepSeek-R1-Zero A particularly intriguing phenomenon observed during\nthe training of DeepSeek-R1-Zero is the occurrence of an “aha moment”. This moment, as ",
      " DeepSeek-R1 represents a significant advancement in large language models (LLMs) by focusing on reasoning capabilities through innovative training methodologies and architectural improvements. The model tackles key challenges and introduces new methods to improve its reasoning abilities. Unlike traditional models, DeepSeek-R1-Zero doesn't go through the usual supervised fine-tuning step. ",
      " R1-Zero demonstrates that sophisticated reasoning (or chain of thought) capabilities can emerge in an LLM through pure reinforcement learning without the traditional supervised finetuning stage. This model, built on the DeepSeek-V3-Base architecture with 671 billion parameters, represents a significant departure from conventional training ..."
    ],
    "response": "DeepSeek-R1-Zero improves reasoning capabilities through extended test-time computation, which allows the model to generate and refine extensive chains of thought. This process leads to sophisticated behaviors like reflection and exploration of alternative solutions. These advancements emerge naturally during reinforcement learning, enhancing the model's efficiency and accuracy in tackling complex tasks.",
    "reference": "By using Group Relative Policy Optimization (GRPO) as the RL algorithm, a rule - based reward system (accuracy and format rewards), and a simple training template. Through RL training, it shows a self - evolution process with increasing thinking time and emerging sophisticated reasoning behaviors, like reflection and exploring alternative problem - solving approaches."
  },
  {
    "user_input": "What is the difference between DeepSeek-R1-Zero and DeepSeek-R1?",
    "retrieved_contexts": [
      " What are the main differences between DeepSeek-R1 and DeepSeek-R1-Zero? DeepSeek-R1 builds on the foundation of DeepSeek-R1-Zero, which serves as the base model. The two differ in their training approaches and overall performance. 1. Training method. DeepSeek-R1-Zero: Trained entirely with reinforcement learning (RL) and no supervised fine ... ",
      " DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via\nReinforcement Learning\nDeepSeek-AI\nresearch@deepseek.com\nAbstract\nWe introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1.\nDeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without super-\nvised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities.\nThrough RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing\nreasoning behaviors. However, it encounters challenges such as poor readability, and language\nmixing. To address these issues and further enhance reasoning performance, we introduce\nDeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-\nR1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the\nresearch community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models\n(1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.\nAIME 2024\n(Pass@1)Codeforces\n(Percentile)GPQA Diamond\n(Pass@1)MATH-500\n(Pass@1)MMLU\n(Pass@1)SWE-bench Verified\n(Resolved)020406080100Accuracy / Percentile (%)79.896.3\n71.597.3\n90.8\n49.279.296.6\n75.796.4\n91.8\n48.972.690.6\n62.194.3\n87.4\n36.863.693.4\n60.090.0\n85.2\n41.6\n39.258.7 59.190.2\n88.5\n42.0DeepSeek-R1 OpenAI-o1-1217 DeepSeek-R1-32B OpenAI-o1-mini DeepSeek-V3\nFigure 1|Benchmark performance of DeepSeek-R1. ",
      " DeepSeek-R1-Zero: This model is a ... The differences between DeepSeek-R1 and DeepSeek-R1-Zero highlight the trade-offs between innovation and practicality. While R1-Zero pushes the boundaries of ..."
    ],
    "response": "The key difference between DeepSeek-R1-Zero and DeepSeek-R1 lies in their training methods and performance. DeepSeek-R1-Zero is trained entirely with reinforcement learning without supervised fine-tuning, which leads to innovative reasoning behaviors but challenges like poor readability and language mixing. DeepSeek-R1, on the other hand, incorporates multi-stage training and cold-start data before reinforcement learning to enhance performance and address these issues.",
    "reference": "DeepSeek-R1-Zero applies RL directly to the base model without SFT data, while DeepSeek-R1 uses a small amount of cold - start data to fine - tune the model first. DeepSeek-R1-Zero has issues like poor readability and language mixing, which DeepSeek-R1 addresses through its multi - stage training pipeline."
  },
  {
    "user_input": "What is the cold start in the training of DeepSeek-R1?",
    "retrieved_contexts": [
      " Inspired by the promising results of DeepSeek-R1-Zero, two natural questions arise: 1) Can\nreasoning performance be further improved or convergence accelerated by incorporating a small\namount of high-quality data as a cold start? 2) How can we train a user-friendly model that\nnot only produces clear and coherent Chains of Thought (CoT) but also demonstrates strong\ngeneral capabilities? To address these questions, we design a pipeline to train DeepSeek-R1. The\npipeline consists of four stages, outlined as follows.\n2.3.1. Cold Start\nUnlike DeepSeek-R1-Zero, to prevent the early unstable cold start phase of RL training from\nthe base model, for DeepSeek-R1 we construct and collect a small amount of long CoT data\nto fine-tune the model as the initial RL actor. To collect such data, we have explored several\napproaches: using few-shot prompting with a long CoT as an example, directly prompting\nmodels to generate detailed answers with reflection and verification, gathering DeepSeek-R1-\nZero outputs in a readable format, and refining the results through post-processing by human\nannotators.\nIn this work, we collect thousands of cold-start data to fine-tune the DeepSeek-V3-Base as\nthe starting point for RL. Compared to DeepSeek-R1-Zero, the advantages of cold start data\n9 ",
      " The training process for DeepSeek-R1 is divided into several stages: 1. Reinforcement Learning with DeepSeek-R1-Zero. DeepSeek-R1-Zero uses Group Relative Policy Optimization (GRPO) ... Cold Start for DeepSeek-R1. High-quality examples (long reasoning chains, clear formats) are collected and used to give the model a starting point. ... ",
      " Here's a quick explanation of each training stage and what it was done: Step 1: They fine-tuned a base model (DeepSeek-V3-Base) with thousands of cold-start data points to lay a solid foundation. FYI, thousands of cold-start data points is a tiny fraction compared to the millions or even billions of labeled data points typically required for supervised learning at scale."
    ],
    "response": "The cold start in the training of DeepSeek-R1 involves fine-tuning the base model (DeepSeek-V3-Base) using a small amount of high-quality long Chains of Thought (CoT) data. This process aims to stabilize the early phase of reinforcement learning by providing a solid initial foundation. The data is collected through methods like few-shot prompting, direct prompts, and post-processing by human annotators.",
    "reference": "It constructs and collects a small amount of long Chain-of-Thought (CoT) data to fine - tune the DeepSeek-V3-Base as the initial RL actor. The cold - start data is designed to be more readable and shows better potential compared to DeepSeek-R1-Zero. Data collection methods include few - shot prompting, direct prompting for detailed answers, gathering DeepSeek-R1-Zero outputs, and post - processing by human annotators."
  },
  {
    "user_input": "How is the reward modeled in DeepSeek-R1-Zero?",
    "retrieved_contexts": [
      " A conversation between User and Assistant. The user asks a question, and the Assistant solves it.\nThe assistant first thinks about the reasoning process in the mind and then provides the user\nwith the answer. The reasoning process and answer are enclosed within <think> </think> and\n<answer> </answer> tags, respectively, i.e., <think> reasoning process here </think>\n<answer> answer here </answer>. User: prompt. Assistant:\nTable 1|Template for DeepSeek-R1-Zero. prompt will be replaced with the specific reasoning\nquestion during training.\n2.2.2. Reward Modeling\nThe reward is the source of the training signal, which decides the optimization direction of RL.\nTo train DeepSeek-R1-Zero, we adopt a rule-based reward system that mainly consists of two\ntypes of rewards:\n•Accuracy rewards : The accuracy reward model evaluates whether the response is correct.\nFor example, in the case of math problems with deterministic results, the model is required\nto provide the final answer in a specified format (e.g., within a box), enabling reliable\nrule-based verification of correctness. Similarly, for LeetCode problems, a compiler can be\nused to generate feedback based on predefined test cases.\n•Format rewards : In addition to the accuracy reward model, we employ a format reward\nmodel that enforces the model to put its thinking process between ‘<think>’ and ‘</think>’\ntags.\nWe do not apply the outcome or process neural reward model in developing DeepSeek-R1-Zero, ",
      " While training DeepSeek-R1-Zero, a rule based reward system is employed, which majorly consists of 2 types - Accuracy rewards — This model evaluates whether or not the response is correct. ",
      " Reward System The reward system provides feedback to the model, indicating the quality of its outputs. DeepSeek-R1-Zero uses a rule-based reward system that consists of two types of rewards: Accuracy Rewards: These rewards are given when the model provides correct answers."
    ],
    "response": "DeepSeek-R1-Zero uses a rule-based reward system that consists of two types of rewards: accuracy rewards and format rewards. Accuracy rewards evaluate whether the response is correct, while format rewards enforce the model to include its thinking process within specified tags.",
    "reference": "It uses a rule - based reward system. The accuracy reward model evaluates whether the response is correct, for example, verifying math problem answers in a specified format or using a compiler for LeetCode problems. The format reward model enforces the model to put its thinking process between ‘<think>’ and ‘</think>’ tags. Neural reward models are not used due to potential reward hacking and high resource requirements."
  },
  {
    "user_input": "What benchmarks are used to evaluate the models?",
    "retrieved_contexts": [
      " These optimizations improve RL efficiency in code execution, providing a consistent and reliable environment for\nevaluating RL-generated code, essential for iterative training and model improvement.\n3 Experiments\n3.1 Evaluation\nSince k1.5 is a multimodal model, we conducted comprehensive evaluation across various benchmarks for different\nmodalities. The detailed evaluation setup can be found in Appendix C. Our benchmarks primarily consist of the\nfollowing three categories:\n•Text Benchmark : MMLU (Hendrycks et al. 2020), IF-Eval (J. Zhou et al. 2023), CLUEWSC (L. Xu et al. 2020),\nC-EV AL (Y . Huang et al. 2023)\n•Reasoning Benchmark : HumanEval-Mul, LiveCodeBench (Jain et al. 2024), Codeforces, AIME 2024, MATH-\n500 (Lightman et al. 2023)\n•Vision Benchmark : MMMU (Yue, Ni, et al. 2024), MATH-Vision (K. Wang et al. 2024), MathVista (Lu et al.\n2023)\n2https://github.com/DMOJ/judge-server\n11 ",
      " IF-Eval (J. Zhou et al. 2023) is a benchmark for evaluating large language models’ ability to follow verifiable\ninstructions. There are 500+ prompts with instructions such as \"write an article with more than 800 words\", etc. Due to\na version shift, the number of IFEval reported in Table 3 derived from an intermediate model. We will update the scores\nbased on the final model.\nCLUEWSC (L. Xu et al. 2020) is a coreference resolution task in CLUE benchmark, requiring models to determine if\na pronoun and a noun phrase in a sentence co-refer, with data from Chinese fiction books.\nC-EV AL (Y . Huang et al. 2023) is a comprehensive Chinese evaluation suite for assessing advanced knowledge and\nreasoning abilities of foundation models. It includes 13,948 multiple-choice questions across 52 disciplines and four\ndifficulty levels.\nC.2 Reasoning Benchmark\nHumanEval-Mul is a subset of Multipl-E (Cassano, Gouwar, D. Nguyen, S. D. Nguyen, et al. 2022). MultiPL-E\nextends the HumanEval benchmark and MBPP benchmark to 18 languages that encompass a range of programming\n24 ",
      " Evaluating machine learning models is a important step in ensuring their effectiveness and reliability in real-world applications. Using appropriate metrics such as accuracy, precision, recall, F1 score for classification and regression-specific measures like MAE, MSE, RMSE and MAPE can assess model performance for different tasks."
    ],
    "response": "The models are evaluated using benchmarks in three categories: Text Benchmark (MMLU, IF-Eval, CLUEWSC, C-EV AL), Reasoning Benchmark (HumanEval-Mul, LiveCodeBench, Codeforces, AIME, MATH-500), and Vision Benchmark (MMMU, MATH-Vision, MathVista).",
    "reference": "MMLU, MMLU - Redux, MMLU - Pro, C - Eval, CMMLU, IFEval, FRAMES, GPQA Diamond, SimpleQA, C - SimpleQA, SWE - Bench Verified, Aider 1, LiveCodeBench, Codeforces, Chinese National High School Mathematics Olympiad (CNMO 2024), American Invitational Mathematics Examination 2024 (AIME 2024), and open - ended generation tasks like AlpacaEval 2.0 and Arena - Hard."
  },
  {
    "user_input": "What are the baselines for model evaluation?",
    "retrieved_contexts": [
      " Evaluating machine learning models is a important step in ensuring their effectiveness and reliability in real-world applications. Using appropriate metrics such as accuracy, precision, recall, F1 score for classification and regression-specific measures like MAE, MSE, RMSE and MAPE can assess model performance for different tasks. ",
      " This would be where model evaluation metrics come in: to help one understand the strengths and weaknesses of a model with a view to optimization and real-world application. ... RMSE is the standard deviation of residuals: Benefits: Penalizes large errors, ... Brownlee, J. (2020). Machine Learning Mastery: How to Evaluate Machine Learning Models ... ",
      " Lantz, B. Machine Learning with R: Learn Techniques for Building and Improving Machine Learning Models, from Data Preparation to Model Tuning, Evaluation, and Working with Big Data (Packt ..."
    ],
    "response": "The baselines for model evaluation include metrics such as accuracy, precision, recall, and F1 score for classification tasks, and MAE, MSE, RMSE, and MAPE for regression tasks. These metrics help assess the strengths and weaknesses of a model.",
    "reference": "DeepSeek-V3, Claude - Sonnet - 3.5 - 1022, GPT - 4o - 0513, OpenAI - o1 - mini, OpenAI - o1 - 1217. For distilled models, QwQ - 32B - Preview is also used as a baseline."
  },
  {
    "user_input": "What is the performance of DeepSeek-R1 on math tasks?",
    "retrieved_contexts": [
      " its robustness across multiple tasks.\nOn math tasks, DeepSeek-R1 demonstrates performance on par with OpenAI-o1-1217,\nsurpassing other models by a large margin. A similar trend is observed on coding algorithm\ntasks, such as LiveCodeBench and Codeforces, where reasoning-focused models dominate these\nbenchmarks. On engineering-oriented coding tasks, OpenAI-o1-1217 outperforms DeepSeek-R1\non Aider but achieves comparable performance on SWE Verified. We believe the engineering\n13 ",
      " DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via\nReinforcement Learning\nDeepSeek-AI\nresearch@deepseek.com\nAbstract\nWe introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1.\nDeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without super-\nvised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities.\nThrough RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing\nreasoning behaviors. However, it encounters challenges such as poor readability, and language\nmixing. To address these issues and further enhance reasoning performance, we introduce\nDeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-\nR1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the\nresearch community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models\n(1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.\nAIME 2024\n(Pass@1)Codeforces\n(Percentile)GPQA Diamond\n(Pass@1)MATH-500\n(Pass@1)MMLU\n(Pass@1)SWE-bench Verified\n(Resolved)020406080100Accuracy / Percentile (%)79.896.3\n71.597.3\n90.8\n49.279.296.6\n75.796.4\n91.8\n48.972.690.6\n62.194.3\n87.4\n36.863.693.4\n60.090.0\n85.2\n41.6\n39.258.7 59.190.2\n88.5\n42.0DeepSeek-R1 OpenAI-o1-1217 DeepSeek-R1-32B OpenAI-o1-mini DeepSeek-V3\nFigure 1|Benchmark performance of DeepSeek-R1. ",
      " On math tasks, DeepSeek-R1 demonstrates performance on par with OpenAI-o1-1217, surpassing other models by a large margin. A similar trend is observed on coding algorithm tasks, such as LiveCodeBench and Codeforces, where reasoning-focused models dominate these benchmarks."
    ],
    "response": "DeepSeek-R1 demonstrates performance on math tasks on par with OpenAI-o1-1217, surpassing other models by a large margin.",
    "reference": "On AIME 2024, it has a Pass@1 score of 79.8%, slightly surpassing OpenAI - o1 - 1217. On MATH - 500, it attains a score of 97.3%, performing on par with OpenAI - o1 - 1217 and significantly outperforming other models."
  },
  {
    "user_input": "Why is distillation of reasoning capabilities important?",
    "retrieved_contexts": [
      " Reasoning is one crucial capability in Large Language Models (LLMs), allowing them to perform complex tasks such as solving math problems and multi-step planning. While reasoning capability can emerge in larger models, smaller ones usually have to rely on distillation to transfer this capability from a larger model. ",
      " Introduction. In this article, we will explore the advancements and methodologies behind DeepSeek-R1, a cutting-edge approach to enhancing reasoning capabilities in Large Language Models (LLMs).It provide a comprehensive summary of the paper, highlighting key innovations, such as the use of reinforcement learning (RL) to incentivize reasoning and the distillation of capabilities into smaller ... ",
      " The training methodology for SLMs first emphasises distilling self-evaluation capability to lay the foundation for reducing the impact of errors in CoTs on SLMs, followed by incorporating comprehensive reasoning capability through diverse CoTs distillation. Distillation. This study illustrates an important method, called distillation…"
    ],
    "response": "Distillation of reasoning capabilities is important because it allows smaller language models to acquire complex reasoning abilities that typically emerge only in larger models. This process helps smaller models perform tasks like solving math problems and multi-step planning. Additionally, it enhances the overall efficiency and utility of these smaller models.",
    "reference": "It can empower smaller models with reasoning capabilities. Distilling from more powerful models like DeepSeek-R1 into smaller models can achieve better performance than RL training on small models alone. For example, DeepSeek-R1 - Distill - Qwen - 7B outperforms non - reasoning models like GPT - 4o - 0513 across the board."
  }
]
[
  {
    "user_input": "What is the core idea of Janus?",
    "retrieved_contexts": [
      " demands that understanding and generation place on the visual encoder. Extensive experi-\nments have demonstrated the effectiveness and leading performance of Janus. It is also worth\nnoting that Janus is flexible and easy to extend. In addition to having significant potential for\nimprovement in both multimodal understanding and generation, Janus is also easily extendable\nto incorporate more input modalities. The above advantages suggest that Janus may serve as an\ninspiration for the development of the next generation of multimodal general-purpose models.\n13 ",
      " Janus: Decoupling Visual Encoding for Unified\nMultimodal Understanding and Generation\nChengyue Wu1,2Xiaokang Chen1,‚àó,‚Ä†Zhiyu Wu1,3Yiyang Ma1,3Xingchao Liu1Zizheng Pan1\nWen Liu1Zhenda Xie1Xingkai Yu1Chong Ruan1Ping Luo2,‚àó\n1DeepSeek-AI2The University of Hong Kong3Peking University\n‚Ä†: Project lead‚àó: Corresponding authors\nProject Page: https://github.com/deepseek-ai/Janus\nAbstract\nIn this paper, we introduce Janus , an autoregressive framework that unifies multimodal under-\nstanding and generation. Prior research often relies on a single visual encoder for both tasks,\nsuch as Chameleon. However, due to the differing levels of information granularity required by\nmultimodal understanding and generation, this approach can lead to suboptimal performance,\nparticularly in multimodal understanding. To address this issue, we decouple visual encoding\ninto separate pathways, while still leveraging a single, unified transformer architecture for\nprocessing. The decoupling not only alleviates the conflict between the visual encoder‚Äôs roles in\nunderstanding and generation, but also enhances the framework‚Äôs flexibility. For instance, both\nthe multimodal understanding and generation components can independently select their most\nsuitable encoding methods. Experiments show that Janus surpasses previous unified model\nand matches or exceeds the performance of task-specific models. The simplicity, high flexibility, ",
      " Janus (Ours)\nA close -up high -contrast  photo  of Sydney  Opera  House  sitting  next  \nto Eiffel  tower , under  a blue  night  sky of roiling  energy,  exploding  \nyellow  stars , and radiating  swirls  of blue .\nA detailed  portrait  of the Roman  god Janus , featuring  his two faces  looking  in opposite  \ndirections . One face appears  aged,  with  deep -set wrinkles  and a wise,  contemplative  expression,  \nwhile  the other  face is youthful , exuding  vigor  and curiosity . His hair is styled  in flowing  curls , \nframing  both  faces  with  a sense  of divine  symmetry . The artwork  is rich in contrasting  colors,  \nwith  the left side dominated  by cold  blues  and silvers , symbolizing  winter  and reflection,  and the \nright  side awash  with  warm  golds  and reds , representing  spring  and renewal . The background  is \na celestial  tapestry , adorned  with  stars  and symbolic  motifs  of time  and passage . \nA wise  old owl with  golden  plumage  perched  on a luminous  crystal  \ntree in a magical  forest . Radiant  fireflies  swirl  around  while  ethereal  \nmist  rolls  through  the trees,  illuminated  by swirls  of iridescent  \nmoonlight  and glistening  emerald  leaves .\nA brave  dog wearing  a futuristic  space  suit, exploring  an alien  planet  \namidst  swirling  dunes  of stardust  and meteor  showers . The landscape  \nis dotted  with  glowing  crystal  formations  and ethereal  terraforms,"
    ],
    "response": "The core idea of Janus is to decouple visual encoding for unified multimodal understanding and generation by separating pathways while using a single unified transformer architecture. This enhances flexibility and performance in both tasks.",
    "reference": "The core idea of Janus is to decouple visual encoding for multimodal understanding and generation. This is achieved by introducing two independent visual encoding pathways, one for multimodal understanding and one for multimodal generation, unified by the same transformer architecture."
  },
  {
    "user_input": "How does Janus's architecture differ from previous unified multimodal models?",
    "retrieved_contexts": [
      " demands that understanding and generation place on the visual encoder. Extensive experi-\nments have demonstrated the effectiveness and leading performance of Janus. It is also worth\nnoting that Janus is flexible and easy to extend. In addition to having significant potential for\nimprovement in both multimodal understanding and generation, Janus is also easily extendable\nto incorporate more input modalities. The above advantages suggest that Janus may serve as an\ninspiration for the development of the next generation of multimodal general-purpose models.\n13 ",
      " Janus: Decoupling Visual Encoding for Unified\nMultimodal Understanding and Generation\nChengyue Wu1,2Xiaokang Chen1,‚àó,‚Ä†Zhiyu Wu1,3Yiyang Ma1,3Xingchao Liu1Zizheng Pan1\nWen Liu1Zhenda Xie1Xingkai Yu1Chong Ruan1Ping Luo2,‚àó\n1DeepSeek-AI2The University of Hong Kong3Peking University\n‚Ä†: Project lead‚àó: Corresponding authors\nProject Page: https://github.com/deepseek-ai/Janus\nAbstract\nIn this paper, we introduce Janus , an autoregressive framework that unifies multimodal under-\nstanding and generation. Prior research often relies on a single visual encoder for both tasks,\nsuch as Chameleon. However, due to the differing levels of information granularity required by\nmultimodal understanding and generation, this approach can lead to suboptimal performance,\nparticularly in multimodal understanding. To address this issue, we decouple visual encoding\ninto separate pathways, while still leveraging a single, unified transformer architecture for\nprocessing. The decoupling not only alleviates the conflict between the visual encoder‚Äôs roles in\nunderstanding and generation, but also enhances the framework‚Äôs flexibility. For instance, both\nthe multimodal understanding and generation components can independently select their most\nsuitable encoding methods. Experiments show that Janus surpasses previous unified model\nand matches or exceeds the performance of task-specific models. The simplicity, high flexibility, ",
      " Auto-RegressiveTransformer\nUnd.Encoder\nTextTokenizer\nGen.Encoder\nTextDe-Tokenizer\nTextTokenizer\nImageDecoder\nImage:X!LanguageInstruct:X\"LanguageInstruct:X\"Image:X!LanguageResponse:X#GeneratedImage:X!UnderstandingImageGeneration\n‚Ä¶‚Ä¶\n‚Ä¶‚Ä¶Figure 2|Architecture of our Janus. Different from previous approaches [ 77,85] that typically\nassume visual understanding and generation require the same visual encoder, our Janus de-\ncouples visual encoding for visual understanding and visual generation. ‚ÄúUnd. Encoder‚Äù and\n‚ÄúGen. Encoder‚Äù are abbreviations for ‚ÄúUnderstanding Encoder‚Äù and ‚ÄúGeneration Encoder‚Äù,\nrespectively. Best viewed in color.\nand generation tasks, regardless of whether they are based on autoregressive (AR) models\n[77,85] or diffusion models [ 86,94]. For example, Chameleon [ 77] adopts a VQ Tokenizer to\nencode images for both multimodal understanding and generation. However, this practice\nmay lead to suboptimal outcomes, as the vision encoder might face a trade-off between the\ndemands of understanding and generation. In contrast, our Janus can explicitly decouple the\nvisual representations for understanding and generation, recognizing that different tasks may\nrequire varying levels of information.\n3. Janus: A Simple, Unified and Flexible Multimodal Framework\n3.1. Architecture\nThe architecture of Janus is shown in Figure 2. For pure text understanding, multimodal\nunderstanding, and visual generation, we apply independent encoding methods to convert the"
    ],
    "response": "Janus's architecture differs from previous unified multimodal models because it decouples visual encoding into two separate pathways for multimodal understanding and generation. This allows each task to select its most suitable visual encoding method independently. In contrast, previous models like Chameleon use a single, shared visual encoder that can lead to suboptimal performance due to the differing demands on information granularity.",
    "reference": "Previous models often use a single visual encoder for both understanding and generation tasks. In contrast, Janus decouples visual encoding. It uses the SigLIP encoder for multimodal understanding to extract high - dimensional semantic features and a VQ tokenizer for visual generation to convert images into discrete IDs, with separate adaptors for each task."
  },
  {
    "user_input": "What are the three training stages of Janus?",
    "retrieved_contexts": [
      " Gen. Encoder\nImageHead\nGen. AdaptorStageI:Training AdaptorsandImageHead\nLLM\nUnd.Adaptor\nüî•\nüî•\nüî•\nUnderstanding\nTextHead\nUnd. Encoder\n‚ùÑ\n‚ùÑ\n‚ùÑImageGenerationStageII: UnifiedPretrainingStageIII:SupervisedFine-tuning\nGen. Encoder\nImageHead\nGen. Adaptor\nLLM\nUnd.Adaptor\nüî•\nüî•\nüî•\nUnderstanding\nTextHead\nUnd. Encoder\n‚ùÑ\n‚ùÑ\n‚ùÑImageGeneration\nüî•\nüî•\nGen. Encoder\nImageHead\nGen. Adaptor\nLLM\nUnd.Adaptor\nüî•\nüî•\nüî•\nUnderstanding\nTextHead\nUnd. EncoderImageGeneration\nüî•\nüî•\nüî•\n‚ùÑFigure 3|Our Janus adopts a three-stage training procedure. We use flame symbols/snowflake\nsymbols in the diagram to indicate the module updates/does not update its parameters.\nattention masks.\n3.2. Training Procedure\nThe training of Janus is divided into three stages, as illustrated in Figure 3. Details are provided\nin the below.\nStage I: Training Adaptors and Image Head. The main goal of this stage is to create a conceptual\nconnection between visual and linguistic elements within the embedding space, enabling the\nLLM to understand the entities shown in images and have preliminary visual generation\nability. We keep the visual encoders and the LLM frozen during this stage, allowing only the\ntrainable parameters within the understanding adaptor, generation adaptor and image head to\nbe updated.\nStage II: Unified Pretraining. In this stage, we perform unified pretraining with multimodal\ncorpus to enable Janus to learn both multimodal understanding and generation. We unfreeze the ",
      " corpus to enable Janus to learn both multimodal understanding and generation. We unfreeze the\nLLM and utilize all types of training data: pure text data, multimodal understanding data, and\nvisual generation data. Inspired by Pixart [ 9], we begin by conducting simple visual generation\ntraining using ImageNet- 1k to help the model grasp basic pixel dependencies. Subsequently, we\nenhance the model‚Äôs open-domain visual generation capability with general text-to-image data.\nStage III: Supervised Fine-tuning. During this stage, we fine-tune the pretrained model with\ninstruction tuning data to enhance its instruction-following and dialogue capabilities. We\nfine-tune all parameters except the generation encoder. We focus on supervising the answers\nwhile masking system and user prompts. To ensure Janus‚Äôs proficiency in both multimodal\nunderstanding and generation, we don‚Äôt fine-tune separate models for a certain task. Instead,\nwe use a blend of pure text dialogue data, multimodal understanding data and visual generation\ndata, ensuring versatility across various scenarios.\n3.3. Training Objective\nJanus is an autoregressive model, and we simply adopt the cross-entropy loss during training:\nL=‚àí‚àëÔ∏Å\nùëñ=1logùëÉùúÉ(ùë•ùëñ|ùë•<ùëñ) (1)\nHere,ùëÉ(¬∑|¬∑) indicates the conditional probability modeled by the weights ùúÉof Janus. For\npure text understanding and multimodal understanding tasks, we compute the loss on the text\n5 ",
      " demands that understanding and generation place on the visual encoder. Extensive experi-\nments have demonstrated the effectiveness and leading performance of Janus. It is also worth\nnoting that Janus is flexible and easy to extend. In addition to having significant potential for\nimprovement in both multimodal understanding and generation, Janus is also easily extendable\nto incorporate more input modalities. The above advantages suggest that Janus may serve as an\ninspiration for the development of the next generation of multimodal general-purpose models.\n13"
    ],
    "response": "The three training stages of Janus are:\n\n1. **Stage I: Training Adaptors and Image Head**: The goal is to connect visual and linguistic elements in the embedding space, enabling the LLM to understand images and have initial generation ability.\n\n2. **Stage II: Unified Pretraining**: The model learns multimodal understanding and generation using a unified pretraining approach with multimodal corpus, including text, multimodal understanding data, and visual generation data.\n\n3. **Stage III: Supervised Fine-tuning**: The pretrained model is fine-tuned with instruction tuning data to improve instruction-following and dialogue capabilities while supervising answers and masking system/user prompts.",
    "reference": "The first stage is training adaptors and the image head, where only the understanding adaptor, generation adaptor, and image head are updated while keeping the visual encoders and LLM frozen. The second stage is unified pretraining with multimodal corpus, including using ImageNet - 1k for simple visual generation training first and then general text - to - image data. The third stage is supervised fine - tuning with instruction tuning data, fine - tuning all parameters except the generation encoder."
  },
  {
    "user_input": "What is the training objective of Janus?",
    "retrieved_contexts": [
      " corpus to enable Janus to learn both multimodal understanding and generation. We unfreeze the\nLLM and utilize all types of training data: pure text data, multimodal understanding data, and\nvisual generation data. Inspired by Pixart [ 9], we begin by conducting simple visual generation\ntraining using ImageNet- 1k to help the model grasp basic pixel dependencies. Subsequently, we\nenhance the model‚Äôs open-domain visual generation capability with general text-to-image data.\nStage III: Supervised Fine-tuning. During this stage, we fine-tune the pretrained model with\ninstruction tuning data to enhance its instruction-following and dialogue capabilities. We\nfine-tune all parameters except the generation encoder. We focus on supervising the answers\nwhile masking system and user prompts. To ensure Janus‚Äôs proficiency in both multimodal\nunderstanding and generation, we don‚Äôt fine-tune separate models for a certain task. Instead,\nwe use a blend of pure text dialogue data, multimodal understanding data and visual generation\ndata, ensuring versatility across various scenarios.\n3.3. Training Objective\nJanus is an autoregressive model, and we simply adopt the cross-entropy loss during training:\nL=‚àí‚àëÔ∏Å\nùëñ=1logùëÉùúÉ(ùë•ùëñ|ùë•<ùëñ) (1)\nHere,ùëÉ(¬∑|¬∑) indicates the conditional probability modeled by the weights ùúÉof Janus. For\npure text understanding and multimodal understanding tasks, we compute the loss on the text\n5 ",
      " demands that understanding and generation place on the visual encoder. Extensive experi-\nments have demonstrated the effectiveness and leading performance of Janus. It is also worth\nnoting that Janus is flexible and easy to extend. In addition to having significant potential for\nimprovement in both multimodal understanding and generation, Janus is also easily extendable\nto incorporate more input modalities. The above advantages suggest that Janus may serve as an\ninspiration for the development of the next generation of multimodal general-purpose models.\n13 ",
      " Janus (Ours)\nA close -up high -contrast  photo  of Sydney  Opera  House  sitting  next  \nto Eiffel  tower , under  a blue  night  sky of roiling  energy,  exploding  \nyellow  stars , and radiating  swirls  of blue .\nA detailed  portrait  of the Roman  god Janus , featuring  his two faces  looking  in opposite  \ndirections . One face appears  aged,  with  deep -set wrinkles  and a wise,  contemplative  expression,  \nwhile  the other  face is youthful , exuding  vigor  and curiosity . His hair is styled  in flowing  curls , \nframing  both  faces  with  a sense  of divine  symmetry . The artwork  is rich in contrasting  colors,  \nwith  the left side dominated  by cold  blues  and silvers , symbolizing  winter  and reflection,  and the \nright  side awash  with  warm  golds  and reds , representing  spring  and renewal . The background  is \na celestial  tapestry , adorned  with  stars  and symbolic  motifs  of time  and passage . \nA wise  old owl with  golden  plumage  perched  on a luminous  crystal  \ntree in a magical  forest . Radiant  fireflies  swirl  around  while  ethereal  \nmist  rolls  through  the trees,  illuminated  by swirls  of iridescent  \nmoonlight  and glistening  emerald  leaves .\nA brave  dog wearing  a futuristic  space  suit, exploring  an alien  planet  \namidst  swirling  dunes  of stardust  and meteor  showers . The landscape  \nis dotted  with  glowing  crystal  formations  and ethereal  terraforms,"
    ],
    "response": "The training objective of Janus is multimodal understanding and generation. It's an autoregressive model trained using the cross-entropy loss function on various datasets including pure text data, multimodal understanding, and visual generation data.",
    "reference": "Janus is an autoregressive model and uses the cross - entropy loss during training. The loss is calculated as $\\mathcal{L}=-\\sum_{i = 1} \\log P_{\\theta}(x_{i}"
  },
  {
    "user_input": "How does Janus perform inference for image generation?",
    "retrieved_contexts": [
      " demands that understanding and generation place on the visual encoder. Extensive experi-\nments have demonstrated the effectiveness and leading performance of Janus. It is also worth\nnoting that Janus is flexible and easy to extend. In addition to having significant potential for\nimprovement in both multimodal understanding and generation, Janus is also easily extendable\nto incorporate more input modalities. The above advantages suggest that Janus may serve as an\ninspiration for the development of the next generation of multimodal general-purpose models.\n13 ",
      " Auto-RegressiveTransformer\nUnd.Encoder\nTextTokenizer\nGen.Encoder\nTextDe-Tokenizer\nTextTokenizer\nImageDecoder\nImage:X!LanguageInstruct:X\"LanguageInstruct:X\"Image:X!LanguageResponse:X#GeneratedImage:X!UnderstandingImageGeneration\n‚Ä¶‚Ä¶\n‚Ä¶‚Ä¶Figure 2|Architecture of our Janus. Different from previous approaches [ 77,85] that typically\nassume visual understanding and generation require the same visual encoder, our Janus de-\ncouples visual encoding for visual understanding and visual generation. ‚ÄúUnd. Encoder‚Äù and\n‚ÄúGen. Encoder‚Äù are abbreviations for ‚ÄúUnderstanding Encoder‚Äù and ‚ÄúGeneration Encoder‚Äù,\nrespectively. Best viewed in color.\nand generation tasks, regardless of whether they are based on autoregressive (AR) models\n[77,85] or diffusion models [ 86,94]. For example, Chameleon [ 77] adopts a VQ Tokenizer to\nencode images for both multimodal understanding and generation. However, this practice\nmay lead to suboptimal outcomes, as the vision encoder might face a trade-off between the\ndemands of understanding and generation. In contrast, our Janus can explicitly decouple the\nvisual representations for understanding and generation, recognizing that different tasks may\nrequire varying levels of information.\n3. Janus: A Simple, Unified and Flexible Multimodal Framework\n3.1. Architecture\nThe architecture of Janus is shown in Figure 2. For pure text understanding, multimodal\nunderstanding, and visual generation, we apply independent encoding methods to convert the ",
      " corpus to enable Janus to learn both multimodal understanding and generation. We unfreeze the\nLLM and utilize all types of training data: pure text data, multimodal understanding data, and\nvisual generation data. Inspired by Pixart [ 9], we begin by conducting simple visual generation\ntraining using ImageNet- 1k to help the model grasp basic pixel dependencies. Subsequently, we\nenhance the model‚Äôs open-domain visual generation capability with general text-to-image data.\nStage III: Supervised Fine-tuning. During this stage, we fine-tune the pretrained model with\ninstruction tuning data to enhance its instruction-following and dialogue capabilities. We\nfine-tune all parameters except the generation encoder. We focus on supervising the answers\nwhile masking system and user prompts. To ensure Janus‚Äôs proficiency in both multimodal\nunderstanding and generation, we don‚Äôt fine-tune separate models for a certain task. Instead,\nwe use a blend of pure text dialogue data, multimodal understanding data and visual generation\ndata, ensuring versatility across various scenarios.\n3.3. Training Objective\nJanus is an autoregressive model, and we simply adopt the cross-entropy loss during training:\nL=‚àí‚àëÔ∏Å\nùëñ=1logùëÉùúÉ(ùë•ùëñ|ùë•<ùëñ) (1)\nHere,ùëÉ(¬∑|¬∑) indicates the conditional probability modeled by the weights ùúÉof Janus. For\npure text understanding and multimodal understanding tasks, we compute the loss on the text\n5"
    ],
    "response": "I don‚Äôt know.",
    "reference": "For image generation, Janus utilizes classifier - free guidance (CFG). Specifically, for each token, the logit¬†¬†is calculated as¬†, where¬†¬†is the conditional logit,¬†¬†is the unconditional logit, and¬†¬†is the scale for the classifier - free guidance (default¬†¬†for evaluation)."
  },
  {
    "user_input": "What datasets are used in Janus's training?",
    "retrieved_contexts": [
      " corpus to enable Janus to learn both multimodal understanding and generation. We unfreeze the\nLLM and utilize all types of training data: pure text data, multimodal understanding data, and\nvisual generation data. Inspired by Pixart [ 9], we begin by conducting simple visual generation\ntraining using ImageNet- 1k to help the model grasp basic pixel dependencies. Subsequently, we\nenhance the model‚Äôs open-domain visual generation capability with general text-to-image data.\nStage III: Supervised Fine-tuning. During this stage, we fine-tune the pretrained model with\ninstruction tuning data to enhance its instruction-following and dialogue capabilities. We\nfine-tune all parameters except the generation encoder. We focus on supervising the answers\nwhile masking system and user prompts. To ensure Janus‚Äôs proficiency in both multimodal\nunderstanding and generation, we don‚Äôt fine-tune separate models for a certain task. Instead,\nwe use a blend of pure text dialogue data, multimodal understanding data and visual generation\ndata, ensuring versatility across various scenarios.\n3.3. Training Objective\nJanus is an autoregressive model, and we simply adopt the cross-entropy loss during training:\nL=‚àí‚àëÔ∏Å\nùëñ=1logùëÉùúÉ(ùë•ùëñ|ùë•<ùëñ) (1)\nHere,ùëÉ(¬∑|¬∑) indicates the conditional probability modeled by the weights ùúÉof Janus. For\npure text understanding and multimodal understanding tasks, we compute the loss on the text\n5 ",
      " demands that understanding and generation place on the visual encoder. Extensive experi-\nments have demonstrated the effectiveness and leading performance of Janus. It is also worth\nnoting that Janus is flexible and easy to extend. In addition to having significant potential for\nimprovement in both multimodal understanding and generation, Janus is also easily extendable\nto incorporate more input modalities. The above advantages suggest that Janus may serve as an\ninspiration for the development of the next generation of multimodal general-purpose models.\n13 ",
      " Table 1|Detailed hyperparameters of our Janus . Data ratio refers to the ratio of multimodal\nunderstanding data, pure text data, and visual generation data.\nHyperparameters Stage 1 Stage 2 Stage 3\nLearning rate 1.0 √ó10‚àí31√ó10‚àí42.0√ó10‚àí5\nLR scheduler Cosine Constant Constant\nWeight decay 0.0 0.0 0.1\nGradient clip 1.0 1.0 1.0\nOptimizer AdamW ( ùõΩ1=0.9,ùõΩ2=0.95)\nWarm-up steps 300 5, 000 0\nTraining steps 10, 000 180, 000 24, 000\nBatch size 256 512 256\nData Ratio 1 : 0 : 1 2 : 3 : 5 7 : 3 : 10\n4.1. Implementation Details\nIn our experiments, we utilize DeepSeek-LLM ( 1.3B) [5] with a maximum supported sequence\nlength of 4096 as the base language model. For the vision encoder used in understanding tasks,\nwe select SigLIP-Large-Patch 16-384[92]. The generation encoder has a codebook of size 16, 384\nand downsamples images by a factor of 16. Both the understanding adaptor and the generation\nadaptor are two-layer MLPs. The detailed hyperparameters for each stage are provided in\nTable 1. All images are resized to 384√ó384pixels. For multimodal understanding data, we\nresize the long side of the image and pad the short side with the background color (RGB: 127,\n127,127) to reach 384. For visual generation data, the short side is resized to 384, and the long\nside is cropped to 384. We use sequence packing during training to improve training efficiency.\nWe mix all data types according to the specified ratios in a single training step. Our Janus is"
    ],
    "response": "The dataset used for Janus's training includes pure text data, multimodal understanding data, and visual generation data. Specifically, this involves ImageNet-1k for initial visual generation training and general text-to-image datasets for enhancing its sequence completion capabilities.",
    "reference": "In stage I, it uses 1.25 million image - text paired captions from ShareGPT4V for multimodal understanding and approximately 1.2 million samples from ImageNet - 1k for visual generation. In stage II, it uses text - only data from DeepSeek - LLM, interleaved image - text data from WikiHow and WIT, image caption data from multiple sources, table and chart data from DeepSeek - VL, and visual generation data from various datasets. In stage III, it uses data from different sources for text understanding, multimodal understanding, and visual generation, with specific formatting for instruction tuning."
  },
  {
    "user_input": "How does Janus compare with previous unified models in terms of performance?",
    "retrieved_contexts": [
      " demands that understanding and generation place on the visual encoder. Extensive experi-\nments have demonstrated the effectiveness and leading performance of Janus. It is also worth\nnoting that Janus is flexible and easy to extend. In addition to having significant potential for\nimprovement in both multimodal understanding and generation, Janus is also easily extendable\nto incorporate more input modalities. The above advantages suggest that Janus may serve as an\ninspiration for the development of the next generation of multimodal general-purpose models.\n13 ",
      " results among models of similar scale. Specifically, compared to the previous best unified model,\nShow-o [ 86], we achieve performance improvements of 41% (949‚Üí1338 ) and 30% (48.7‚Üí59.1)\non the MME and GQA datasets, respectively. This can be attributed to Janus decoupling the\nvisual encoding for multimodal understanding and generation, mitigating the conflict between\nthese two tasks. When compared to models with significantly larger sizes, Janus remains highly\ncompetitive. For instance, Janus outperforms LLaVA-v 1.5(7B) on several datasets, including\nPOPE, MMbench, SEED Bench, and MM-Vet.\nVisual Generation Performance. We report visual generation performance on GenEval, COCO-\n30K and MJHQ- 30K benchmarks. As shown in Table 3, our Janus obtains 61% overall accuracy\non GenEval, which outperforms the previous best unified model Show-o ( 53%) and some\npopular generation-only methods, e.g., SDXL ( 55%) and DALL-E 2(52%). This demonstrates\nthat our approach has better instruction-following capabilities. As shown in Table 4, Janus\nachieves FIDs of 8.53 and 10.10 on the COCO- 30K and MJHQ- 30K benchmarks, respectively,\nsurpassing unified models Show-o and LWM, and demonstrating competitive performance\ncompared to some well-known generation-only methods. This demonstrates that the images\ngenerated by Janus have good quality and highlights its potential in visual generation.\n4.5. Ablation Studies ",
      " techniques specific to their domain. Moreover, it is possible for Janus to accommodate additional\ninput types in the future, such as point clouds, EEG signals, or audio data, where independent\nencoders can extract features and then use a unified transformer to process them.\nTo the best of our knowledge, we are the first to highlight the importance of decoupling\nvisual encoding within the unified multimodal understanding and generation framework.\nOur experimental results show that Janus surpasses existing unified models with comparable\nparameter sizes on both multimodal understanding and generation benchmarks, achieving\nstate-of-the-art results. Notably, Janus even outperforms some task-specific models which have\nsignificantly more parameters (Figure 1). Specifically, on multimodal understanding benchmarks\nMMBench [ 54], SEED-Bench [ 42], and POPE [ 48], Janus ( 1.3B) achieved scores of 69.4,63.7, and\n87.0, respectively, outperforming LLaVA-v 1.5(7B) [50] and Qwen-VL-Chat ( 7B) [3] . On visual\ngeneration benchmarks MSCOCO- 30K [11] and GenEval [ 30], Janus achieved an FID score of\n8.53 and an accuracy of 61%, surpassing text-to-image generative models such as DALL-E 2[66]\nand SDXL [ 62]. We believe that the strong performance, coupled with the high flexibility and\nextensibility of Janus, presents it as a strong candidate for next-generation unified multimodal\nmodels.\n2. Related Work\n2.1. Visual Generation"
    ],
    "response": "Janus shows better performance compared to previous unified models such as Show-o, achieving improvements of 41% and 30% on the MME and GQA datasets respectively. Additionally, it outperforms large-scale models like LLaVA-v 1.5(7B) on various benchmarks.",
    "reference": "Janus outperforms previous unified models on both multimodal understanding and generation benchmarks. On multimodal understanding benchmarks like MMBench, SEED - Bench, and POPE, Janus (1.3B) achieves higher scores than models such as LLaVA - v1.5 (7B) and Qwen - VL - Chat (7B). In visual generation, it has better results than some well - known models like DALL - E 2 and SDXL on benchmarks like MSCOCO - 30K and GenEval."
  },
  {
    "user_input": "What are the possible extensions of Janus in multimodal understanding?",
    "retrieved_contexts": [
      " demands that understanding and generation place on the visual encoder. Extensive experi-\nments have demonstrated the effectiveness and leading performance of Janus. It is also worth\nnoting that Janus is flexible and easy to extend. In addition to having significant potential for\nimprovement in both multimodal understanding and generation, Janus is also easily extendable\nto incorporate more input modalities. The above advantages suggest that Janus may serve as an\ninspiration for the development of the next generation of multimodal general-purpose models.\n13 ",
      " The results show that our model demonstrates superior instruction-following capabilities in\nvisual generation, accurately capturing most of details in the user‚Äôs prompt. This indicates the\npotential of the unified model in the realm of visual generation. More visualizations can be\nfound in the Appendix B.\nMultimodal Understanding on MEME Images. Figure 5 showcases the qualitative results of\nJanus‚Äôs multimodal understanding ability, compared with Chameleon [ 77] and Show-o [ 86].\nJanus accurately interprets the text caption and captures the emotion conveyed in the meme. In\ncontrast, both Chameleon and Show-o struggle with accurately recognizing the text in the image.\nAdditionally, Chameleon fails to identify objects in the meme, while Show-o misinterprets the\ndog‚Äôs color. These examples highlight that the decoupled vision encoder significantly enhances\nJanus‚Äôs fine-grained multimodal understanding ability compared to the shared encoder used\nby Chameleon and Show-o. More multimodal understanding exmples can be found in the\nAppendix B.\n5. Conclusion\nIn this paper, we introduced Janus, a simple, unified and extensible multimodal understanding\nand generation model. The core idea of Janus is to decouple visual encoding for multimodal\nunderstanding and generation, which could alleviate the conflict arising from the differing\ndemands that understanding and generation place on the visual encoder. Extensive experi- ",
      " Janus: Decoupling Visual Encoding for Unified\nMultimodal Understanding and Generation\nChengyue Wu1,2Xiaokang Chen1,‚àó,‚Ä†Zhiyu Wu1,3Yiyang Ma1,3Xingchao Liu1Zizheng Pan1\nWen Liu1Zhenda Xie1Xingkai Yu1Chong Ruan1Ping Luo2,‚àó\n1DeepSeek-AI2The University of Hong Kong3Peking University\n‚Ä†: Project lead‚àó: Corresponding authors\nProject Page: https://github.com/deepseek-ai/Janus\nAbstract\nIn this paper, we introduce Janus , an autoregressive framework that unifies multimodal under-\nstanding and generation. Prior research often relies on a single visual encoder for both tasks,\nsuch as Chameleon. However, due to the differing levels of information granularity required by\nmultimodal understanding and generation, this approach can lead to suboptimal performance,\nparticularly in multimodal understanding. To address this issue, we decouple visual encoding\ninto separate pathways, while still leveraging a single, unified transformer architecture for\nprocessing. The decoupling not only alleviates the conflict between the visual encoder‚Äôs roles in\nunderstanding and generation, but also enhances the framework‚Äôs flexibility. For instance, both\nthe multimodal understanding and generation components can independently select their most\nsuitable encoding methods. Experiments show that Janus surpasses previous unified model\nand matches or exceeds the performance of task-specific models. The simplicity, high flexibility,"
    ],
    "response": "Janus is easily extendable to incorporate more input modalities for multimodal understanding and generation. This makes it highly flexible for various applications. The core architecture also allows for separate pathways for visual encoding, which enhances fine-grained multimodal understanding.",
    "reference": "In multimodal understanding, Janus can use stronger vision encoders like EVA - CLIP or InternViT. It can also use dynamic high - resolution techniques to handle high - resolution images and compress tokens using operations like pixel shuffle to save computational cost."
  },
  {
    "user_input": "What are the possible extensions of Janus in visual generation?",
    "retrieved_contexts": [
      " demands that understanding and generation place on the visual encoder. Extensive experi-\nments have demonstrated the effectiveness and leading performance of Janus. It is also worth\nnoting that Janus is flexible and easy to extend. In addition to having significant potential for\nimprovement in both multimodal understanding and generation, Janus is also easily extendable\nto incorporate more input modalities. The above advantages suggest that Janus may serve as an\ninspiration for the development of the next generation of multimodal general-purpose models.\n13 ",
      " techniques specific to their domain. Moreover, it is possible for Janus to accommodate additional\ninput types in the future, such as point clouds, EEG signals, or audio data, where independent\nencoders can extract features and then use a unified transformer to process them.\nTo the best of our knowledge, we are the first to highlight the importance of decoupling\nvisual encoding within the unified multimodal understanding and generation framework.\nOur experimental results show that Janus surpasses existing unified models with comparable\nparameter sizes on both multimodal understanding and generation benchmarks, achieving\nstate-of-the-art results. Notably, Janus even outperforms some task-specific models which have\nsignificantly more parameters (Figure 1). Specifically, on multimodal understanding benchmarks\nMMBench [ 54], SEED-Bench [ 42], and POPE [ 48], Janus ( 1.3B) achieved scores of 69.4,63.7, and\n87.0, respectively, outperforming LLaVA-v 1.5(7B) [50] and Qwen-VL-Chat ( 7B) [3] . On visual\ngeneration benchmarks MSCOCO- 30K [11] and GenEval [ 30], Janus achieved an FID score of\n8.53 and an accuracy of 61%, surpassing text-to-image generative models such as DALL-E 2[66]\nand SDXL [ 62]. We believe that the strong performance, coupled with the high flexibility and\nextensibility of Janus, presents it as a strong candidate for next-generation unified multimodal\nmodels.\n2. Related Work\n2.1. Visual Generation ",
      " any resolution, without performing positional embedding interpolation for ViTs. Tokens can be\nfurther compressed to save computational cost, for instance, using pixel shuffle operation [12].\nVisual Generation. (1) For visual generation, finer-grained encoders can be chosen in order\nto preserve more image details after encoding, such as MoVQGan [ 93]. (2) Loss functions\nspecifically designed for visual generation can be employed, such as diffusion loss [ 46]. (3) A\ncombination of AR (causal attention) and parallel (bidirectional attention) methods can be used\nin the visual generation process to reduce accumulated errors during visual generation [79].\nSupport for Additional Modalities. The straightforward architecture of Janus allows for easy\nintegration with additional encoders, accommodating various modalities such as 3D point\ncloud [ 53], tactile [ 88], and EEG [ 4]. This gives Janus the potential to become a more powerful\nmultimodal generalist model.\n4. Experiments\nIn this section, we present a series of comprehensive experiments designed to assess the perfor-\nmance of our method across a range of visual understanding and generation tasks. We begin by\ndetailing our experimental setup, which includes the model architecture, training datasets, and\nevaluation benchmarks. Next, we report the performance of Janus, followed by a comparison\nwith other state-of-the-art models on various benchmarks for multimodal understanding and"
    ],
    "response": "The possible extensions of Janus in visual generation include using finer-grained encoders, such as MoVQGan [ 93], to preserve more image details after encoding. Additionally, loss functions specifically designed for visual generation, like diffusion loss [ 46], can be employed to improve performance.",
    "reference": "In visual generation, Janus can choose finer - grained encoders like MoVQGan to preserve more image details. It can employ loss functions specifically designed for visual generation, such as diffusion loss. Also, a combination of AR (causal attention) and parallel (bidirectional attention) methods can be used to reduce accumulated errors during visual generation."
  },
  {
    "user_input": "Why is Janus considered a strong candidate for next - generation unified multimodal models?",
    "retrieved_contexts": [
      " demands that understanding and generation place on the visual encoder. Extensive experi-\nments have demonstrated the effectiveness and leading performance of Janus. It is also worth\nnoting that Janus is flexible and easy to extend. In addition to having significant potential for\nimprovement in both multimodal understanding and generation, Janus is also easily extendable\nto incorporate more input modalities. The above advantages suggest that Janus may serve as an\ninspiration for the development of the next generation of multimodal general-purpose models.\n13 ",
      " Janus: Decoupling Visual Encoding for Unified\nMultimodal Understanding and Generation\nChengyue Wu1,2Xiaokang Chen1,‚àó,‚Ä†Zhiyu Wu1,3Yiyang Ma1,3Xingchao Liu1Zizheng Pan1\nWen Liu1Zhenda Xie1Xingkai Yu1Chong Ruan1Ping Luo2,‚àó\n1DeepSeek-AI2The University of Hong Kong3Peking University\n‚Ä†: Project lead‚àó: Corresponding authors\nProject Page: https://github.com/deepseek-ai/Janus\nAbstract\nIn this paper, we introduce Janus , an autoregressive framework that unifies multimodal under-\nstanding and generation. Prior research often relies on a single visual encoder for both tasks,\nsuch as Chameleon. However, due to the differing levels of information granularity required by\nmultimodal understanding and generation, this approach can lead to suboptimal performance,\nparticularly in multimodal understanding. To address this issue, we decouple visual encoding\ninto separate pathways, while still leveraging a single, unified transformer architecture for\nprocessing. The decoupling not only alleviates the conflict between the visual encoder‚Äôs roles in\nunderstanding and generation, but also enhances the framework‚Äôs flexibility. For instance, both\nthe multimodal understanding and generation components can independently select their most\nsuitable encoding methods. Experiments show that Janus surpasses previous unified model\nand matches or exceeds the performance of task-specific models. The simplicity, high flexibility, ",
      " techniques specific to their domain. Moreover, it is possible for Janus to accommodate additional\ninput types in the future, such as point clouds, EEG signals, or audio data, where independent\nencoders can extract features and then use a unified transformer to process them.\nTo the best of our knowledge, we are the first to highlight the importance of decoupling\nvisual encoding within the unified multimodal understanding and generation framework.\nOur experimental results show that Janus surpasses existing unified models with comparable\nparameter sizes on both multimodal understanding and generation benchmarks, achieving\nstate-of-the-art results. Notably, Janus even outperforms some task-specific models which have\nsignificantly more parameters (Figure 1). Specifically, on multimodal understanding benchmarks\nMMBench [ 54], SEED-Bench [ 42], and POPE [ 48], Janus ( 1.3B) achieved scores of 69.4,63.7, and\n87.0, respectively, outperforming LLaVA-v 1.5(7B) [50] and Qwen-VL-Chat ( 7B) [3] . On visual\ngeneration benchmarks MSCOCO- 30K [11] and GenEval [ 30], Janus achieved an FID score of\n8.53 and an accuracy of 61%, surpassing text-to-image generative models such as DALL-E 2[66]\nand SDXL [ 62]. We believe that the strong performance, coupled with the high flexibility and\nextensibility of Janus, presents it as a strong candidate for next-generation unified multimodal\nmodels.\n2. Related Work\n2.1. Visual Generation"
    ],
    "response": "The main reason is that understanding and generation have differing demands on the visual encoder in traditional integrated encoding schemes.",
    "reference": "Janus is considered a strong candidate because it has a simple, unified, and flexible architecture. It decouples visual encoding, which leads to high performance on both multimodal understanding and generation tasks, even outperforming some task - specific models. It is also easy to extend, with the potential to incorporate more input modalities, such as 3D point cloud, tactile, and EEG data."
  },
  {
    "user_input": "What is the main innovation of Kimi k1.5?",
    "retrieved_contexts": [
      " Kimi k1.5 TECHNICAL REPORT\n3.2 Main Results\nK1.5 long-CoT model The performance of the Kimi k1.5 long-CoT model is presented in Table 2. Through long-CoT\nsupervised fine-tuning (described in Section 2.2) and vision-text joint reinforcement learning (discussed in Section 2.3),\nthe model‚Äôs long-term reasoning capabilities are enhanced significantly. The test-time computation scaling further\nstrengthens its performance, enabling the model to achieve state-of-the-art results across a range of modalities. Our\nevaluation reveals marked improvements in the model‚Äôs capacity to reason, comprehend, and synthesize information\nover extended contexts, representing a advancement in multi-modal AI capabilities.\nK1.5 short-CoT model The performance of the Kimi k1.5 short-CoT model is presented in Table 3. This model\nintegrates several techniques, including traditional supervised fine-tuning (discussed in Section 2.5.2), reinforcement\nlearning (explored in Section 2.3), and long-to-short distillation (outlined in Section 2.4). The results demonstrate\nthat the k1.5 short-CoT model delivers competitive or superior performance compared to leading open-source and\nproprietary models across multiple tasks. These include text, vision, and reasoning challenges, with notable strengths in\nnatural language understanding, mathematics, coding, and logical reasoning.\nBenchmark (Metric)Language-only Model Vision-Language Model\nQwQ-32B OpenAI QVQ-72B OpenAI Kimi\nPreview o1-mini Preview o1 k1.5 ",
      " Kimi k1.5 TECHNICAL REPORT\nKimi k1.5 short-CoTOpenAI 4oClaude 3.5 Sonnet DeepSeek V3LLaMA-3.1 405B-Inst.\nGeneralCodeMath\n47.333.4\nLiveCodeBench v4 24.08-24.11 (Pass@1-COT)\n36.328.440.531.1Qwen2.5 72B-Inst.\n87.487.2\nMMLU (EM)\n88.388.688.585.3\n94.674.6\nMATH-500 (EM)\n78.373.890.280\n60.8\n9.3AIME 2024 (Pass@1)\n1623.339.223.3\n6869.1\nMMMU_val(Pass@1)\n66.464.5\n70.163.8\nMathVista_test(Pass@1)\n65.369.7\n87.284.3\nIF-Eval (Prompt Strict)\n86.58686.184.1\n91.787.9\nCLUEWSC (EM)\n85.484.790.991.4\n88.376\nC-Eval (EM)\n76.761.586.586.1Qwen2-VLVision\nFigure 2: Kimi k1.5 short-CoT results.\n1 Introduction\nLanguage model pretraining with next token prediction has been studied under the context of the scaling law, where\nproportionally scaling model parameters and data sizes leads to the continued improvement of intelligence. (Kaplan\net al. 2020; Hoffmann et al. 2022) However, this approach is limited to the amount of available high-quality training\ndata (Villalobos et al. 2024; Muennighoff et al. 2023). In this report, we present the training recipe of Kimi k1.5,\nour latest multi-modal LLM trained with reinforcement learning (RL). The goal is to explore a possible new axis\nfor continued scaling. Using RL with LLMs, the models learns to explore with rewards and thus is not limited to a\npre-existing static dataset.\nThere are a few key ingredients about the design and training of k1.5.\n‚Ä¢Long context scaling . We scale the context window of RL to 128k and observe continued improvement of ",
      " Kimi k1.5 TECHNICAL REPORT\n2.6.1 Large Scale Reinforcement Learning Training System for LLM\nIn the realm of artificial intelligence, reinforcement learning (RL) has emerged as a pivotal training methodology\nfor large language models (LLMs)(Ouyang et al. 2022)(Jaech et al. 2024), drawing inspiration from its success in\nmastering complex games like Go, StarCraft II, and Dota 2 through systems such as AlphaGo(Silver et al. 2017),\nAlphaStar(Vinyals et al. 2019), and OpenAI Dota Five (Berner et al. 2019). Following in this tradition, the Kimi\nk1.5 system adopts an iterative synchronous RL framework, meticulously designed to bolster the model‚Äôs reasoning\ncapabilities through persistent learning and adaptation. A key innovation in this system is the introduction of a Partial\nRollout technique, designed to optimize the handling of complex reasoning trajectories.\nThe RL training system as illustrated in Figure 3a operates through an iterative synchronous approach, with each\niteration encompassing a rollout phase and a training phase. During the rollout phase, rollout workers, coordinated\nby a central master, generate rollout trajectories by interacting with the model, producing sequences of responses to\nvarious inputs. These trajectories are then stored in a replay buffer, which ensures a diverse and unbiased dataset for\ntraining by disrupting temporal correlations. In the subsequent training phase, trainer workers access these experiences"
    ],
    "response": "The main innovation of Kimi k1.5 is the adoption of reinforcement learning with partial rollback technology to optimize complex reasoning trajectories. This enables long-term sequence processing, improving performance in tasks such as math and natural language understanding.\n\nReinforcement learning optimizes model performance by incorporating rewards for correct responses and penalties for errors, leading to enhanced adaptability and reasoning capabilities.",
    "reference": "The main innovation is scaling reinforcement learning (RL) with LLMs by introducing long context scaling (scaling the RL context window to 128k) and improved policy optimization methods. It also presents long2short methods to improve short-CoT models, achieving state-of-the-art reasoning performance across multiple benchmarks and modalities without relying on complex techniques like Monte Carlo tree search."
  },
  {
    "user_input": "How does Kimi k1.5 perform in different benchmarks compared to other models?",
    "retrieved_contexts": [
      " Kimi k1.5 TECHNICAL REPORT\n3.2 Main Results\nK1.5 long-CoT model The performance of the Kimi k1.5 long-CoT model is presented in Table 2. Through long-CoT\nsupervised fine-tuning (described in Section 2.2) and vision-text joint reinforcement learning (discussed in Section 2.3),\nthe model‚Äôs long-term reasoning capabilities are enhanced significantly. The test-time computation scaling further\nstrengthens its performance, enabling the model to achieve state-of-the-art results across a range of modalities. Our\nevaluation reveals marked improvements in the model‚Äôs capacity to reason, comprehend, and synthesize information\nover extended contexts, representing a advancement in multi-modal AI capabilities.\nK1.5 short-CoT model The performance of the Kimi k1.5 short-CoT model is presented in Table 3. This model\nintegrates several techniques, including traditional supervised fine-tuning (discussed in Section 2.5.2), reinforcement\nlearning (explored in Section 2.3), and long-to-short distillation (outlined in Section 2.4). The results demonstrate\nthat the k1.5 short-CoT model delivers competitive or superior performance compared to leading open-source and\nproprietary models across multiple tasks. These include text, vision, and reasoning challenges, with notable strengths in\nnatural language understanding, mathematics, coding, and logical reasoning.\nBenchmark (Metric)Language-only Model Vision-Language Model\nQwQ-32B OpenAI QVQ-72B OpenAI Kimi\nPreview o1-mini Preview o1 k1.5 ",
      " Kimi k1.5 TECHNICAL REPORT\nKimi k1.5 short-CoTOpenAI 4oClaude 3.5 Sonnet DeepSeek V3LLaMA-3.1 405B-Inst.\nGeneralCodeMath\n47.333.4\nLiveCodeBench v4 24.08-24.11 (Pass@1-COT)\n36.328.440.531.1Qwen2.5 72B-Inst.\n87.487.2\nMMLU (EM)\n88.388.688.585.3\n94.674.6\nMATH-500 (EM)\n78.373.890.280\n60.8\n9.3AIME 2024 (Pass@1)\n1623.339.223.3\n6869.1\nMMMU_val(Pass@1)\n66.464.5\n70.163.8\nMathVista_test(Pass@1)\n65.369.7\n87.284.3\nIF-Eval (Prompt Strict)\n86.58686.184.1\n91.787.9\nCLUEWSC (EM)\n85.484.790.991.4\n88.376\nC-Eval (EM)\n76.761.586.586.1Qwen2-VLVision\nFigure 2: Kimi k1.5 short-CoT results.\n1 Introduction\nLanguage model pretraining with next token prediction has been studied under the context of the scaling law, where\nproportionally scaling model parameters and data sizes leads to the continued improvement of intelligence. (Kaplan\net al. 2020; Hoffmann et al. 2022) However, this approach is limited to the amount of available high-quality training\ndata (Villalobos et al. 2024; Muennighoff et al. 2023). In this report, we present the training recipe of Kimi k1.5,\nour latest multi-modal LLM trained with reinforcement learning (RL). The goal is to explore a possible new axis\nfor continued scaling. Using RL with LLMs, the models learns to explore with rewards and thus is not limited to a\npre-existing static dataset.\nThere are a few key ingredients about the design and training of k1.5.\n‚Ä¢Long context scaling . We scale the context window of RL to 128k and observe continued improvement of ",
      " Kimi k1.5 TECHNICAL REPORT\nLong2short RL After a standard RL training phase, we select a model that offers the best balance between perfor-\nmance and token efficiency to serve as the base model, and conduct a separate long2short RL training phase. In this\nsecond phase, we apply the length penalty introduced in Section 2.3.3, and significantly reduce the maximum rollout\nlength to further penalize responses that exceed the desired length while possibly correct.\n2.5 Other Training Details\n2.5.1 Pretraining\nThe Kimi k1.5 base model is trained on a diverse, high-quality multimodal corpus. The language data covers five\ndomains: English, Chinese, Code, Mathematics Reasoning, and Knowledge. Multimodal data, including Captioning,\nImage-text Interleaving, OCR, Knowledge, and QA datasets, enables our model to acquire vision-language capabilities.\nRigorous quality control ensures relevance, diversity, and balance in the overall pretrain dataset. Our pretraining\nproceeds in three stages: (1) Vision-language pretraining, where a strong language foundation is established, followed\nby gradual multimodal integration; (2) Cooldown, which consolidates capabilities using curated and synthetic data,\nparticularly for reasoning and knowledge-based tasks; and (3) Long-context activation, extending sequence processing\nto 131,072 tokens. More details regarding our pretraining efforts can be found in Appendix B.\n2.5.2 Vanilla Supervised Finetuning"
    ],
    "response": "The performance comparison between Kimi k1.5 models with other models isn't present in the provided files. Therefore, I know that their results may or may not match.",
    "reference": "In long-CoT, it achieves 77.5 on AIME, 96.2 on MATH 500, 94-th percentile on Codeforces, 74.9 on MathVista, matching or outperforming models like OpenAI‚Äôs o1. In short-CoT, it gets 60.8 on AIME, 94.6 on MATH500, 47.3 on LiveCodeBench, outperforming models such as GPT-4o and Claude Sonnet 3.5 by a large margin (up to +550%)."
  },
  {
    "user_input": "What are the key ingredients of Kimi k1.5's design and training?",
    "retrieved_contexts": [
      " Kimi k1.5 TECHNICAL REPORT\nKimi k1.5 short-CoTOpenAI 4oClaude 3.5 Sonnet DeepSeek V3LLaMA-3.1 405B-Inst.\nGeneralCodeMath\n47.333.4\nLiveCodeBench v4 24.08-24.11 (Pass@1-COT)\n36.328.440.531.1Qwen2.5 72B-Inst.\n87.487.2\nMMLU (EM)\n88.388.688.585.3\n94.674.6\nMATH-500 (EM)\n78.373.890.280\n60.8\n9.3AIME 2024 (Pass@1)\n1623.339.223.3\n6869.1\nMMMU_val(Pass@1)\n66.464.5\n70.163.8\nMathVista_test(Pass@1)\n65.369.7\n87.284.3\nIF-Eval (Prompt Strict)\n86.58686.184.1\n91.787.9\nCLUEWSC (EM)\n85.484.790.991.4\n88.376\nC-Eval (EM)\n76.761.586.586.1Qwen2-VLVision\nFigure 2: Kimi k1.5 short-CoT results.\n1 Introduction\nLanguage model pretraining with next token prediction has been studied under the context of the scaling law, where\nproportionally scaling model parameters and data sizes leads to the continued improvement of intelligence. (Kaplan\net al. 2020; Hoffmann et al. 2022) However, this approach is limited to the amount of available high-quality training\ndata (Villalobos et al. 2024; Muennighoff et al. 2023). In this report, we present the training recipe of Kimi k1.5,\nour latest multi-modal LLM trained with reinforcement learning (RL). The goal is to explore a possible new axis\nfor continued scaling. Using RL with LLMs, the models learns to explore with rewards and thus is not limited to a\npre-existing static dataset.\nThere are a few key ingredients about the design and training of k1.5.\n‚Ä¢Long context scaling . We scale the context window of RL to 128k and observe continued improvement of ",
      " Kimi k1.5 TECHNICAL REPORT\n3.2 Main Results\nK1.5 long-CoT model The performance of the Kimi k1.5 long-CoT model is presented in Table 2. Through long-CoT\nsupervised fine-tuning (described in Section 2.2) and vision-text joint reinforcement learning (discussed in Section 2.3),\nthe model‚Äôs long-term reasoning capabilities are enhanced significantly. The test-time computation scaling further\nstrengthens its performance, enabling the model to achieve state-of-the-art results across a range of modalities. Our\nevaluation reveals marked improvements in the model‚Äôs capacity to reason, comprehend, and synthesize information\nover extended contexts, representing a advancement in multi-modal AI capabilities.\nK1.5 short-CoT model The performance of the Kimi k1.5 short-CoT model is presented in Table 3. This model\nintegrates several techniques, including traditional supervised fine-tuning (discussed in Section 2.5.2), reinforcement\nlearning (explored in Section 2.3), and long-to-short distillation (outlined in Section 2.4). The results demonstrate\nthat the k1.5 short-CoT model delivers competitive or superior performance compared to leading open-source and\nproprietary models across multiple tasks. These include text, vision, and reasoning challenges, with notable strengths in\nnatural language understanding, mathematics, coding, and logical reasoning.\nBenchmark (Metric)Language-only Model Vision-Language Model\nQwQ-32B OpenAI QVQ-72B OpenAI Kimi\nPreview o1-mini Preview o1 k1.5 ",
      " Kimi k1.5 TECHNICAL REPORT\nLong2short RL After a standard RL training phase, we select a model that offers the best balance between perfor-\nmance and token efficiency to serve as the base model, and conduct a separate long2short RL training phase. In this\nsecond phase, we apply the length penalty introduced in Section 2.3.3, and significantly reduce the maximum rollout\nlength to further penalize responses that exceed the desired length while possibly correct.\n2.5 Other Training Details\n2.5.1 Pretraining\nThe Kimi k1.5 base model is trained on a diverse, high-quality multimodal corpus. The language data covers five\ndomains: English, Chinese, Code, Mathematics Reasoning, and Knowledge. Multimodal data, including Captioning,\nImage-text Interleaving, OCR, Knowledge, and QA datasets, enables our model to acquire vision-language capabilities.\nRigorous quality control ensures relevance, diversity, and balance in the overall pretrain dataset. Our pretraining\nproceeds in three stages: (1) Vision-language pretraining, where a strong language foundation is established, followed\nby gradual multimodal integration; (2) Cooldown, which consolidates capabilities using curated and synthetic data,\nparticularly for reasoning and knowledge-based tasks; and (3) Long-context activation, extending sequence processing\nto 131,072 tokens. More details regarding our pretraining efforts can be found in Appendix B.\n2.5.2 Vanilla Supervised Finetuning"
    ],
    "response": "The design of Kimi k1.5 incorporates long context scaling up to 128k tokens. The training process involves reinforcement learning for vision-text joint and traditional supervised fine-tuning alongside long-to-short distillation methodology.",
    "reference": "Key ingredients include long context scaling, improved policy optimization (deriving a formulation of RL with long-CoT and using a variant of online mirror descent), a simplistic framework, multimodal training (text and vision data), and effective long2short methods."
  },
  {
    "user_input": "How is the RL prompt set curated for Kimi k1.5?",
    "retrieved_contexts": [
      " Kimi k1.5 TECHNICAL REPORT\n2 Approach: Reinforcement Learning with LLMs\nThe development of Kimi k1.5 consists of several stages: pretraining, vanilla supervised fine-tuning (SFT), long-CoT\nsupervised fine-turning, and reinforcement learning (RL). This report focuses on RL, beginning with an overview of\nthe RL prompt set curation (Section 2.1) and long-CoT supervised finetuning (Section 2.2), followed by an in-depth\ndiscussion of RL training strategies in Section 2.3. Additional details on pretraining and vanilla supervised finetuning\ncan be found in Section 2.5.\n2.1 RL Prompt Set Curation\nThrough our preliminary experiments, we found that the quality and diversity of the RL prompt set play a critical role in\nensuring the effectiveness of reinforcement learning. A well-constructed prompt set not only guides the model toward\nrobust reasoning but also mitigates the risk of reward hacking and overfitting to superficial patterns. Specifically, three\nkey properties define a high-quality RL prompt set:\n‚Ä¢Diverse Coverage : Prompts should span a wide array of disciplines, such as STEM, coding, and general reasoning,\nto enhance the model‚Äôs adaptability and ensure broad applicability across different domains.\n‚Ä¢Balanced Difficulty : The prompt set should include a well-distributed range of easy, moderate, and difficult\nquestions to facilitate gradual learning and prevent overfitting to specific complexity levels. ",
      " Kimi k1.5 TECHNICAL REPORT\nKimi k1.5 short-CoTOpenAI 4oClaude 3.5 Sonnet DeepSeek V3LLaMA-3.1 405B-Inst.\nGeneralCodeMath\n47.333.4\nLiveCodeBench v4 24.08-24.11 (Pass@1-COT)\n36.328.440.531.1Qwen2.5 72B-Inst.\n87.487.2\nMMLU (EM)\n88.388.688.585.3\n94.674.6\nMATH-500 (EM)\n78.373.890.280\n60.8\n9.3AIME 2024 (Pass@1)\n1623.339.223.3\n6869.1\nMMMU_val(Pass@1)\n66.464.5\n70.163.8\nMathVista_test(Pass@1)\n65.369.7\n87.284.3\nIF-Eval (Prompt Strict)\n86.58686.184.1\n91.787.9\nCLUEWSC (EM)\n85.484.790.991.4\n88.376\nC-Eval (EM)\n76.761.586.586.1Qwen2-VLVision\nFigure 2: Kimi k1.5 short-CoT results.\n1 Introduction\nLanguage model pretraining with next token prediction has been studied under the context of the scaling law, where\nproportionally scaling model parameters and data sizes leads to the continued improvement of intelligence. (Kaplan\net al. 2020; Hoffmann et al. 2022) However, this approach is limited to the amount of available high-quality training\ndata (Villalobos et al. 2024; Muennighoff et al. 2023). In this report, we present the training recipe of Kimi k1.5,\nour latest multi-modal LLM trained with reinforcement learning (RL). The goal is to explore a possible new axis\nfor continued scaling. Using RL with LLMs, the models learns to explore with rewards and thus is not limited to a\npre-existing static dataset.\nThere are a few key ingredients about the design and training of k1.5.\n‚Ä¢Long context scaling . We scale the context window of RL to 128k and observe continued improvement of ",
      " Kimi k1.5 TECHNICAL REPORT\nLong2short RL After a standard RL training phase, we select a model that offers the best balance between perfor-\nmance and token efficiency to serve as the base model, and conduct a separate long2short RL training phase. In this\nsecond phase, we apply the length penalty introduced in Section 2.3.3, and significantly reduce the maximum rollout\nlength to further penalize responses that exceed the desired length while possibly correct.\n2.5 Other Training Details\n2.5.1 Pretraining\nThe Kimi k1.5 base model is trained on a diverse, high-quality multimodal corpus. The language data covers five\ndomains: English, Chinese, Code, Mathematics Reasoning, and Knowledge. Multimodal data, including Captioning,\nImage-text Interleaving, OCR, Knowledge, and QA datasets, enables our model to acquire vision-language capabilities.\nRigorous quality control ensures relevance, diversity, and balance in the overall pretrain dataset. Our pretraining\nproceeds in three stages: (1) Vision-language pretraining, where a strong language foundation is established, followed\nby gradual multimodal integration; (2) Cooldown, which consolidates capabilities using curated and synthetic data,\nparticularly for reasoning and knowledge-based tasks; and (3) Long-context activation, extending sequence processing\nto 131,072 tokens. More details regarding our pretraining efforts can be found in Appendix B.\n2.5.2 Vanilla Supervised Finetuning"
    ],
    "response": "The RL prompt set curation for Kimi k1.5 includes diverse coverage across disciplines (STEM, coding, general reasoning), and balanced difficulty levels (easy, moderate, difficult) to ensure model adaptability and robust learning. This process prevents overfitting to specific complexity levels and promotes rewarding exploration.",
    "reference": "The RL prompt set is curated to have diverse coverage (spanning STEM, coding, general reasoning), balanced difficulty (including easy, moderate, and difficult questions), and accurate evaluability. It uses automatic filters, a tagging system, a model-based difficulty assessment method, and a process to remove easy-to-hack prompts."
  },
  {
    "user_input": "What is the process of long-CoT supervised fine-tuning in Kimi k1.5?",
    "retrieved_contexts": [
      " Kimi k1.5 TECHNICAL REPORT\n3.2 Main Results\nK1.5 long-CoT model The performance of the Kimi k1.5 long-CoT model is presented in Table 2. Through long-CoT\nsupervised fine-tuning (described in Section 2.2) and vision-text joint reinforcement learning (discussed in Section 2.3),\nthe model‚Äôs long-term reasoning capabilities are enhanced significantly. The test-time computation scaling further\nstrengthens its performance, enabling the model to achieve state-of-the-art results across a range of modalities. Our\nevaluation reveals marked improvements in the model‚Äôs capacity to reason, comprehend, and synthesize information\nover extended contexts, representing a advancement in multi-modal AI capabilities.\nK1.5 short-CoT model The performance of the Kimi k1.5 short-CoT model is presented in Table 3. This model\nintegrates several techniques, including traditional supervised fine-tuning (discussed in Section 2.5.2), reinforcement\nlearning (explored in Section 2.3), and long-to-short distillation (outlined in Section 2.4). The results demonstrate\nthat the k1.5 short-CoT model delivers competitive or superior performance compared to leading open-source and\nproprietary models across multiple tasks. These include text, vision, and reasoning challenges, with notable strengths in\nnatural language understanding, mathematics, coding, and logical reasoning.\nBenchmark (Metric)Language-only Model Vision-Language Model\nQwQ-32B OpenAI QVQ-72B OpenAI Kimi\nPreview o1-mini Preview o1 k1.5 ",
      " Kimi k1.5 TECHNICAL REPORT\nLong2short RL After a standard RL training phase, we select a model that offers the best balance between perfor-\nmance and token efficiency to serve as the base model, and conduct a separate long2short RL training phase. In this\nsecond phase, we apply the length penalty introduced in Section 2.3.3, and significantly reduce the maximum rollout\nlength to further penalize responses that exceed the desired length while possibly correct.\n2.5 Other Training Details\n2.5.1 Pretraining\nThe Kimi k1.5 base model is trained on a diverse, high-quality multimodal corpus. The language data covers five\ndomains: English, Chinese, Code, Mathematics Reasoning, and Knowledge. Multimodal data, including Captioning,\nImage-text Interleaving, OCR, Knowledge, and QA datasets, enables our model to acquire vision-language capabilities.\nRigorous quality control ensures relevance, diversity, and balance in the overall pretrain dataset. Our pretraining\nproceeds in three stages: (1) Vision-language pretraining, where a strong language foundation is established, followed\nby gradual multimodal integration; (2) Cooldown, which consolidates capabilities using curated and synthetic data,\nparticularly for reasoning and knowledge-based tasks; and (3) Long-context activation, extending sequence processing\nto 131,072 tokens. More details regarding our pretraining efforts can be found in Appendix B.\n2.5.2 Vanilla Supervised Finetuning ",
      " Kimi k1.5 TECHNICAL REPORT\nKimi k1.5 short-CoTOpenAI 4oClaude 3.5 Sonnet DeepSeek V3LLaMA-3.1 405B-Inst.\nGeneralCodeMath\n47.333.4\nLiveCodeBench v4 24.08-24.11 (Pass@1-COT)\n36.328.440.531.1Qwen2.5 72B-Inst.\n87.487.2\nMMLU (EM)\n88.388.688.585.3\n94.674.6\nMATH-500 (EM)\n78.373.890.280\n60.8\n9.3AIME 2024 (Pass@1)\n1623.339.223.3\n6869.1\nMMMU_val(Pass@1)\n66.464.5\n70.163.8\nMathVista_test(Pass@1)\n65.369.7\n87.284.3\nIF-Eval (Prompt Strict)\n86.58686.184.1\n91.787.9\nCLUEWSC (EM)\n85.484.790.991.4\n88.376\nC-Eval (EM)\n76.761.586.586.1Qwen2-VLVision\nFigure 2: Kimi k1.5 short-CoT results.\n1 Introduction\nLanguage model pretraining with next token prediction has been studied under the context of the scaling law, where\nproportionally scaling model parameters and data sizes leads to the continued improvement of intelligence. (Kaplan\net al. 2020; Hoffmann et al. 2022) However, this approach is limited to the amount of available high-quality training\ndata (Villalobos et al. 2024; Muennighoff et al. 2023). In this report, we present the training recipe of Kimi k1.5,\nour latest multi-modal LLM trained with reinforcement learning (RL). The goal is to explore a possible new axis\nfor continued scaling. Using RL with LLMs, the models learns to explore with rewards and thus is not limited to a\npre-existing static dataset.\nThere are a few key ingredients about the design and training of k1.5.\n‚Ä¢Long context scaling . We scale the context window of RL to 128k and observe continued improvement of"
    ],
    "response": "The process of long-CoT supervised fine-tuning in Kimi k1.5 isn't explicitly described; however, it likely involves extending sequence processing and reinforcing joint vision-text capabilities during supervised training phases.",
    "reference": "A small yet high-quality long-CoT warmup dataset is constructed using prompt engineering. It contains verified reasoning paths for text and image inputs. Then, a lightweight SFT is performed on this dataset to prime the model to internalize reasoning strategies and improve its performance in generating detailed and coherent responses."
  },
  {
    "user_input": "What is the policy optimization algorithm used in Kimi k1.5's RL training?",
    "retrieved_contexts": [
      " Kimi k1.5 TECHNICAL REPORT\n2.6.1 Large Scale Reinforcement Learning Training System for LLM\nIn the realm of artificial intelligence, reinforcement learning (RL) has emerged as a pivotal training methodology\nfor large language models (LLMs)(Ouyang et al. 2022)(Jaech et al. 2024), drawing inspiration from its success in\nmastering complex games like Go, StarCraft II, and Dota 2 through systems such as AlphaGo(Silver et al. 2017),\nAlphaStar(Vinyals et al. 2019), and OpenAI Dota Five (Berner et al. 2019). Following in this tradition, the Kimi\nk1.5 system adopts an iterative synchronous RL framework, meticulously designed to bolster the model‚Äôs reasoning\ncapabilities through persistent learning and adaptation. A key innovation in this system is the introduction of a Partial\nRollout technique, designed to optimize the handling of complex reasoning trajectories.\nThe RL training system as illustrated in Figure 3a operates through an iterative synchronous approach, with each\niteration encompassing a rollout phase and a training phase. During the rollout phase, rollout workers, coordinated\nby a central master, generate rollout trajectories by interacting with the model, producing sequences of responses to\nvarious inputs. These trajectories are then stored in a replay buffer, which ensures a diverse and unbiased dataset for\ntraining by disrupting temporal correlations. In the subsequent training phase, trainer workers access these experiences ",
      " Kimi k1.5 TECHNICAL REPORT\n2 Approach: Reinforcement Learning with LLMs\nThe development of Kimi k1.5 consists of several stages: pretraining, vanilla supervised fine-tuning (SFT), long-CoT\nsupervised fine-turning, and reinforcement learning (RL). This report focuses on RL, beginning with an overview of\nthe RL prompt set curation (Section 2.1) and long-CoT supervised finetuning (Section 2.2), followed by an in-depth\ndiscussion of RL training strategies in Section 2.3. Additional details on pretraining and vanilla supervised finetuning\ncan be found in Section 2.5.\n2.1 RL Prompt Set Curation\nThrough our preliminary experiments, we found that the quality and diversity of the RL prompt set play a critical role in\nensuring the effectiveness of reinforcement learning. A well-constructed prompt set not only guides the model toward\nrobust reasoning but also mitigates the risk of reward hacking and overfitting to superficial patterns. Specifically, three\nkey properties define a high-quality RL prompt set:\n‚Ä¢Diverse Coverage : Prompts should span a wide array of disciplines, such as STEM, coding, and general reasoning,\nto enhance the model‚Äôs adaptability and ensure broad applicability across different domains.\n‚Ä¢Balanced Difficulty : The prompt set should include a well-distributed range of easy, moderate, and difficult\nquestions to facilitate gradual learning and prevent overfitting to specific complexity levels. ",
      " Kimi k1.5 TECHNICAL REPORT\nKimi k1.5 short-CoTOpenAI 4oClaude 3.5 Sonnet DeepSeek V3LLaMA-3.1 405B-Inst.\nGeneralCodeMath\n47.333.4\nLiveCodeBench v4 24.08-24.11 (Pass@1-COT)\n36.328.440.531.1Qwen2.5 72B-Inst.\n87.487.2\nMMLU (EM)\n88.388.688.585.3\n94.674.6\nMATH-500 (EM)\n78.373.890.280\n60.8\n9.3AIME 2024 (Pass@1)\n1623.339.223.3\n6869.1\nMMMU_val(Pass@1)\n66.464.5\n70.163.8\nMathVista_test(Pass@1)\n65.369.7\n87.284.3\nIF-Eval (Prompt Strict)\n86.58686.184.1\n91.787.9\nCLUEWSC (EM)\n85.484.790.991.4\n88.376\nC-Eval (EM)\n76.761.586.586.1Qwen2-VLVision\nFigure 2: Kimi k1.5 short-CoT results.\n1 Introduction\nLanguage model pretraining with next token prediction has been studied under the context of the scaling law, where\nproportionally scaling model parameters and data sizes leads to the continued improvement of intelligence. (Kaplan\net al. 2020; Hoffmann et al. 2022) However, this approach is limited to the amount of available high-quality training\ndata (Villalobos et al. 2024; Muennighoff et al. 2023). In this report, we present the training recipe of Kimi k1.5,\nour latest multi-modal LLM trained with reinforcement learning (RL). The goal is to explore a possible new axis\nfor continued scaling. Using RL with LLMs, the models learns to explore with rewards and thus is not limited to a\npre-existing static dataset.\nThere are a few key ingredients about the design and training of k1.5.\n‚Ä¢Long context scaling . We scale the context window of RL to 128k and observe continued improvement of"
    ],
    "response": "The policy optimization algorithm used in Kimi k1.5's RL training is not specified directly, but it appears to utilize a sequence-to-sequence approach with training phases that leverage rollout workers generating diverse trajectories for biased datasets, suggesting a deep reinforcement learning method like PPO or TRPO may be applied.",
    "reference": "A variant of online policy mirror decent is used. At each iteration, it optimizes a relative entropy regularized policy optimization problem. The surrogate loss is calculated and gradients are taken using samples from the reference policy. It excludes the value network to encourage the model to explore diverse reasoning paths."
  },
  {
    "user_input": "How does Kimi k1.5 address the overthinking phenomenon during RL training?",
    "retrieved_contexts": [
      " Kimi k1.5 TECHNICAL REPORT\n2 Approach: Reinforcement Learning with LLMs\nThe development of Kimi k1.5 consists of several stages: pretraining, vanilla supervised fine-tuning (SFT), long-CoT\nsupervised fine-turning, and reinforcement learning (RL). This report focuses on RL, beginning with an overview of\nthe RL prompt set curation (Section 2.1) and long-CoT supervised finetuning (Section 2.2), followed by an in-depth\ndiscussion of RL training strategies in Section 2.3. Additional details on pretraining and vanilla supervised finetuning\ncan be found in Section 2.5.\n2.1 RL Prompt Set Curation\nThrough our preliminary experiments, we found that the quality and diversity of the RL prompt set play a critical role in\nensuring the effectiveness of reinforcement learning. A well-constructed prompt set not only guides the model toward\nrobust reasoning but also mitigates the risk of reward hacking and overfitting to superficial patterns. Specifically, three\nkey properties define a high-quality RL prompt set:\n‚Ä¢Diverse Coverage : Prompts should span a wide array of disciplines, such as STEM, coding, and general reasoning,\nto enhance the model‚Äôs adaptability and ensure broad applicability across different domains.\n‚Ä¢Balanced Difficulty : The prompt set should include a well-distributed range of easy, moderate, and difficult\nquestions to facilitate gradual learning and prevent overfitting to specific complexity levels. ",
      " Kimi k1.5 TECHNICAL REPORT\nKimi k1.5 short-CoTOpenAI 4oClaude 3.5 Sonnet DeepSeek V3LLaMA-3.1 405B-Inst.\nGeneralCodeMath\n47.333.4\nLiveCodeBench v4 24.08-24.11 (Pass@1-COT)\n36.328.440.531.1Qwen2.5 72B-Inst.\n87.487.2\nMMLU (EM)\n88.388.688.585.3\n94.674.6\nMATH-500 (EM)\n78.373.890.280\n60.8\n9.3AIME 2024 (Pass@1)\n1623.339.223.3\n6869.1\nMMMU_val(Pass@1)\n66.464.5\n70.163.8\nMathVista_test(Pass@1)\n65.369.7\n87.284.3\nIF-Eval (Prompt Strict)\n86.58686.184.1\n91.787.9\nCLUEWSC (EM)\n85.484.790.991.4\n88.376\nC-Eval (EM)\n76.761.586.586.1Qwen2-VLVision\nFigure 2: Kimi k1.5 short-CoT results.\n1 Introduction\nLanguage model pretraining with next token prediction has been studied under the context of the scaling law, where\nproportionally scaling model parameters and data sizes leads to the continued improvement of intelligence. (Kaplan\net al. 2020; Hoffmann et al. 2022) However, this approach is limited to the amount of available high-quality training\ndata (Villalobos et al. 2024; Muennighoff et al. 2023). In this report, we present the training recipe of Kimi k1.5,\nour latest multi-modal LLM trained with reinforcement learning (RL). The goal is to explore a possible new axis\nfor continued scaling. Using RL with LLMs, the models learns to explore with rewards and thus is not limited to a\npre-existing static dataset.\nThere are a few key ingredients about the design and training of k1.5.\n‚Ä¢Long context scaling . We scale the context window of RL to 128k and observe continued improvement of ",
      " Kimi k1.5 TECHNICAL REPORT\n2.6.1 Large Scale Reinforcement Learning Training System for LLM\nIn the realm of artificial intelligence, reinforcement learning (RL) has emerged as a pivotal training methodology\nfor large language models (LLMs)(Ouyang et al. 2022)(Jaech et al. 2024), drawing inspiration from its success in\nmastering complex games like Go, StarCraft II, and Dota 2 through systems such as AlphaGo(Silver et al. 2017),\nAlphaStar(Vinyals et al. 2019), and OpenAI Dota Five (Berner et al. 2019). Following in this tradition, the Kimi\nk1.5 system adopts an iterative synchronous RL framework, meticulously designed to bolster the model‚Äôs reasoning\ncapabilities through persistent learning and adaptation. A key innovation in this system is the introduction of a Partial\nRollout technique, designed to optimize the handling of complex reasoning trajectories.\nThe RL training system as illustrated in Figure 3a operates through an iterative synchronous approach, with each\niteration encompassing a rollout phase and a training phase. During the rollout phase, rollout workers, coordinated\nby a central master, generate rollout trajectories by interacting with the model, producing sequences of responses to\nvarious inputs. These trajectories are then stored in a replay buffer, which ensures a diverse and unbiased dataset for\ntraining by disrupting temporal correlations. In the subsequent training phase, trainer workers access these experiences"
    ],
    "response": "I don't know.",
    "reference": "Kimi k1.5 introduces a length reward to restrain the growth of token length. Given sampled responses, it calculates a length reward based on the response length and correctness. It also gradually warms up the length penalty during training to alleviate potential slowdowns."
  },
  {
    "user_input": "What are the sampling strategies used in Kimi k1.5's RL training?",
    "retrieved_contexts": [
      " Kimi k1.5 TECHNICAL REPORT\n2 Approach: Reinforcement Learning with LLMs\nThe development of Kimi k1.5 consists of several stages: pretraining, vanilla supervised fine-tuning (SFT), long-CoT\nsupervised fine-turning, and reinforcement learning (RL). This report focuses on RL, beginning with an overview of\nthe RL prompt set curation (Section 2.1) and long-CoT supervised finetuning (Section 2.2), followed by an in-depth\ndiscussion of RL training strategies in Section 2.3. Additional details on pretraining and vanilla supervised finetuning\ncan be found in Section 2.5.\n2.1 RL Prompt Set Curation\nThrough our preliminary experiments, we found that the quality and diversity of the RL prompt set play a critical role in\nensuring the effectiveness of reinforcement learning. A well-constructed prompt set not only guides the model toward\nrobust reasoning but also mitigates the risk of reward hacking and overfitting to superficial patterns. Specifically, three\nkey properties define a high-quality RL prompt set:\n‚Ä¢Diverse Coverage : Prompts should span a wide array of disciplines, such as STEM, coding, and general reasoning,\nto enhance the model‚Äôs adaptability and ensure broad applicability across different domains.\n‚Ä¢Balanced Difficulty : The prompt set should include a well-distributed range of easy, moderate, and difficult\nquestions to facilitate gradual learning and prevent overfitting to specific complexity levels. ",
      " Kimi k1.5 TECHNICAL REPORT\nKimi k1.5 short-CoTOpenAI 4oClaude 3.5 Sonnet DeepSeek V3LLaMA-3.1 405B-Inst.\nGeneralCodeMath\n47.333.4\nLiveCodeBench v4 24.08-24.11 (Pass@1-COT)\n36.328.440.531.1Qwen2.5 72B-Inst.\n87.487.2\nMMLU (EM)\n88.388.688.585.3\n94.674.6\nMATH-500 (EM)\n78.373.890.280\n60.8\n9.3AIME 2024 (Pass@1)\n1623.339.223.3\n6869.1\nMMMU_val(Pass@1)\n66.464.5\n70.163.8\nMathVista_test(Pass@1)\n65.369.7\n87.284.3\nIF-Eval (Prompt Strict)\n86.58686.184.1\n91.787.9\nCLUEWSC (EM)\n85.484.790.991.4\n88.376\nC-Eval (EM)\n76.761.586.586.1Qwen2-VLVision\nFigure 2: Kimi k1.5 short-CoT results.\n1 Introduction\nLanguage model pretraining with next token prediction has been studied under the context of the scaling law, where\nproportionally scaling model parameters and data sizes leads to the continued improvement of intelligence. (Kaplan\net al. 2020; Hoffmann et al. 2022) However, this approach is limited to the amount of available high-quality training\ndata (Villalobos et al. 2024; Muennighoff et al. 2023). In this report, we present the training recipe of Kimi k1.5,\nour latest multi-modal LLM trained with reinforcement learning (RL). The goal is to explore a possible new axis\nfor continued scaling. Using RL with LLMs, the models learns to explore with rewards and thus is not limited to a\npre-existing static dataset.\nThere are a few key ingredients about the design and training of k1.5.\n‚Ä¢Long context scaling . We scale the context window of RL to 128k and observe continued improvement of ",
      " Kimi k1.5 TECHNICAL REPORT\n2.6.1 Large Scale Reinforcement Learning Training System for LLM\nIn the realm of artificial intelligence, reinforcement learning (RL) has emerged as a pivotal training methodology\nfor large language models (LLMs)(Ouyang et al. 2022)(Jaech et al. 2024), drawing inspiration from its success in\nmastering complex games like Go, StarCraft II, and Dota 2 through systems such as AlphaGo(Silver et al. 2017),\nAlphaStar(Vinyals et al. 2019), and OpenAI Dota Five (Berner et al. 2019). Following in this tradition, the Kimi\nk1.5 system adopts an iterative synchronous RL framework, meticulously designed to bolster the model‚Äôs reasoning\ncapabilities through persistent learning and adaptation. A key innovation in this system is the introduction of a Partial\nRollout technique, designed to optimize the handling of complex reasoning trajectories.\nThe RL training system as illustrated in Figure 3a operates through an iterative synchronous approach, with each\niteration encompassing a rollout phase and a training phase. During the rollout phase, rollout workers, coordinated\nby a central master, generate rollout trajectories by interacting with the model, producing sequences of responses to\nvarious inputs. These trajectories are then stored in a replay buffer, which ensures a diverse and unbiased dataset for\ntraining by disrupting temporal correlations. In the subsequent training phase, trainer workers access these experiences"
    ],
    "response": "The RL training strategies in Kimi k1.5's system involve using an iterative synchronous framework with rollout phases and training phases. The iteration sequence includes generating rollout trajectories through interactions with the model and storing them in a replay buffer to ensure diversity and unbiased data for training.\n\nPartial Rollout techniques are also used to handle complex reasoning trajectories.",
    "reference": "Curriculum Sampling is used, where training starts with easier tasks and progresses to more challenging ones. Prioritized Sampling is also employed, which focuses on problems where the model underperforms by sampling proportional to 1 - success rate."
  },
  {
    "user_input": "What are the long2short methods used in Kimi k1.5?",
    "retrieved_contexts": [
      " Kimi k1.5 TECHNICAL REPORT\n3.2 Main Results\nK1.5 long-CoT model The performance of the Kimi k1.5 long-CoT model is presented in Table 2. Through long-CoT\nsupervised fine-tuning (described in Section 2.2) and vision-text joint reinforcement learning (discussed in Section 2.3),\nthe model‚Äôs long-term reasoning capabilities are enhanced significantly. The test-time computation scaling further\nstrengthens its performance, enabling the model to achieve state-of-the-art results across a range of modalities. Our\nevaluation reveals marked improvements in the model‚Äôs capacity to reason, comprehend, and synthesize information\nover extended contexts, representing a advancement in multi-modal AI capabilities.\nK1.5 short-CoT model The performance of the Kimi k1.5 short-CoT model is presented in Table 3. This model\nintegrates several techniques, including traditional supervised fine-tuning (discussed in Section 2.5.2), reinforcement\nlearning (explored in Section 2.3), and long-to-short distillation (outlined in Section 2.4). The results demonstrate\nthat the k1.5 short-CoT model delivers competitive or superior performance compared to leading open-source and\nproprietary models across multiple tasks. These include text, vision, and reasoning challenges, with notable strengths in\nnatural language understanding, mathematics, coding, and logical reasoning.\nBenchmark (Metric)Language-only Model Vision-Language Model\nQwQ-32B OpenAI QVQ-72B OpenAI Kimi\nPreview o1-mini Preview o1 k1.5 ",
      " Kimi k1.5 TECHNICAL REPORT\nKimi k1.5 short-CoTOpenAI 4oClaude 3.5 Sonnet DeepSeek V3LLaMA-3.1 405B-Inst.\nGeneralCodeMath\n47.333.4\nLiveCodeBench v4 24.08-24.11 (Pass@1-COT)\n36.328.440.531.1Qwen2.5 72B-Inst.\n87.487.2\nMMLU (EM)\n88.388.688.585.3\n94.674.6\nMATH-500 (EM)\n78.373.890.280\n60.8\n9.3AIME 2024 (Pass@1)\n1623.339.223.3\n6869.1\nMMMU_val(Pass@1)\n66.464.5\n70.163.8\nMathVista_test(Pass@1)\n65.369.7\n87.284.3\nIF-Eval (Prompt Strict)\n86.58686.184.1\n91.787.9\nCLUEWSC (EM)\n85.484.790.991.4\n88.376\nC-Eval (EM)\n76.761.586.586.1Qwen2-VLVision\nFigure 2: Kimi k1.5 short-CoT results.\n1 Introduction\nLanguage model pretraining with next token prediction has been studied under the context of the scaling law, where\nproportionally scaling model parameters and data sizes leads to the continued improvement of intelligence. (Kaplan\net al. 2020; Hoffmann et al. 2022) However, this approach is limited to the amount of available high-quality training\ndata (Villalobos et al. 2024; Muennighoff et al. 2023). In this report, we present the training recipe of Kimi k1.5,\nour latest multi-modal LLM trained with reinforcement learning (RL). The goal is to explore a possible new axis\nfor continued scaling. Using RL with LLMs, the models learns to explore with rewards and thus is not limited to a\npre-existing static dataset.\nThere are a few key ingredients about the design and training of k1.5.\n‚Ä¢Long context scaling . We scale the context window of RL to 128k and observe continued improvement of ",
      " Kimi k1.5 TECHNICAL REPORT\nLong2short RL After a standard RL training phase, we select a model that offers the best balance between perfor-\nmance and token efficiency to serve as the base model, and conduct a separate long2short RL training phase. In this\nsecond phase, we apply the length penalty introduced in Section 2.3.3, and significantly reduce the maximum rollout\nlength to further penalize responses that exceed the desired length while possibly correct.\n2.5 Other Training Details\n2.5.1 Pretraining\nThe Kimi k1.5 base model is trained on a diverse, high-quality multimodal corpus. The language data covers five\ndomains: English, Chinese, Code, Mathematics Reasoning, and Knowledge. Multimodal data, including Captioning,\nImage-text Interleaving, OCR, Knowledge, and QA datasets, enables our model to acquire vision-language capabilities.\nRigorous quality control ensures relevance, diversity, and balance in the overall pretrain dataset. Our pretraining\nproceeds in three stages: (1) Vision-language pretraining, where a strong language foundation is established, followed\nby gradual multimodal integration; (2) Cooldown, which consolidates capabilities using curated and synthetic data,\nparticularly for reasoning and knowledge-based tasks; and (3) Long-context activation, extending sequence processing\nto 131,072 tokens. More details regarding our pretraining efforts can be found in Appendix B.\n2.5.2 Vanilla Supervised Finetuning"
    ],
    "response": "The long2short methods used in Kimi k1.5 model involve a two-phase reinforcement learning (RL) training pipeline: 1) standard RL that selects the best base model, and 2) a separate phase with a length penalty to prioritize token efficiency. This approach allows for continued improvement under scaling limitations by reducing maximum rollout lengths.",
    "reference": "The long2short methods include model merging (averaging weights of long-cot and short-cot models), shortest rejection sampling (selecting the shortest correct response for fine-tuning), DPO (using long CoT model to generate samples for pairwise preference training), and long2short RL (applying length penalty and reducing rollout length after a standard RL training phase)."
  },
  {
    "user_input": "What is the training process of Kimi k1.5 in terms of pretraining and vanilla supervised finetuning?",
    "retrieved_contexts": [
      " Kimi k1.5 TECHNICAL REPORT\nLong2short RL After a standard RL training phase, we select a model that offers the best balance between perfor-\nmance and token efficiency to serve as the base model, and conduct a separate long2short RL training phase. In this\nsecond phase, we apply the length penalty introduced in Section 2.3.3, and significantly reduce the maximum rollout\nlength to further penalize responses that exceed the desired length while possibly correct.\n2.5 Other Training Details\n2.5.1 Pretraining\nThe Kimi k1.5 base model is trained on a diverse, high-quality multimodal corpus. The language data covers five\ndomains: English, Chinese, Code, Mathematics Reasoning, and Knowledge. Multimodal data, including Captioning,\nImage-text Interleaving, OCR, Knowledge, and QA datasets, enables our model to acquire vision-language capabilities.\nRigorous quality control ensures relevance, diversity, and balance in the overall pretrain dataset. Our pretraining\nproceeds in three stages: (1) Vision-language pretraining, where a strong language foundation is established, followed\nby gradual multimodal integration; (2) Cooldown, which consolidates capabilities using curated and synthetic data,\nparticularly for reasoning and knowledge-based tasks; and (3) Long-context activation, extending sequence processing\nto 131,072 tokens. More details regarding our pretraining efforts can be found in Appendix B.\n2.5.2 Vanilla Supervised Finetuning ",
      " Kimi k1.5 TECHNICAL REPORT\nKimi k1.5 short-CoTOpenAI 4oClaude 3.5 Sonnet DeepSeek V3LLaMA-3.1 405B-Inst.\nGeneralCodeMath\n47.333.4\nLiveCodeBench v4 24.08-24.11 (Pass@1-COT)\n36.328.440.531.1Qwen2.5 72B-Inst.\n87.487.2\nMMLU (EM)\n88.388.688.585.3\n94.674.6\nMATH-500 (EM)\n78.373.890.280\n60.8\n9.3AIME 2024 (Pass@1)\n1623.339.223.3\n6869.1\nMMMU_val(Pass@1)\n66.464.5\n70.163.8\nMathVista_test(Pass@1)\n65.369.7\n87.284.3\nIF-Eval (Prompt Strict)\n86.58686.184.1\n91.787.9\nCLUEWSC (EM)\n85.484.790.991.4\n88.376\nC-Eval (EM)\n76.761.586.586.1Qwen2-VLVision\nFigure 2: Kimi k1.5 short-CoT results.\n1 Introduction\nLanguage model pretraining with next token prediction has been studied under the context of the scaling law, where\nproportionally scaling model parameters and data sizes leads to the continued improvement of intelligence. (Kaplan\net al. 2020; Hoffmann et al. 2022) However, this approach is limited to the amount of available high-quality training\ndata (Villalobos et al. 2024; Muennighoff et al. 2023). In this report, we present the training recipe of Kimi k1.5,\nour latest multi-modal LLM trained with reinforcement learning (RL). The goal is to explore a possible new axis\nfor continued scaling. Using RL with LLMs, the models learns to explore with rewards and thus is not limited to a\npre-existing static dataset.\nThere are a few key ingredients about the design and training of k1.5.\n‚Ä¢Long context scaling . We scale the context window of RL to 128k and observe continued improvement of ",
      " Extensive scaling experiments indicate that most of the base model performance comes from improvements in the\nquality and diversity of the pretraining data. Specific details regarding model architecture scaling experiments lie\nbeyond the scope of this report and will be addressed in future publications.\nB.4 Training Stages\nThe Kimi k1.5 model is trained in three stages: the vision-language pretraining stage, the vision-language cooldown\nstage, and the long-context activation stage. Each stage of the Kimi k1.5 model‚Äôs training focuses on a particular\ncapability enhancement.\nVision-language pretraining stage In this stage, the model is firstly trained solely on language data, establishing\na robust language model foundation. Then the model is gradually introduced to interleaved vision-language data,\nacquiring multimodal capabilities. The visual tower is initially trained in isolation without updating the language model\nparameters, then we unfreeze the language model layers, and ultimately increase the proportion of vision-text data\n23"
    ],
    "response": "The Kimi k1.5 base model's pretraining phase completes in approximately three stages: (1) Vision-language initialization with a strong language foundation established through large-scale monolingual datasets; (2) Integration of multimodal data for enhanced capabilities; and (3) Long-context activation extending sequence length to 131,072 tokens.",
    "reference": "In pretraining, it is trained on a diverse multimodal corpus in three stages: vision-language pretraining, cooldown, and long-context activation. In vanilla supervised finetuning, a corpus covering multiple domains is created. For non-reasoning tasks, a seed dataset is constructed through human annotation and refined. For reasoning tasks, rejection sampling is used. The model is trained at different sequence lengths with learning rate decay."
  },
  {
    "user_input": "What are the main contributions of this research?",
    "retrieved_contexts": [
      " 1.1. Contributions\nPost-Training: Large-Scale Reinforcement Learning on the Base Model\n‚Ä¢We directly apply reinforcement learning (RL) to the base model without relying on super-\nvised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore\nchain-of-thought (CoT) for solving complex problems, resulting in the development of\nDeepSeek-R1-Zero. DeepSeek-R1-Zero demonstrates capabilities such as self-verification,\nreflection, and generating long CoTs, marking a significant milestone for the research\ncommunity. Notably, it is the first open research to validate that reasoning capabilities of\nLLMs can be incentivized purely through RL, without the need for SFT. This breakthrough\npaves the way for future advancements in this area.\n‚Ä¢We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL\nstages aimed at discovering improved reasoning patterns and aligning with human pref-\nerences, as well as two SFT stages that serve as the seed for the model‚Äôs reasoning and\nnon-reasoning capabilities. We believe the pipeline will benefit the industry by creating\nbetter models.\nDistillation: Smaller Models Can Be Powerful Too\n‚Ä¢We demonstrate that the reasoning patterns of larger models can be distilled into smaller\nmodels, resulting in better performance compared to the reasoning patterns discovered\nthrough RL on small models. The open source DeepSeek-R1, as well as its API, will benefit ",
      " Appendix\nA. Contributions and Acknowledgments\nCore Contributors\nDaya Guo\nDejian Yang\nHaowei Zhang\nJunxiao Song\nRuoyu Zhang\nRunxin Xu\nQihao Zhu\nShirong Ma\nPeiyi Wang\nXiao Bi\nXiaokang Zhang\nXingkai Yu\nYu Wu\nZ.F. Wu\nZhibin Gou\nZhihong Shao\nZhuoshu Li\nZiyi Gao\nContributors\nAixin Liu\nBing Xue\nBingxuan Wang\nBochao Wu\nBei Feng\nChengda Lu\nChenggang Zhao\nChengqi Deng\nChong Ruan\nDamai Dai\nDeli Chen\nDongjie Ji\nErhang Li\nFangyun Lin\nFucong Dai\nFuli Luo*\nGuangbo Hao\nGuanting Chen\nGuowei Li\nH. Zhang\nHanwei Xu\nHonghui Ding\nHuazuo Gao\nHui QuHui Li\nJianzhong Guo\nJiashi Li\nJingchang Chen\nJingyang Yuan\nJinhao Tu\nJunjie Qiu\nJunlong Li\nJ.L. Cai\nJiaqi Ni\nJian Liang\nJin Chen\nKai Dong\nKai Hu*\nKaichao You\nKaige Gao\nKang Guan\nKexin Huang\nKuai Yu\nLean Wang\nLecong Zhang\nLiang Zhao\nLitong Wang\nLiyue Zhang\nLei Xu\nLeyi Xia\nMingchuan Zhang\nMinghua Zhang\nMinghui Tang\nMingxu Zhou\nMeng Li\nMiaojun Wang\nMingming Li\nNing Tian\nPanpan Huang\nPeng Zhang\nQiancheng Wang\nQinyu Chen\nQiushi Du\nRuiqi Ge*\nRuisong Zhang\nRuizhe Pan\nRunji Wang\nR.J. Chen\nR.L. Jin\n20 ",
      " significant challenge.\n5. Conclusion, Limitation, and Future Work\nIn this work, we share our journey in enhancing model reasoning abilities through reinforcement\nlearning (RL). DeepSeek-R1-Zero represents a pure RL approach without relying on cold-start\ndata, achieving strong performance across various tasks. DeepSeek-R1 is more powerful,\nleveraging cold-start data alongside iterative RL fine-tuning. Ultimately, DeepSeek-R1 achieves\nperformance comparable to OpenAI-o1-1217 on a range of tasks.\nWe further explore distillation the reasoning capability to small dense models. We use\nDeepSeek-R1 as the teacher model to generate 800K data, and fine-tune several small dense\nmodels. The results are promising: DeepSeek-R1-Distill-Qwen-1.5B outperforms GPT-4o and\nClaude-3.5-Sonnet on math benchmarks with 28.9% on AIME and 83.9% on MATH. Other dense\nmodels also achieve impressive results, significantly outperforming other instruction-tuned\nmodels based on the same underlying checkpoints.\nIn the future, we plan to invest in research across the following directions for DeepSeek-R1.\n‚Ä¢General Capability: Currently, the capabilities of DeepSeek-R1 fall short of DeepSeek-\nV3 in tasks such as function calling, multi-turn, complex role-playing, and json output.\nMoving forward, we plan to explore how leveraging long CoT to enhance tasks in these\nfields.\n‚Ä¢Language Mixing: DeepSeek-R1 is currently optimized for Chinese and English, which"
    ],
    "response": "The main contributions of this research are using reinforcement learning (RL) on the base model without supervised fine-tuning (SFT), achieving self-verification and chain-of-thought capabilities, and validating that reasoning capabilities can be incentivized by pure RL. It also introduces a pipeline for developing small but powerful models through distillation from larger models.",
    "reference": "Directly applied RL to the base model without preliminary SFT, developing DeepSeek-R1-Zero which exhibits self-verification, reflection, etc.Introduced the training pipeline for DeepSeek-R1, involving two RL and two SFT stages.Introduced the training pipeline for DeepSeek-R1, involving two RL and two SFT stages.Demonstrated the distillation of reasoning patterns from larger to smaller models and open - sourced relevant models and API."
  },
  {
    "user_input": "How does DeepSeek-R1-Zero improve reasoning capabilities?",
    "retrieved_contexts": [
      " DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via\nReinforcement Learning\nDeepSeek-AI\nresearch@deepseek.com\nAbstract\nWe introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1.\nDeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without super-\nvised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities.\nThrough RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing\nreasoning behaviors. However, it encounters challenges such as poor readability, and language\nmixing. To address these issues and further enhance reasoning performance, we introduce\nDeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-\nR1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the\nresearch community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models\n(1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.\nAIME 2024\n(Pass@1)Codeforces\n(Percentile)GPQA Diamond\n(Pass@1)MATH-500\n(Pass@1)MMLU\n(Pass@1)SWE-bench Verified\n(Resolved)020406080100Accuracy / Percentile (%)79.896.3\n71.597.3\n90.8\n49.279.296.6\n75.796.4\n91.8\n48.972.690.6\n62.194.3\n87.4\n36.863.693.4\n60.090.0\n85.2\n41.6\n39.258.7 59.190.2\n88.5\n42.0DeepSeek-R1 OpenAI-o1-1217 DeepSeek-R1-32B OpenAI-o1-mini DeepSeek-V3\nFigure 1|Benchmark performance of DeepSeek-R1. ",
      " ‚Ä¢Others : DeepSeek-R1 also excels in a wide range of tasks, including creative writing,\ngeneral question answering, editing, summarization, and more. It achieves an impressive\nlength-controlled win-rate of 87.6% on AlpacaEval 2.0 and a win-rate of 92.3% on Are-\nnaHard, showcasing its strong ability to intelligently handle non-exam-oriented queries.\nAdditionally, DeepSeek-R1 demonstrates outstanding performance on tasks requiring\nlong-context understanding, substantially outperforming DeepSeek-V3 on long-context\nbenchmarks.\n2. Approach\n2.1. Overview\nPrevious work has heavily relied on large amounts of supervised data to enhance model\nperformance. In this study, we demonstrate that reasoning capabilities can be significantly\nimproved through large-scale reinforcement learning (RL), even without using supervised\nfine-tuning (SFT) as a cold start. Furthermore, performance can be further enhanced with\nthe inclusion of a small amount of cold-start data. In the following sections, we present: (1)\nDeepSeek-R1-Zero, which applies RL directly to the base model without any SFT data, and\n(2) DeepSeek-R1, which applies RL starting from a checkpoint fine-tuned with thousands of\nlong Chain-of-Thought (CoT) examples. 3) Distill the reasoning capability from DeepSeek-R1 to\nsmall dense models.\n2.2. DeepSeek-R1-Zero: Reinforcement Learning on the Base Model\nReinforcement learning has demonstrated significant effectiveness in reasoning tasks, as ev- ",
      " Figure 3|The average response length of DeepSeek-R1-Zero on the training set during the RL\nprocess. DeepSeek-R1-Zero naturally learns to solve reasoning tasks with more thinking time.\nment throughout the training process. This improvement is not the result of external adjustments\nbut rather an intrinsic development within the model. DeepSeek-R1-Zero naturally acquires the\nability to solve increasingly complex reasoning tasks by leveraging extended test-time compu-\ntation. This computation ranges from generating hundreds to thousands of reasoning tokens,\nallowing the model to explore and refine its thought processes in greater depth.\nOne of the most remarkable aspects of this self-evolution is the emergence of sophisticated\nbehaviors as the test-time computation increases. Behaviors such as reflection‚Äîwhere the model\nrevisits and reevaluates its previous steps‚Äîand the exploration of alternative approaches to\nproblem-solving arise spontaneously. These behaviors are not explicitly programmed but instead\nemerge as a result of the model‚Äôs interaction with the reinforcement learning environment. This\nspontaneous development significantly enhances DeepSeek-R1-Zero‚Äôs reasoning capabilities,\nenabling it to tackle more challenging tasks with greater efficiency and accuracy.\nAha Moment of DeepSeek-R1-Zero A particularly intriguing phenomenon observed during\nthe training of DeepSeek-R1-Zero is the occurrence of an ‚Äúaha moment‚Äù. This moment, as"
    ],
    "response": "DeepSeek-R1-Zero improves reasoning capabilities through large-scale reinforcement learning (RL), without supervised fine-tuning as a preliminary step. The model naturally emerges with powerful and intriguing reasoning behaviors, including extended test-time computation and exploration of alternative problem-solving approaches. This development enhances DeepSeek-R1-Zero's ability to solve complex tasks efficiently.",
    "reference": "By using Group Relative Policy Optimization (GRPO) as the RL algorithm, a rule - based reward system (accuracy and format rewards), and a simple training template. Through RL training, it shows a self - evolution process with increasing thinking time and emerging sophisticated reasoning behaviors, like reflection and exploring alternative problem - solving approaches."
  },
  {
    "user_input": "What is the difference between DeepSeek-R1-Zero and DeepSeek-R1?",
    "retrieved_contexts": [
      " DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via\nReinforcement Learning\nDeepSeek-AI\nresearch@deepseek.com\nAbstract\nWe introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1.\nDeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without super-\nvised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities.\nThrough RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing\nreasoning behaviors. However, it encounters challenges such as poor readability, and language\nmixing. To address these issues and further enhance reasoning performance, we introduce\nDeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-\nR1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the\nresearch community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models\n(1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.\nAIME 2024\n(Pass@1)Codeforces\n(Percentile)GPQA Diamond\n(Pass@1)MATH-500\n(Pass@1)MMLU\n(Pass@1)SWE-bench Verified\n(Resolved)020406080100Accuracy / Percentile (%)79.896.3\n71.597.3\n90.8\n49.279.296.6\n75.796.4\n91.8\n48.972.690.6\n62.194.3\n87.4\n36.863.693.4\n60.090.0\n85.2\n41.6\n39.258.7 59.190.2\n88.5\n42.0DeepSeek-R1 OpenAI-o1-1217 DeepSeek-R1-32B OpenAI-o1-mini DeepSeek-V3\nFigure 1|Benchmark performance of DeepSeek-R1. ",
      " Benchmark (Metric)Claude-3.5- GPT-4o DeepSeek OpenAI OpenAI DeepSeek\nSonnet-1022 0513 V3 o1-mini o1-1217 R1\nArchitecture - - MoE - - MoE\n# Activated Params - - 37B - - 37B\n# Total Params - - 671B - - 671B\nEnglishMMLU (Pass@1) 88.3 87.2 88.5 85.2 91.8 90.8\nMMLU-Redux (EM) 88.9 88.0 89.1 86.7 - 92.9\nMMLU-Pro (EM) 78.0 72.6 75.9 80.3 - 84.0\nDROP (3-shot F1) 88.3 83.7 91.6 83.9 90.2 92.2\nIF-Eval (Prompt Strict) 86.5 84.3 86.1 84.8 - 83.3\nGPQA Diamond (Pass@1) 65.0 49.9 59.1 60.0 75.7 71.5\nSimpleQA (Correct) 28.4 38.2 24.9 7.0 47.0 30.1\nFRAMES (Acc.) 72.5 80.5 73.3 76.9 - 82.5\nAlpacaEval2.0 (LC-winrate) 52.0 51.1 70.0 57.8 - 87.6\nArenaHard (GPT-4-1106) 85.2 80.4 85.5 92.0 - 92.3\nCodeLiveCodeBench (Pass@1-COT) 38.9 32.9 36.2 53.8 63.4 65.9\nCodeforces (Percentile) 20.3 23.6 58.7 93.4 96.6 96.3\nCodeforces (Rating) 717 759 1134 1820 2061 2029\nSWE Verified (Resolved) 50.8 38.8 42.0 41.6 48.9 49.2\nAider-Polyglot (Acc.) 45.3 16.0 49.6 32.9 61.7 53.3\nMathAIME 2024 (Pass@1) 16.0 9.3 39.2 63.6 79.2 79.8\nMATH-500 (Pass@1) 78.3 74.6 90.2 90.0 96.4 97.3\nCNMO 2024 (Pass@1) 13.1 10.8 43.2 67.6 - 78.8\nChineseCLUEWSC (EM) 85.4 87.9 90.9 89.9 - 92.8\nC-Eval (EM) 76.7 76.0 86.5 68.9 - 91.8\nC-SimpleQA (Correct) 55.4 58.7 68.0 40.3 - 63.7\nTable 4|Comparison between DeepSeek-R1 and other representative models.\nsearch and data analysis tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms\nDeepSeek-V3, demonstrating its capability in handling fact-based queries. A similar trend ",
      " DeepSeek-V3, which could help developers in real world tasks.\n‚Ä¢Knowledge : On benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek-\nR1 achieves outstanding results, significantly outperforming DeepSeek-V3 with scores\nof 90.8% on MMLU, 84.0% on MMLU-Pro, and 71.5% on GPQA Diamond. While its\nperformance is slightly below that of OpenAI-o1-1217 on these benchmarks, DeepSeek-R1\nsurpasses other closed-source models, demonstrating its competitive edge in educational\ntasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3,\ndemonstrating its capability in handling fact-based queries. A similar trend is observed\nwhere OpenAI-o1 surpasses 4o on this benchmark.\n4"
    ],
    "response": "The key differences between DeepSeek-R1-Zero and DeepSeek-R1 are:\n\n1. **Training process**: DeepSeek-R1-Zero is trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT), whereas DeepSeek-R1 incorporates multi-stage training with cold-start data before RL.\n\n2. **Performance enhancement**: DeepSeek-R1 achieves outcomes comparable to OpenAI-o1-1217 on reasoning tasks but exhibits better performance in certain aspects such as factual query resolution demonstrated through its sequence on SimpleQA, outperforming DeepSeek-V3.",
    "reference": "DeepSeek-R1-Zero applies RL directly to the base model without SFT data, while DeepSeek-R1 uses a small amount of cold - start data to fine - tune the model first. DeepSeek-R1-Zero has issues like poor readability and language mixing, which DeepSeek-R1 addresses through its multi - stage training pipeline."
  },
  {
    "user_input": "What is the cold start in the training of DeepSeek-R1?",
    "retrieved_contexts": [
      " Inspired by the promising results of DeepSeek-R1-Zero, two natural questions arise: 1) Can\nreasoning performance be further improved or convergence accelerated by incorporating a small\namount of high-quality data as a cold start? 2) How can we train a user-friendly model that\nnot only produces clear and coherent Chains of Thought (CoT) but also demonstrates strong\ngeneral capabilities? To address these questions, we design a pipeline to train DeepSeek-R1. The\npipeline consists of four stages, outlined as follows.\n2.3.1. Cold Start\nUnlike DeepSeek-R1-Zero, to prevent the early unstable cold start phase of RL training from\nthe base model, for DeepSeek-R1 we construct and collect a small amount of long CoT data\nto fine-tune the model as the initial RL actor. To collect such data, we have explored several\napproaches: using few-shot prompting with a long CoT as an example, directly prompting\nmodels to generate detailed answers with reflection and verification, gathering DeepSeek-R1-\nZero outputs in a readable format, and refining the results through post-processing by human\nannotators.\nIn this work, we collect thousands of cold-start data to fine-tune the DeepSeek-V3-Base as\nthe starting point for RL. Compared to DeepSeek-R1-Zero, the advantages of cold start data\n9 ",
      " include:\n‚Ä¢Readability: A key limitation of DeepSeek-R1-Zero is that its content is often not suitable\nfor reading. Responses may mix multiple languages or lack markdown formatting to\nhighlight answers for users. In contrast, when creating cold-start data for DeepSeek-R1,\nwe design a readable pattern that includes a summary at the end of each response and\nfilters out responses that are not reader-friendly. Here, we define the output format as\n|special_token|<reasoning_process>|special_token|<summary>, where the reasoning\nprocess is the CoT for the query, and the summary is used to summarize the reasoning\nresults.\n‚Ä¢Potential: By carefully designing the pattern for cold-start data with human priors, we\nobserve better performance against DeepSeek-R1-Zero. We believe the iterative training is\na better way for reasoning models.\n2.3.2. Reasoning-oriented Reinforcement Learning\nAfter fine-tuning DeepSeek-V3-Base on the cold start data, we apply the same large-scale\nreinforcement learning training process as employed in DeepSeek-R1-Zero. This phase focuses\non enhancing the model‚Äôs reasoning capabilities, particularly in reasoning-intensive tasks such\nas coding, mathematics, science, and logic reasoning, which involve well-defined problems with\nclear solutions. During the training process, we observe that CoT often exhibits language mixing,\nparticularly when RL prompts involve multiple languages. To mitigate the issue of language ",
      " DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via\nReinforcement Learning\nDeepSeek-AI\nresearch@deepseek.com\nAbstract\nWe introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1.\nDeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without super-\nvised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities.\nThrough RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing\nreasoning behaviors. However, it encounters challenges such as poor readability, and language\nmixing. To address these issues and further enhance reasoning performance, we introduce\nDeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-\nR1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the\nresearch community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models\n(1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.\nAIME 2024\n(Pass@1)Codeforces\n(Percentile)GPQA Diamond\n(Pass@1)MATH-500\n(Pass@1)MMLU\n(Pass@1)SWE-bench Verified\n(Resolved)020406080100Accuracy / Percentile (%)79.896.3\n71.597.3\n90.8\n49.279.296.6\n75.796.4\n91.8\n48.972.690.6\n62.194.3\n87.4\n36.863.693.4\n60.090.0\n85.2\n41.6\n39.258.7 59.190.2\n88.5\n42.0DeepSeek-R1 OpenAI-o1-1217 DeepSeek-R1-32B OpenAI-o1-mini DeepSeek-V3\nFigure 1|Benchmark performance of DeepSeek-R1."
    ],
    "response": "The cold start in the training of DeepSeek-R1 refers to the initial phase where a small amount of high-quality data is used to fine-tune the model, which helps prevent early unstable training and improves readability. This step collects thousands of long-chain thinking (CoT) data through various approaches such as few-shot prompting and human post-processing.",
    "reference": "It constructs and collects a small amount of long Chain-of-Thought (CoT) data to fine - tune the DeepSeek-V3-Base as the initial RL actor. The cold - start data is designed to be more readable and shows better potential compared to DeepSeek-R1-Zero. Data collection methods include few - shot prompting, direct prompting for detailed answers, gathering DeepSeek-R1-Zero outputs, and post - processing by human annotators."
  },
  {
    "user_input": "How is the reward modeled in DeepSeek-R1-Zero?",
    "retrieved_contexts": [
      " tags.\nWe do not apply the outcome or process neural reward model in developing DeepSeek-R1-Zero,\nbecause we find that the neural reward model may suffer from reward hacking in the large-scale\nreinforcement learning process, and retraining the reward model needs additional training\nresources and it complicates the whole training pipeline.\n2.2.3. Training Template\nTo train DeepSeek-R1-Zero, we begin by designing a straightforward template that guides\nthe base model to adhere to our specified instructions. As depicted in Table 1, this template\nrequires DeepSeek-R1-Zero to first produce a reasoning process, followed by the final answer.\nWe intentionally limit our constraints to this structural format, avoiding any content-specific\nbiases‚Äîsuch as mandating reflective reasoning or promoting particular problem-solving strate-\ngies‚Äîto ensure that we can accurately observe the model‚Äôs natural progression during the\nreinforcement learning (RL) process.\n2.2.4. Performance, Self-evolution Process and Aha Moment of DeepSeek-R1-Zero\nPerformance of DeepSeek-R1-Zero Figure 2 depicts the performance trajectory of DeepSeek-\nR1-Zero on the AIME 2024 benchmark throughout the reinforcement learning (RL) training\nprocess. As illustrated, DeepSeek-R1-Zero demonstrates a steady and consistent enhancement\nin performance as the RL training advances. Notably, the average pass@1 score on AIME 2024\nshows a significant increase, jumping from an initial 15.6% to an impressive 71.0%, reaching ",
      " A conversation between User and Assistant. The user asks a question, and the Assistant solves it.\nThe assistant first thinks about the reasoning process in the mind and then provides the user\nwith the answer. The reasoning process and answer are enclosed within <think> </think> and\n<answer> </answer> tags, respectively, i.e., <think> reasoning process here </think>\n<answer> answer here </answer>. User: prompt. Assistant:\nTable 1|Template for DeepSeek-R1-Zero. prompt will be replaced with the specific reasoning\nquestion during training.\n2.2.2. Reward Modeling\nThe reward is the source of the training signal, which decides the optimization direction of RL.\nTo train DeepSeek-R1-Zero, we adopt a rule-based reward system that mainly consists of two\ntypes of rewards:\n‚Ä¢Accuracy rewards : The accuracy reward model evaluates whether the response is correct.\nFor example, in the case of math problems with deterministic results, the model is required\nto provide the final answer in a specified format (e.g., within a box), enabling reliable\nrule-based verification of correctness. Similarly, for LeetCode problems, a compiler can be\nused to generate feedback based on predefined test cases.\n‚Ä¢Format rewards : In addition to the accuracy reward model, we employ a format reward\nmodel that enforces the model to put its thinking process between ‚Äò<think>‚Äô and ‚Äò</think>‚Äô\ntags.\nWe do not apply the outcome or process neural reward model in developing DeepSeek-R1-Zero, ",
      " DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via\nReinforcement Learning\nDeepSeek-AI\nresearch@deepseek.com\nAbstract\nWe introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1.\nDeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without super-\nvised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities.\nThrough RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing\nreasoning behaviors. However, it encounters challenges such as poor readability, and language\nmixing. To address these issues and further enhance reasoning performance, we introduce\nDeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-\nR1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the\nresearch community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models\n(1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.\nAIME 2024\n(Pass@1)Codeforces\n(Percentile)GPQA Diamond\n(Pass@1)MATH-500\n(Pass@1)MMLU\n(Pass@1)SWE-bench Verified\n(Resolved)020406080100Accuracy / Percentile (%)79.896.3\n71.597.3\n90.8\n49.279.296.6\n75.796.4\n91.8\n48.972.690.6\n62.194.3\n87.4\n36.863.693.4\n60.090.0\n85.2\n41.6\n39.258.7 59.190.2\n88.5\n42.0DeepSeek-R1 OpenAI-o1-1217 DeepSeek-R1-32B OpenAI-o1-mini DeepSeek-V3\nFigure 1|Benchmark performance of DeepSeek-R1."
    ],
    "response": "The reward in DeepSeek-R1-Zero is modeled through a rule-based system, consisting of two types: \n1. **Accuracy rewards**: evaluating correctness on deterministic results; and\n2. **Format rewards**: enforcing proper formatting for the thinking process between '<think>' and '</think>' tags.\n\nWe do not use neural reward models due to potential issues with \"reward hacking\" and increased training resource demands.",
    "reference": "It uses a rule - based reward system. The accuracy reward model evaluates whether the response is correct, for example, verifying math problem answers in a specified format or using a compiler for LeetCode problems. The format reward model enforces the model to put its thinking process between ‚Äò<think>‚Äô and ‚Äò</think>‚Äô tags. Neural reward models are not used due to potential reward hacking and high resource requirements."
  },
  {
    "user_input": "What benchmarks are used to evaluate the models?",
    "retrieved_contexts": [
      " These optimizations improve RL efficiency in code execution, providing a consistent and reliable environment for\nevaluating RL-generated code, essential for iterative training and model improvement.\n3 Experiments\n3.1 Evaluation\nSince k1.5 is a multimodal model, we conducted comprehensive evaluation across various benchmarks for different\nmodalities. The detailed evaluation setup can be found in Appendix C. Our benchmarks primarily consist of the\nfollowing three categories:\n‚Ä¢Text Benchmark : MMLU (Hendrycks et al. 2020), IF-Eval (J. Zhou et al. 2023), CLUEWSC (L. Xu et al. 2020),\nC-EV AL (Y . Huang et al. 2023)\n‚Ä¢Reasoning Benchmark : HumanEval-Mul, LiveCodeBench (Jain et al. 2024), Codeforces, AIME 2024, MATH-\n500 (Lightman et al. 2023)\n‚Ä¢Vision Benchmark : MMMU (Yue, Ni, et al. 2024), MATH-Vision (K. Wang et al. 2024), MathVista (Lu et al.\n2023)\n2https://github.com/DMOJ/judge-server\n11 ",
      " HumanEval-Mul dataset covers eight mainstream programming languages (Python, Java, C++,\nC#, JavaScript, TypeScript, PHP , and Bash). Model performance on LiveCodeBench is evaluated\nusing CoT format, with data collected between August 2024 and January 2025. The Codeforces\ndataset is evaluated using problems from 10 Div.2 contests along with expert-crafted test cases,\nafter which the expected ratings and percentages of competitors are calculated. SWE-Bench\nverified results are obtained via the agentless framework (Xia et al., 2024). AIDER-related\nbenchmarks are measured using a \"diff\" format. DeepSeek-R1 outputs are capped at a maximum\nof 32,768 tokens for each benchmark.\nBaselines We conduct comprehensive evaluations against several strong baselines, including\nDeepSeek-V3, Claude-Sonnet-3.5-1022, GPT-4o-0513, OpenAI-o1-mini, and OpenAI-o1-1217.\nSince accessing the OpenAI-o1-1217 API is challenging in mainland China, we report its perfor-\nmance based on official reports. For distilled models, we also compare the open-source model\nQwQ-32B-Preview (Qwen, 2024a).\nGeneration Setup For all our models, the maximum generation length is set to 32,768 tokens.\nFor benchmarks requiring sampling, we use a temperature of 0.6, a top-p value of 0.95, and\ngenerate 64 responses per query to estimate pass@1.\n3.1. DeepSeek-R1 Evaluation\nFor education-oriented knowledge benchmarks such as MMLU, MMLU-Pro, and GPQA Di- ",
      " IF-Eval (J. Zhou et al. 2023) is a benchmark for evaluating large language models‚Äô ability to follow verifiable\ninstructions. There are 500+ prompts with instructions such as \"write an article with more than 800 words\", etc. Due to\na version shift, the number of IFEval reported in Table 3 derived from an intermediate model. We will update the scores\nbased on the final model.\nCLUEWSC (L. Xu et al. 2020) is a coreference resolution task in CLUE benchmark, requiring models to determine if\na pronoun and a noun phrase in a sentence co-refer, with data from Chinese fiction books.\nC-EV AL (Y . Huang et al. 2023) is a comprehensive Chinese evaluation suite for assessing advanced knowledge and\nreasoning abilities of foundation models. It includes 13,948 multiple-choice questions across 52 disciplines and four\ndifficulty levels.\nC.2 Reasoning Benchmark\nHumanEval-Mul is a subset of Multipl-E (Cassano, Gouwar, D. Nguyen, S. D. Nguyen, et al. 2022). MultiPL-E\nextends the HumanEval benchmark and MBPP benchmark to 18 languages that encompass a range of programming\n24"
    ],
    "response": "The models are evaluated using benchmarks across three categories: \n1. Text Benchmark (including MMLU, IF-Eval, CLUEWSC, and C-EV AL)\n2. Reasoning Benchmark (encompassing HumanEval-Mul, LiveCodeBench, Codeforces, AIME 2024, and MATH-500)\n3. Vision Benchmark (comprising MMMU, MATH-Vision, and MathVista)",
    "reference": "MMLU, MMLU - Redux, MMLU - Pro, C - Eval, CMMLU, IFEval, FRAMES, GPQA Diamond, SimpleQA, C - SimpleQA, SWE - Bench Verified, Aider 1, LiveCodeBench, Codeforces, Chinese National High School Mathematics Olympiad (CNMO 2024), American Invitational Mathematics Examination 2024 (AIME 2024), and open - ended generation tasks like AlpacaEval 2.0 and Arena - Hard."
  },
  {
    "user_input": "What are the baselines for model evaluation?",
    "retrieved_contexts": [
      " IF-Eval (J. Zhou et al. 2023) is a benchmark for evaluating large language models‚Äô ability to follow verifiable\ninstructions. There are 500+ prompts with instructions such as \"write an article with more than 800 words\", etc. Due to\na version shift, the number of IFEval reported in Table 3 derived from an intermediate model. We will update the scores\nbased on the final model.\nCLUEWSC (L. Xu et al. 2020) is a coreference resolution task in CLUE benchmark, requiring models to determine if\na pronoun and a noun phrase in a sentence co-refer, with data from Chinese fiction books.\nC-EV AL (Y . Huang et al. 2023) is a comprehensive Chinese evaluation suite for assessing advanced knowledge and\nreasoning abilities of foundation models. It includes 13,948 multiple-choice questions across 52 disciplines and four\ndifficulty levels.\nC.2 Reasoning Benchmark\nHumanEval-Mul is a subset of Multipl-E (Cassano, Gouwar, D. Nguyen, S. D. Nguyen, et al. 2022). MultiPL-E\nextends the HumanEval benchmark and MBPP benchmark to 18 languages that encompass a range of programming\n24 ",
      " These optimizations improve RL efficiency in code execution, providing a consistent and reliable environment for\nevaluating RL-generated code, essential for iterative training and model improvement.\n3 Experiments\n3.1 Evaluation\nSince k1.5 is a multimodal model, we conducted comprehensive evaluation across various benchmarks for different\nmodalities. The detailed evaluation setup can be found in Appendix C. Our benchmarks primarily consist of the\nfollowing three categories:\n‚Ä¢Text Benchmark : MMLU (Hendrycks et al. 2020), IF-Eval (J. Zhou et al. 2023), CLUEWSC (L. Xu et al. 2020),\nC-EV AL (Y . Huang et al. 2023)\n‚Ä¢Reasoning Benchmark : HumanEval-Mul, LiveCodeBench (Jain et al. 2024), Codeforces, AIME 2024, MATH-\n500 (Lightman et al. 2023)\n‚Ä¢Vision Benchmark : MMMU (Yue, Ni, et al. 2024), MATH-Vision (K. Wang et al. 2024), MathVista (Lu et al.\n2023)\n2https://github.com/DMOJ/judge-server\n11 ",
      " implementing rejection sampling techniques to maintain quality standards (Yue, Qu, et al. 2023; D. Su et al. 2024).\nThese synthesized QA pairs undergo comprehensive validation before being integrated into the cooldown dataset.\nLong-context activation stage Finally, in the third stage, k1.5 is trained with upsampled long-context cooldown\ndata, enabling it to process extended sequences and support tasks that demand longer context. To ensure excellent\nlong-text capabilities of the base model, we upsampled long-context data and used 40% full attention data and 60%\npartial attention data during long context training. The full attention data came partly from high-quality natural data and\npartly from synthetic long context Q&A and summary data. The partial attention data came from uniform sampling of\ncooldown data. The RoPE frequency (J. Su et al. 2024) was set to 1,000,000. During this stage, we gradually extended\nlength activation training by increasing the maximum sequence length from 4,096 to 32,768, and ultimately to 131,072.\nC Evaluation Details\nC.1 Text Benchmark\nMMLU (Hendrycks et al. 2020) covers 57 subjects in STEM, the humanities, social sciences, and more. It ranges\nin difficulty from an elementary level to an advanced professional level, and it tests both world knowledge and\nproblem-solving ability.\nIF-Eval (J. Zhou et al. 2023) is a benchmark for evaluating large language models‚Äô ability to follow verifiable"
    ],
    "response": "The baselines for model evaluation from the text are MMLU, IF-Eval, CLUEWSC, C-EVAL, HumanEval-Mul, and Codeforces.",
    "reference": "DeepSeek-V3, Claude - Sonnet - 3.5 - 1022, GPT - 4o - 0513, OpenAI - o1 - mini, OpenAI - o1 - 1217. For distilled models, QwQ - 32B - Preview is also used as a baseline."
  },
  {
    "user_input": "What is the performance of DeepSeek-R1 on math tasks?",
    "retrieved_contexts": [
      " DeepSeek-V3, which could help developers in real world tasks.\n‚Ä¢Knowledge : On benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek-\nR1 achieves outstanding results, significantly outperforming DeepSeek-V3 with scores\nof 90.8% on MMLU, 84.0% on MMLU-Pro, and 71.5% on GPQA Diamond. While its\nperformance is slightly below that of OpenAI-o1-1217 on these benchmarks, DeepSeek-R1\nsurpasses other closed-source models, demonstrating its competitive edge in educational\ntasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3,\ndemonstrating its capability in handling fact-based queries. A similar trend is observed\nwhere OpenAI-o1 surpasses 4o on this benchmark.\n4 ",
      " DeepSeek-V3, demonstrating its capability in handling fact-based queries. A similar trend\nis observed where OpenAI-o1 surpasses GPT-4o on this benchmark. However, DeepSeek-R1\nperforms worse than DeepSeek-V3 on the Chinese SimpleQA benchmark, primarily due to its\ntendency to refuse answering certain queries after safety RL. Without safety RL, DeepSeek-R1\ncould achieve an accuracy of over 70%.\nDeepSeek-R1 also delivers impressive results on IF-Eval, a benchmark designed to assess a\nmodel‚Äôs ability to follow format instructions. These improvements can be linked to the inclusion\nof instruction-following data during the final stages of supervised fine-tuning (SFT) and RL\ntraining. Furthermore, remarkable performance is observed on AlpacaEval2.0 and ArenaHard,\nindicating DeepSeek-R1‚Äôs strengths in writing tasks and open-domain question answering. Its\nsignificant outperformance of DeepSeek-V3 underscores the generalization benefits of large-scale\nRL, which not only boosts reasoning capabilities but also improves performance across diverse\ndomains. Moreover, the summary lengths generated by DeepSeek-R1 are concise, with an\naverage of 689 tokens on ArenaHard and 2,218 characters on AlpacaEval 2.0. This indicates that\nDeepSeek-R1 avoids introducing length bias during GPT-based evaluations, further solidifying\nits robustness across multiple tasks.\nOn math tasks, DeepSeek-R1 demonstrates performance on par with OpenAI-o1-1217, ",
      " its robustness across multiple tasks.\nOn math tasks, DeepSeek-R1 demonstrates performance on par with OpenAI-o1-1217,\nsurpassing other models by a large margin. A similar trend is observed on coding algorithm\ntasks, such as LiveCodeBench and Codeforces, where reasoning-focused models dominate these\nbenchmarks. On engineering-oriented coding tasks, OpenAI-o1-1217 outperforms DeepSeek-R1\non Aider but achieves comparable performance on SWE Verified. We believe the engineering\n13"
    ],
    "response": "DeepSeek-R1 demonstrates performance on par with Openai-o1-1217 and significantly surpasses other models on math tasks, notably achieving accuracy of 90.8% in MMLU math benchmark. Furthermore, it scores strongly across multiple domains and performs well alongside the robust open source model OpenAI-o1-1217, indicating its capability in mathematical problem resolution.",
    "reference": "On AIME 2024, it has a Pass@1 score of 79.8%, slightly surpassing OpenAI - o1 - 1217. On MATH - 500, it attains a score of 97.3%, performing on par with OpenAI - o1 - 1217 and significantly outperforming other models."
  },
  {
    "user_input": "Why is distillation of reasoning capabilities important?",
    "retrieved_contexts": [
      " ing prompts. For helpfulness, we focus exclusively on the final summary, ensuring that the\nassessment emphasizes the utility and relevance of the response to the user while minimizing\ninterference with the underlying reasoning process. For harmlessness, we evaluate the entire\nresponse of the model, including both the reasoning process and the summary, to identify and\nmitigate any potential risks, biases, or harmful content that may arise during the generation\nprocess. Ultimately, the integration of reward signals and diverse data distributions enables us\nto train a model that excels in reasoning while prioritizing helpfulness and harmlessness.\n2.4. Distillation: Empower Small Models with Reasoning Capability\nTo equip more efficient smaller models with reasoning capabilities like DeekSeek-R1, we directly\nfine-tuned open-source models like Qwen (Qwen, 2024b) and Llama (AI@Meta, 2024) using\nthe 800k samples curated with DeepSeek-R1, as detailed in ¬ß2.3.3. Our findings indicate that\nthis straightforward distillation method significantly enhances the reasoning abilities of smaller\nmodels. The base models we use here are Qwen2.5-Math-1.5B, Qwen2.5-Math-7B, Qwen2.5-\n14B, Qwen2.5-32B, Llama-3.1-8B, and Llama-3.3-70B-Instruct. We select Llama-3.3 because its\nreasoning capability is slightly better than that of Llama-3.1.\nFor distilled models, we apply only SFT and do not include an RL stage, even though ",
      " 3.2 Distilled Model Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\n4 Discussion 14\n4.1 Distillation v.s. Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . . . . 14\n4.2 Unsuccessful Attempts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n5 Conclusion, Limitation, and Future Work 16\nA Contributions and Acknowledgments 20\n2 ",
      " Zero. Upon nearing convergence in the RL process, we create new SFT data through rejection\nsampling on the RL checkpoint, combined with supervised data from DeepSeek-V3 in domains\nsuch as writing, factual QA, and self-cognition, and then retrain the DeepSeek-V3-Base model.\nAfter fine-tuning with the new data, the checkpoint undergoes an additional RL process, taking\ninto account prompts from all scenarios. After these steps, we obtained a checkpoint referred to\nas DeepSeek-R1, which achieves performance on par with OpenAI-o1-1217.\nWe further explore distillation from DeepSeek-R1 to smaller dense models. Using Qwen2.5-\n32B (Qwen, 2024b) as the base model, direct distillation from DeepSeek-R1 outperforms applying\nRL on it. This demonstrates that the reasoning patterns discovered by larger base models are cru-\ncial for improving reasoning capabilities. We open-source the distilled Qwen and Llama (Dubey\net al., 2024) series. Notably, our distilled 14B model outperforms state-of-the-art open-source\nQwQ-32B-Preview (Qwen, 2024a) by a large margin, and the distilled 32B and 70B models set a\nnew record on the reasoning benchmarks among dense models.\n3"
    ],
    "response": "Distillation of reasoning capabilities is important because it empowers smaller models with complex thinking skills. Distillation helps to integrate knowledge from larger models into smaller ones, improving their problem-solving abilities. This makes smaller models more effective and efficient in handling tasks that require strong reasoning.",
    "reference": "It can empower smaller models with reasoning capabilities. Distilling from more powerful models like DeepSeek-R1 into smaller models can achieve better performance than RL training on small models alone. For example, DeepSeek-R1 - Distill - Qwen - 7B outperforms non - reasoning models like GPT - 4o - 0513 across the board."
  }
]
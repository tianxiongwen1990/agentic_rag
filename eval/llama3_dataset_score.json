[
    {
        "user_input":"What is the core idea of Janus?",
        "retrieved_contexts":[
            " Janus: Decoupling Visual Encoding for Unified\nMultimodal Understanding and Generation\nChengyue Wu1,2Xiaokang Chen1,‚àó,‚Ä†Zhiyu Wu1,3Yiyang Ma1,3Xingchao Liu1Zizheng Pan1\nWen Liu1Zhenda Xie1Xingkai Yu1Chong Ruan1Ping Luo2,‚àó\n1DeepSeek-AI2The University of Hong Kong3Peking University\n‚Ä†: Project lead‚àó: Corresponding authors\nProject Page: https:\/\/github.com\/deepseek-ai\/Janus\nAbstract\nIn this paper, we introduce Janus , an autoregressive framework that unifies multimodal under-\nstanding and generation. Prior research often relies on a single visual encoder for both tasks,\nsuch as Chameleon. However, due to the differing levels of information granularity required by\nmultimodal understanding and generation, this approach can lead to suboptimal performance,\nparticularly in multimodal understanding. To address this issue, we decouple visual encoding\ninto separate pathways, while still leveraging a single, unified transformer architecture for\nprocessing. The decoupling not only alleviates the conflict between the visual encoder‚Äôs roles in\nunderstanding and generation, but also enhances the framework‚Äôs flexibility. For instance, both\nthe multimodal understanding and generation components can independently select their most\nsuitable encoding methods. Experiments show that Janus surpasses previous unified model\nand matches or exceeds the performance of task-specific models. The simplicity, high flexibility, ",
            " Auto-RegressiveTransformer\nUnd.Encoder\nTextTokenizer\nGen.Encoder\nTextDe-Tokenizer\nTextTokenizer\nImageDecoder\nImage:X!LanguageInstruct:X\"LanguageInstruct:X\"Image:X!LanguageResponse:X#GeneratedImage:X!UnderstandingImageGeneration\n‚Ä¶‚Ä¶\n‚Ä¶‚Ä¶Figure 2|Architecture of our Janus. Different from previous approaches [ 77,85] that typically\nassume visual understanding and generation require the same visual encoder, our Janus de-\ncouples visual encoding for visual understanding and visual generation. ‚ÄúUnd. Encoder‚Äù and\n‚ÄúGen. Encoder‚Äù are abbreviations for ‚ÄúUnderstanding Encoder‚Äù and ‚ÄúGeneration Encoder‚Äù,\nrespectively. Best viewed in color.\nand generation tasks, regardless of whether they are based on autoregressive (AR) models\n[77,85] or diffusion models [ 86,94]. For example, Chameleon [ 77] adopts a VQ Tokenizer to\nencode images for both multimodal understanding and generation. However, this practice\nmay lead to suboptimal outcomes, as the vision encoder might face a trade-off between the\ndemands of understanding and generation. In contrast, our Janus can explicitly decouple the\nvisual representations for understanding and generation, recognizing that different tasks may\nrequire varying levels of information.\n3. Janus: A Simple, Unified and Flexible Multimodal Framework\n3.1. Architecture\nThe architecture of Janus is shown in Figure 2. For pure text understanding, multimodal\nunderstanding, and visual generation, we apply independent encoding methods to convert the ",
            " demands that understanding and generation place on the visual encoder. Extensive experi-\nments have demonstrated the effectiveness and leading performance of Janus. It is also worth\nnoting that Janus is flexible and easy to extend. In addition to having significant potential for\nimprovement in both multimodal understanding and generation, Janus is also easily extendable\nto incorporate more input modalities. The above advantages suggest that Janus may serve as an\ninspiration for the development of the next generation of multimodal general-purpose models.\n13"
        ],
        "response":"The core idea of Janus is to decouple visual encoding into separate pathways for multimodal understanding and generation, allowing each task to select its most suitable encoding method independently. This approach alleviates conflicts between the visual encoder's roles in different tasks and enhances the framework's flexibility. It enables a unified transformer architecture to be used for both tasks, surpassing previous unified models and matching the performance of task-specific models.",
        "reference":"The core idea of Janus is to decouple visual encoding for multimodal understanding and generation. This is achieved by introducing two independent visual encoding pathways, one for multimodal understanding and one for multimodal generation, unified by the same transformer architecture.",
        "faithfulness":1.0,
        "answer_relevancy":0.8472327297,
        "context_precision":1.0,
        "answer_correctness":0.5203814201
    },
    {
        "user_input":"How does Janus's architecture differ from previous unified multimodal models?",
        "retrieved_contexts":[
            " demands that understanding and generation place on the visual encoder. Extensive experi-\nments have demonstrated the effectiveness and leading performance of Janus. It is also worth\nnoting that Janus is flexible and easy to extend. In addition to having significant potential for\nimprovement in both multimodal understanding and generation, Janus is also easily extendable\nto incorporate more input modalities. The above advantages suggest that Janus may serve as an\ninspiration for the development of the next generation of multimodal general-purpose models.\n13 ",
            " Janus: Decoupling Visual Encoding for Unified\nMultimodal Understanding and Generation\nChengyue Wu1,2Xiaokang Chen1,‚àó,‚Ä†Zhiyu Wu1,3Yiyang Ma1,3Xingchao Liu1Zizheng Pan1\nWen Liu1Zhenda Xie1Xingkai Yu1Chong Ruan1Ping Luo2,‚àó\n1DeepSeek-AI2The University of Hong Kong3Peking University\n‚Ä†: Project lead‚àó: Corresponding authors\nProject Page: https:\/\/github.com\/deepseek-ai\/Janus\nAbstract\nIn this paper, we introduce Janus , an autoregressive framework that unifies multimodal under-\nstanding and generation. Prior research often relies on a single visual encoder for both tasks,\nsuch as Chameleon. However, due to the differing levels of information granularity required by\nmultimodal understanding and generation, this approach can lead to suboptimal performance,\nparticularly in multimodal understanding. To address this issue, we decouple visual encoding\ninto separate pathways, while still leveraging a single, unified transformer architecture for\nprocessing. The decoupling not only alleviates the conflict between the visual encoder‚Äôs roles in\nunderstanding and generation, but also enhances the framework‚Äôs flexibility. For instance, both\nthe multimodal understanding and generation components can independently select their most\nsuitable encoding methods. Experiments show that Janus surpasses previous unified model\nand matches or exceeds the performance of task-specific models. The simplicity, high flexibility, ",
            " Auto-RegressiveTransformer\nUnd.Encoder\nTextTokenizer\nGen.Encoder\nTextDe-Tokenizer\nTextTokenizer\nImageDecoder\nImage:X!LanguageInstruct:X\"LanguageInstruct:X\"Image:X!LanguageResponse:X#GeneratedImage:X!UnderstandingImageGeneration\n‚Ä¶‚Ä¶\n‚Ä¶‚Ä¶Figure 2|Architecture of our Janus. Different from previous approaches [ 77,85] that typically\nassume visual understanding and generation require the same visual encoder, our Janus de-\ncouples visual encoding for visual understanding and visual generation. ‚ÄúUnd. Encoder‚Äù and\n‚ÄúGen. Encoder‚Äù are abbreviations for ‚ÄúUnderstanding Encoder‚Äù and ‚ÄúGeneration Encoder‚Äù,\nrespectively. Best viewed in color.\nand generation tasks, regardless of whether they are based on autoregressive (AR) models\n[77,85] or diffusion models [ 86,94]. For example, Chameleon [ 77] adopts a VQ Tokenizer to\nencode images for both multimodal understanding and generation. However, this practice\nmay lead to suboptimal outcomes, as the vision encoder might face a trade-off between the\ndemands of understanding and generation. In contrast, our Janus can explicitly decouple the\nvisual representations for understanding and generation, recognizing that different tasks may\nrequire varying levels of information.\n3. Janus: A Simple, Unified and Flexible Multimodal Framework\n3.1. Architecture\nThe architecture of Janus is shown in Figure 2. For pure text understanding, multimodal\nunderstanding, and visual generation, we apply independent encoding methods to convert the"
        ],
        "response":"Janus differs from previous unified multimodal models by decoupling visual encoding for separate pathways of understanding and generation, allowing for more flexibility and improved performance. This is in contrast to approaches that assume a single visual encoder can handle both tasks equally well. By decoupling visual encoding, Janus avoids the conflicts between competing demands during these tasks.",
        "reference":"Previous models often use a single visual encoder for both understanding and generation tasks. In contrast, Janus decouples visual encoding. It uses the SigLIP encoder for multimodal understanding to extract high - dimensional semantic features and a VQ tokenizer for visual generation to convert images into discrete IDs, with separate adaptors for each task.",
        "faithfulness":1.0,
        "answer_relevancy":0.9232100577,
        "context_precision":0.5833333333,
        "answer_correctness":0.5481653831
    },
    {
        "user_input":"What are the three training stages of Janus?",
        "retrieved_contexts":[
            " Gen. Encoder\nImageHead\nGen. AdaptorStageI:Training AdaptorsandImageHead\nLLM\nUnd.Adaptor\nüî•\nüî•\nüî•\nUnderstanding\nTextHead\nUnd. Encoder\n‚ùÑ\n‚ùÑ\n‚ùÑImageGenerationStageII: UnifiedPretrainingStageIII:SupervisedFine-tuning\nGen. Encoder\nImageHead\nGen. Adaptor\nLLM\nUnd.Adaptor\nüî•\nüî•\nüî•\nUnderstanding\nTextHead\nUnd. Encoder\n‚ùÑ\n‚ùÑ\n‚ùÑImageGeneration\nüî•\nüî•\nGen. Encoder\nImageHead\nGen. Adaptor\nLLM\nUnd.Adaptor\nüî•\nüî•\nüî•\nUnderstanding\nTextHead\nUnd. EncoderImageGeneration\nüî•\nüî•\nüî•\n‚ùÑFigure 3|Our Janus adopts a three-stage training procedure. We use flame symbols\/snowflake\nsymbols in the diagram to indicate the module updates\/does not update its parameters.\nattention masks.\n3.2. Training Procedure\nThe training of Janus is divided into three stages, as illustrated in Figure 3. Details are provided\nin the below.\nStage I: Training Adaptors and Image Head. The main goal of this stage is to create a conceptual\nconnection between visual and linguistic elements within the embedding space, enabling the\nLLM to understand the entities shown in images and have preliminary visual generation\nability. We keep the visual encoders and the LLM frozen during this stage, allowing only the\ntrainable parameters within the understanding adaptor, generation adaptor and image head to\nbe updated.\nStage II: Unified Pretraining. In this stage, we perform unified pretraining with multimodal\ncorpus to enable Janus to learn both multimodal understanding and generation. We unfreeze the ",
            " Table 1|Detailed hyperparameters of our Janus . Data ratio refers to the ratio of multimodal\nunderstanding data, pure text data, and visual generation data.\nHyperparameters Stage 1 Stage 2 Stage 3\nLearning rate 1.0 √ó10‚àí31√ó10‚àí42.0√ó10‚àí5\nLR scheduler Cosine Constant Constant\nWeight decay 0.0 0.0 0.1\nGradient clip 1.0 1.0 1.0\nOptimizer AdamW ( ùõΩ1=0.9,ùõΩ2=0.95)\nWarm-up steps 300 5, 000 0\nTraining steps 10, 000 180, 000 24, 000\nBatch size 256 512 256\nData Ratio 1 : 0 : 1 2 : 3 : 5 7 : 3 : 10\n4.1. Implementation Details\nIn our experiments, we utilize DeepSeek-LLM ( 1.3B) [5] with a maximum supported sequence\nlength of 4096 as the base language model. For the vision encoder used in understanding tasks,\nwe select SigLIP-Large-Patch 16-384[92]. The generation encoder has a codebook of size 16, 384\nand downsamples images by a factor of 16. Both the understanding adaptor and the generation\nadaptor are two-layer MLPs. The detailed hyperparameters for each stage are provided in\nTable 1. All images are resized to 384√ó384pixels. For multimodal understanding data, we\nresize the long side of the image and pad the short side with the background color (RGB: 127,\n127,127) to reach 384. For visual generation data, the short side is resized to 384, and the long\nside is cropped to 384. We use sequence packing during training to improve training efficiency.\nWe mix all data types according to the specified ratios in a single training step. Our Janus is ",
            " Janus-Pro employs a three-stage training strategy: Stage I : Training adaptors and image decoders focuses on aligning image features with text through robust pixel dependency modeling. Using fixed LLM parameters, this stage employs ImageNet to map simple categories (e.g., 'sunflower') into meaningful representations."
        ],
        "response":"The three training stages of Janus are:\n\n1. Stage I: Training Adaptors and Image Head\n2. Stage II: Unified Pretraining\n3. Stage III: Supervised Fine-tuning.",
        "reference":"The first stage is training adaptors and the image head, where only the understanding adaptor, generation adaptor, and image head are updated while keeping the visual encoders and LLM frozen. The second stage is unified pretraining with multimodal corpus, including using ImageNet - 1k for simple visual generation training first and then general text - to - image data. The third stage is supervised fine - tuning with instruction tuning data, fine - tuning all parameters except the generation encoder.",
        "faithfulness":1.0,
        "answer_relevancy":1.0,
        "context_precision":0.9999999999,
        "answer_correctness":0.3188320883
    },
    {
        "user_input":"What is the training objective of Janus?",
        "retrieved_contexts":[
            " corpus to enable Janus to learn both multimodal understanding and generation. We unfreeze the\nLLM and utilize all types of training data: pure text data, multimodal understanding data, and\nvisual generation data. Inspired by Pixart [ 9], we begin by conducting simple visual generation\ntraining using ImageNet- 1k to help the model grasp basic pixel dependencies. Subsequently, we\nenhance the model‚Äôs open-domain visual generation capability with general text-to-image data.\nStage III: Supervised Fine-tuning. During this stage, we fine-tune the pretrained model with\ninstruction tuning data to enhance its instruction-following and dialogue capabilities. We\nfine-tune all parameters except the generation encoder. We focus on supervising the answers\nwhile masking system and user prompts. To ensure Janus‚Äôs proficiency in both multimodal\nunderstanding and generation, we don‚Äôt fine-tune separate models for a certain task. Instead,\nwe use a blend of pure text dialogue data, multimodal understanding data and visual generation\ndata, ensuring versatility across various scenarios.\n3.3. Training Objective\nJanus is an autoregressive model, and we simply adopt the cross-entropy loss during training:\nL=‚àí‚àëÔ∏Å\nùëñ=1logùëÉùúÉ(ùë•ùëñ|ùë•<ùëñ) (1)\nHere,ùëÉ(¬∑|¬∑) indicates the conditional probability modeled by the weights ùúÉof Janus. For\npure text understanding and multimodal understanding tasks, we compute the loss on the text\n5 ",
            " Janus: Decoupling Visual Encoding for Unified\nMultimodal Understanding and Generation\nChengyue Wu1,2Xiaokang Chen1,‚àó,‚Ä†Zhiyu Wu1,3Yiyang Ma1,3Xingchao Liu1Zizheng Pan1\nWen Liu1Zhenda Xie1Xingkai Yu1Chong Ruan1Ping Luo2,‚àó\n1DeepSeek-AI2The University of Hong Kong3Peking University\n‚Ä†: Project lead‚àó: Corresponding authors\nProject Page: https:\/\/github.com\/deepseek-ai\/Janus\nAbstract\nIn this paper, we introduce Janus , an autoregressive framework that unifies multimodal under-\nstanding and generation. Prior research often relies on a single visual encoder for both tasks,\nsuch as Chameleon. However, due to the differing levels of information granularity required by\nmultimodal understanding and generation, this approach can lead to suboptimal performance,\nparticularly in multimodal understanding. To address this issue, we decouple visual encoding\ninto separate pathways, while still leveraging a single, unified transformer architecture for\nprocessing. The decoupling not only alleviates the conflict between the visual encoder‚Äôs roles in\nunderstanding and generation, but also enhances the framework‚Äôs flexibility. For instance, both\nthe multimodal understanding and generation components can independently select their most\nsuitable encoding methods. Experiments show that Janus surpasses previous unified model\nand matches or exceeds the performance of task-specific models. The simplicity, high flexibility, ",
            " demands that understanding and generation place on the visual encoder. Extensive experi-\nments have demonstrated the effectiveness and leading performance of Janus. It is also worth\nnoting that Janus is flexible and easy to extend. In addition to having significant potential for\nimprovement in both multimodal understanding and generation, Janus is also easily extendable\nto incorporate more input modalities. The above advantages suggest that Janus may serve as an\ninspiration for the development of the next generation of multimodal general-purpose models.\n13"
        ],
        "response":"The training objective of Janus is to use cross-entropy loss during training, where the loss function L is defined as:\n\nL=‚àí‚àëÔ∏Å\nùëñ=1logùëÉùúÉ(ùë•ùëñ|ùë•<ùëñ)\n\nThis indicates that Janus learns through a conditional probability modeled by its weights.",
        "reference":"Janus is an autoregressive model and uses the cross - entropy loss during training. The loss is calculated as $\\mathcal{L}=-\\sum_{i = 1} \\log P_{\\theta}(x_{i}",
        "faithfulness":1.0,
        "answer_relevancy":0.9678790735,
        "context_precision":0.9999999999,
        "answer_correctness":0.2217646862
    },
    {
        "user_input":"How does Janus perform inference for image generation?",
        "retrieved_contexts":[
            " demands that understanding and generation place on the visual encoder. Extensive experi-\nments have demonstrated the effectiveness and leading performance of Janus. It is also worth\nnoting that Janus is flexible and easy to extend. In addition to having significant potential for\nimprovement in both multimodal understanding and generation, Janus is also easily extendable\nto incorporate more input modalities. The above advantages suggest that Janus may serve as an\ninspiration for the development of the next generation of multimodal general-purpose models.\n13 ",
            " The model supports 1024x1024 resolution image generation with an average inference time of 2.4 seconds. The 1B version has significantly lower requirements while maintaining strong performance. 4. ... Janus Pro excels in both text-to-image generation and multimodal understanding tasks. It supports high-quality image generation, complex scene ... ",
            " corpus to enable Janus to learn both multimodal understanding and generation. We unfreeze the\nLLM and utilize all types of training data: pure text data, multimodal understanding data, and\nvisual generation data. Inspired by Pixart [ 9], we begin by conducting simple visual generation\ntraining using ImageNet- 1k to help the model grasp basic pixel dependencies. Subsequently, we\nenhance the model‚Äôs open-domain visual generation capability with general text-to-image data.\nStage III: Supervised Fine-tuning. During this stage, we fine-tune the pretrained model with\ninstruction tuning data to enhance its instruction-following and dialogue capabilities. We\nfine-tune all parameters except the generation encoder. We focus on supervising the answers\nwhile masking system and user prompts. To ensure Janus‚Äôs proficiency in both multimodal\nunderstanding and generation, we don‚Äôt fine-tune separate models for a certain task. Instead,\nwe use a blend of pure text dialogue data, multimodal understanding data and visual generation\ndata, ensuring versatility across various scenarios.\n3.3. Training Objective\nJanus is an autoregressive model, and we simply adopt the cross-entropy loss during training:\nL=‚àí‚àëÔ∏Å\nùëñ=1logùëÉùúÉ(ùë•ùëñ|ùë•<ùëñ) (1)\nHere,ùëÉ(¬∑|¬∑) indicates the conditional probability modeled by the weights ùúÉof Janus. For\npure text understanding and multimodal understanding tasks, we compute the loss on the text\n5"
        ],
        "response":"I don't know. According to the retrieved context, the exact steps or details of how Janus performs inference for image generation are not explicitly stated. However, it does mention that it uses an average inference time of 2.4 seconds for generating images at a resolution of 1024x1024.",
        "reference":"For image generation, Janus utilizes classifier - free guidance (CFG). Specifically, for each token, the logit¬†¬†is calculated as¬†, where¬†¬†is the conditional logit,¬†¬†is the unconditional logit, and¬†¬†is the scale for the classifier - free guidance (default¬†¬†for evaluation).",
        "faithfulness":1.0,
        "answer_relevancy":0.0,
        "context_precision":0.0,
        "answer_correctness":0.1840039468
    },
    {
        "user_input":"What datasets are used in Janus's training?",
        "retrieved_contexts":[
            " demands that understanding and generation place on the visual encoder. Extensive experi-\nments have demonstrated the effectiveness and leading performance of Janus. It is also worth\nnoting that Janus is flexible and easy to extend. In addition to having significant potential for\nimprovement in both multimodal understanding and generation, Janus is also easily extendable\nto incorporate more input modalities. The above advantages suggest that Janus may serve as an\ninspiration for the development of the next generation of multimodal general-purpose models.\n13 ",
            " Janus is a model trained using Mistral-7B-v0.2 as its base model. Janus has been trained on Multifaceted Collection, a preference dataset containing 196k unique system messages for aligning LLMs to diverse human preferences. Janus not only excels at generating personalized responses that cater to various human preferences but is also adept at ... ",
            " JANuS provides metadata and image links for four new training datasets; all of these datasets are designed for evaluation on a subset of 100 classes chosen from ImageNet-1000. Each dataset in JANuS is either a subset or a superset of an existing dataset, and each is fully captioned and fully labeled, either using annotated or synthetic labels."
        ],
        "response":"I don't know the specific datasets used in Janus's training beyond Multifaceted Collection. However, JANuS provides metadata and image links for four new training datasets, which are subsets or superset of existing datasets like ImageNet-1000. These datasets are fully captioned and labeled.",
        "reference":"In stage I, it uses 1.25 million image - text paired captions from ShareGPT4V for multimodal understanding and approximately 1.2 million samples from ImageNet - 1k for visual generation. In stage II, it uses text - only data from DeepSeek - LLM, interleaved image - text data from WikiHow and WIT, image caption data from multiple sources, table and chart data from DeepSeek - VL, and visual generation data from various datasets. In stage III, it uses data from different sources for text understanding, multimodal understanding, and visual generation, with specific formatting for instruction tuning.",
        "faithfulness":0.8,
        "answer_relevancy":0.0,
        "context_precision":0.0,
        "answer_correctness":0.2382892317
    },
    {
        "user_input":"How does Janus compare with previous unified models in terms of performance?",
        "retrieved_contexts":[
            " Janus surpasses previous unified model and matches or exceeds the performance of task-specific models. The simplicity, high flexibility, and effectiveness of Janus make it a strong candidate for next-generation unified multimodal models. ... Extensive experiments show that JanusFlow achieves comparable or superior performance to specialized ... ",
            " corpus to enable Janus to learn both multimodal understanding and generation. We unfreeze the\nLLM and utilize all types of training data: pure text data, multimodal understanding data, and\nvisual generation data. Inspired by Pixart [ 9], we begin by conducting simple visual generation\ntraining using ImageNet- 1k to help the model grasp basic pixel dependencies. Subsequently, we\nenhance the model‚Äôs open-domain visual generation capability with general text-to-image data.\nStage III: Supervised Fine-tuning. During this stage, we fine-tune the pretrained model with\ninstruction tuning data to enhance its instruction-following and dialogue capabilities. We\nfine-tune all parameters except the generation encoder. We focus on supervising the answers\nwhile masking system and user prompts. To ensure Janus‚Äôs proficiency in both multimodal\nunderstanding and generation, we don‚Äôt fine-tune separate models for a certain task. Instead,\nwe use a blend of pure text dialogue data, multimodal understanding data and visual generation\ndata, ensuring versatility across various scenarios.\n3.3. Training Objective\nJanus is an autoregressive model, and we simply adopt the cross-entropy loss during training:\nL=‚àí‚àëÔ∏Å\nùëñ=1logùëÉùúÉ(ùë•ùëñ|ùë•<ùëñ) (1)\nHere,ùëÉ(¬∑|¬∑) indicates the conditional probability modeled by the weights ùúÉof Janus. For\npure text understanding and multimodal understanding tasks, we compute the loss on the text\n5 ",
            " Janus surpasses previous unified model and matches or exceeds the performance of task-specific models. The simplicity, high flexibility, and effectiveness of Janus make it a strong candidate for next-generation unified multimodal models. ... Extensive experiments show that JanusFlow achieves comparable or superior performance to specialized ..."
        ],
        "response":"I don't know. According to the provided context, Janus surpasses previous unified models in terms of performance and matches or exceeds task-specific models' performance. However, it doesn't provide a direct comparison to earlier unified models' performance metrics.",
        "reference":"Janus outperforms previous unified models on both multimodal understanding and generation benchmarks. On multimodal understanding benchmarks like MMBench, SEED - Bench, and POPE, Janus (1.3B) achieves higher scores than models such as LLaVA - v1.5 (7B) and Qwen - VL - Chat (7B). In visual generation, it has better results than some well - known models like DALL - E 2 and SDXL on benchmarks like MSCOCO - 30K and GenEval.",
        "faithfulness":0.75,
        "answer_relevancy":0.0,
        "context_precision":0.0,
        "answer_correctness":0.3166001302
    },
    {
        "user_input":"What are the possible extensions of Janus in multimodal understanding?",
        "retrieved_contexts":[
            " In this work, we introduce Janus-Pro, an advanced version of the previous work Janus. Specifically, Janus-Pro incorporates (1) an optimized training strategy, (2) expanded training data, and (3) scaling to larger model size. With these improvements, Janus-Pro achieves significant advancements in both multimodal understanding and text-to-image instruction-following capabilities, while also ... ",
            " Janus: Decoupling Visual Encoding for Unified\nMultimodal Understanding and Generation\nChengyue Wu1,2Xiaokang Chen1,‚àó,‚Ä†Zhiyu Wu1,3Yiyang Ma1,3Xingchao Liu1Zizheng Pan1\nWen Liu1Zhenda Xie1Xingkai Yu1Chong Ruan1Ping Luo2,‚àó\n1DeepSeek-AI2The University of Hong Kong3Peking University\n‚Ä†: Project lead‚àó: Corresponding authors\nProject Page: https:\/\/github.com\/deepseek-ai\/Janus\nAbstract\nIn this paper, we introduce Janus , an autoregressive framework that unifies multimodal under-\nstanding and generation. Prior research often relies on a single visual encoder for both tasks,\nsuch as Chameleon. However, due to the differing levels of information granularity required by\nmultimodal understanding and generation, this approach can lead to suboptimal performance,\nparticularly in multimodal understanding. To address this issue, we decouple visual encoding\ninto separate pathways, while still leveraging a single, unified transformer architecture for\nprocessing. The decoupling not only alleviates the conflict between the visual encoder‚Äôs roles in\nunderstanding and generation, but also enhances the framework‚Äôs flexibility. For instance, both\nthe multimodal understanding and generation components can independently select their most\nsuitable encoding methods. Experiments show that Janus surpasses previous unified model\nand matches or exceeds the performance of task-specific models. The simplicity, high flexibility, ",
            " Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation. Janus is a novel autoregressive framework that unifies multimodal understanding and generation. It addresses the limitations of previous approaches by decoupling visual encoding into separate pathways, while still utilizing a single, unified transformer architecture for processing."
        ],
        "response":"I don't know the specific extensions of Janus in multimodal understanding beyond the provided context. However, I can tell you that Janus introduced an autoregressive framework that decouples visual encoding into separate pathways to address limitations of previous approaches and unify multimodel understanding and generation. This decoupling enhances flexibility and performance.",
        "reference":"In multimodal understanding, Janus can use stronger vision encoders like EVA - CLIP or InternViT. It can also use dynamic high - resolution techniques to handle high - resolution images and compress tokens using operations like pixel shuffle to save computational cost.",
        "faithfulness":0.8571428571,
        "answer_relevancy":0.0,
        "context_precision":0.0,
        "answer_correctness":0.3214626015
    },
    {
        "user_input":"What are the possible extensions of Janus in visual generation?",
        "retrieved_contexts":[
            " üî• DeepSeek Janus Pro 7B: Now Available Janus Pro by DeepSeek. DeepSeek's revolutionary open-source multimodal AI model, featuring advanced text-to-image generation and visual understanding. Outperforming DALL-E 3 with 84.2% DPG-Bench accuracy, available in both 1B and 7B versions for flexible deployment. ",
            " demands that understanding and generation place on the visual encoder. Extensive experi-\nments have demonstrated the effectiveness and leading performance of Janus. It is also worth\nnoting that Janus is flexible and easy to extend. In addition to having significant potential for\nimprovement in both multimodal understanding and generation, Janus is also easily extendable\nto incorporate more input modalities. The above advantages suggest that Janus may serve as an\ninspiration for the development of the next generation of multimodal general-purpose models.\n13 ",
            " Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation. Janus is a novel autoregressive framework that unifies multimodal understanding and generation. It addresses the limitations of previous approaches by decoupling visual encoding into separate pathways, while still utilizing a single, unified transformer architecture for processing."
        ],
        "response":"I don't know. The provided context doesn't mention specific extensions or possible expansions for Janus in visual generation beyond its existing decoupled visual encoding system. However, it does suggest that Janus is flexible and easily extensible to incorporate more input modalities. Further research would be needed to identify potential extensions for visual generation specifically.",
        "reference":"In visual generation, Janus can choose finer - grained encoders like MoVQGan to preserve more image details. It can employ loss functions specifically designed for visual generation, such as diffusion loss. Also, a combination of AR (causal attention) and parallel (bidirectional attention) methods can be used to reduce accumulated errors during visual generation.",
        "faithfulness":0.7142857143,
        "answer_relevancy":0.0,
        "context_precision":0.0,
        "answer_correctness":0.3671802048
    },
    {
        "user_input":"Why is Janus considered a strong candidate for next - generation unified multimodal models?",
        "retrieved_contexts":[
            " Janus-Pro surpasses previous unified model and matches or exceeds the performance of task-specific models. The simplicity, high flexibility, and effectiveness of Janus-Pro make it a strong candidate for next-generation unified multimodal models. Github Repository ",
            " demands that understanding and generation place on the visual encoder. Extensive experi-\nments have demonstrated the effectiveness and leading performance of Janus. It is also worth\nnoting that Janus is flexible and easy to extend. In addition to having significant potential for\nimprovement in both multimodal understanding and generation, Janus is also easily extendable\nto incorporate more input modalities. The above advantages suggest that Janus may serve as an\ninspiration for the development of the next generation of multimodal general-purpose models.\n13 ",
            " Janus surpasses previous unified model and matches or exceeds the performance of task-specific models. The simplicity, high flexibility, and effectiveness of Janus make it a strong candidate for next-generation unified multimodal models."
        ],
        "response":"Janus is considered a strong candidate for next-generation unified multimodal models due to its simplicity, high flexibility, and effectiveness. It has surpassed previous unified models and matched the performance of task-specific models in various tasks. Its ease of extension to incorporate more input modalities makes it a promising model for future development.",
        "reference":"Janus is considered a strong candidate because it has a simple, unified, and flexible architecture. It decouples visual encoding, which leads to high performance on both multimodal understanding and generation tasks, even outperforming some task - specific models. It is also easy to extend, with the potential to incorporate more input modalities, such as 3D point cloud, tactile, and EEG data.",
        "faithfulness":1.0,
        "answer_relevancy":1.0,
        "context_precision":1.0,
        "answer_correctness":0.5394008286
    },
    {
        "user_input":"What is the main innovation of Kimi k1.5?",
        "retrieved_contexts":[
            " KIMI K 1.5:\nSCALING REINFORCEMENT LEARNING WITH LLM S\nTECHNICAL REPORT OF KIMI K 1.5\nKimi Team\nABSTRACT\nLanguage model pretraining with next token prediction has proved effective for scaling compute but\nis limited to the amount of available training data. Scaling reinforcement learning (RL) unlocks a new\naxis for the continued improvement of artificial intelligence, with the promise that large language\nmodels (LLMs) can scale their training data by learning to explore with rewards. However, prior\npublished work has not produced competitive results. In light of this, we report on the training practice\nof Kimi k1.5, our latest multi-modal LLM trained with RL, including its RL training techniques,\nmulti-modal data recipes, and infrastructure optimization. Long context scaling and improved policy\noptimization methods are key ingredients of our approach, which establishes a simplistic, effective\nRL framework without relying on more complex techniques such as Monte Carlo tree search, value\nfunctions, and process reward models. Notably, our system achieves state-of-the-art reasoning\nperformance across multiple benchmarks and modalities‚Äîe.g., 77.5 on AIME, 96.2 on MATH\n500, 94-th percentile on Codeforces, 74.9 on MathVista‚Äîmatching OpenAI‚Äôs o1. Moreover, we\npresent effective long2short methods that use long-CoT techniques to improve short-CoT models,\nyielding state-of-the-art short-CoT reasoning results‚Äîe.g., 60.8 on AIME, 94.6 on MATH500, 47.3 ",
            " Kimi k1.5 TECHNICAL REPORT\n3.2 Main Results\nK1.5 long-CoT model The performance of the Kimi k1.5 long-CoT model is presented in Table 2. Through long-CoT\nsupervised fine-tuning (described in Section 2.2) and vision-text joint reinforcement learning (discussed in Section 2.3),\nthe model‚Äôs long-term reasoning capabilities are enhanced significantly. The test-time computation scaling further\nstrengthens its performance, enabling the model to achieve state-of-the-art results across a range of modalities. Our\nevaluation reveals marked improvements in the model‚Äôs capacity to reason, comprehend, and synthesize information\nover extended contexts, representing a advancement in multi-modal AI capabilities.\nK1.5 short-CoT model The performance of the Kimi k1.5 short-CoT model is presented in Table 3. This model\nintegrates several techniques, including traditional supervised fine-tuning (discussed in Section 2.5.2), reinforcement\nlearning (explored in Section 2.3), and long-to-short distillation (outlined in Section 2.4). The results demonstrate\nthat the k1.5 short-CoT model delivers competitive or superior performance compared to leading open-source and\nproprietary models across multiple tasks. These include text, vision, and reasoning challenges, with notable strengths in\nnatural language understanding, mathematics, coding, and logical reasoning.\nBenchmark (Metric)Language-only Model Vision-Language Model\nQwQ-32B OpenAI QVQ-72B OpenAI Kimi\nPreview o1-mini Preview o1 k1.5 ",
            " A new player that's turning heads has emerged: the Kimi k1.5 model developed by Moonshot AI. This innovative model is making waves, and for good reason. Let's understand what makes Kimi k1.5 stand out. What is Kimi k1.5? Kimi k1.5, Kimi AI's latest version, is an advanced artificial intelligence model developed by Beijing-based startup Moonshot AI."
        ],
        "response":"The main innovation of Kimi k1.5 is the use of scaling reinforcement learning with large language models (LLMs), which unlocks a new axis for improving artificial intelligence by allowing LLMs to scale their training data through exploration with rewards, establishing an effective RL framework without relying on complex techniques like Monte Carlo tree search and value functions.",
        "reference":"The main innovation is scaling reinforcement learning (RL) with LLMs by introducing long context scaling (scaling the RL context window to 128k) and improved policy optimization methods. It also presents long2short methods to improve short-CoT models, achieving state-of-the-art reasoning performance across multiple benchmarks and modalities without relying on complex techniques like Monte Carlo tree search.",
        "faithfulness":0.8333333333,
        "answer_relevancy":1.0,
        "context_precision":0.9999999999,
        "answer_correctness":0.4862155873
    },
    {
        "user_input":"How does Kimi k1.5 perform in different benchmarks compared to other models?",
        "retrieved_contexts":[
            " The AI arms race is evolving, and Kimi k1.5 is setting a new benchmark. As a multimodal thinking model, Kimi k1.5 achieves what no other company outside OpenAI has managed: performance matching the full-powered O1 modelacross reasoning benchmarks, without the preview or mini suffixes.This development represents a major step forward in artificial intelligence, signaling a new era of competition ... ",
            " Kimi k1.5 TECHNICAL REPORT\nKimi k1.5 short-CoTOpenAI 4oClaude 3.5 Sonnet DeepSeek V3LLaMA-3.1 405B-Inst.\nGeneralCodeMath\n47.333.4\nLiveCodeBench v4 24.08-24.11 (Pass@1-COT)\n36.328.440.531.1Qwen2.5 72B-Inst.\n87.487.2\nMMLU (EM)\n88.388.688.585.3\n94.674.6\nMATH-500 (EM)\n78.373.890.280\n60.8\n9.3AIME 2024 (Pass@1)\n1623.339.223.3\n6869.1\nMMMU_val(Pass@1)\n66.464.5\n70.163.8\nMathVista_test(Pass@1)\n65.369.7\n87.284.3\nIF-Eval (Prompt Strict)\n86.58686.184.1\n91.787.9\nCLUEWSC (EM)\n85.484.790.991.4\n88.376\nC-Eval (EM)\n76.761.586.586.1Qwen2-VLVision\nFigure 2: Kimi k1.5 short-CoT results.\n1 Introduction\nLanguage model pretraining with next token prediction has been studied under the context of the scaling law, where\nproportionally scaling model parameters and data sizes leads to the continued improvement of intelligence. (Kaplan\net al. 2020; Hoffmann et al. 2022) However, this approach is limited to the amount of available high-quality training\ndata (Villalobos et al. 2024; Muennighoff et al. 2023). In this report, we present the training recipe of Kimi k1.5,\nour latest multi-modal LLM trained with reinforcement learning (RL). The goal is to explore a possible new axis\nfor continued scaling. Using RL with LLMs, the models learns to explore with rewards and thus is not limited to a\npre-existing static dataset.\nThere are a few key ingredients about the design and training of k1.5.\n‚Ä¢Long context scaling . We scale the context window of RL to 128k and observe continued improvement of ",
            " Language model pretraining with next token prediction has proved effective for scaling compute but is limited to the amount of available training data. ... by Kimi Team and 93 other authors. View PDF HTML ... Notably, our system achieves state-of-the-art reasoning performance across multiple benchmarks and modalities -- e.g., 77.5 on AIME, 96.2 ..."
        ],
        "response":"I don't know the details of how Kimi k1.5 performs specifically compared to other models in each benchmark. However, according to the retrieved context, Kimi k1.5 matches full-powered O1 model performance across multiple reasoning benchmarks, which suggests it performs well. The exact performance differences with other models aren't specified.",
        "reference":"In long-CoT, it achieves 77.5 on AIME, 96.2 on MATH 500, 94-th percentile on Codeforces, 74.9 on MathVista, matching or outperforming models like OpenAI‚Äôs o1. In short-CoT, it gets 60.8 on AIME, 94.6 on MATH500, 47.3 on LiveCodeBench, outperforming models such as GPT-4o and Claude Sonnet 3.5 by a large margin (up to +550%).",
        "faithfulness":0.25,
        "answer_relevancy":0.0,
        "context_precision":0.5,
        "answer_correctness":0.3715305962
    },
    {
        "user_input":"What are the key ingredients of Kimi k1.5's design and training?",
        "retrieved_contexts":[
            " Kimi k1.5 TECHNICAL REPORT\n2 Approach: Reinforcement Learning with LLMs\nThe development of Kimi k1.5 consists of several stages: pretraining, vanilla supervised fine-tuning (SFT), long-CoT\nsupervised fine-turning, and reinforcement learning (RL). This report focuses on RL, beginning with an overview of\nthe RL prompt set curation (Section 2.1) and long-CoT supervised finetuning (Section 2.2), followed by an in-depth\ndiscussion of RL training strategies in Section 2.3. Additional details on pretraining and vanilla supervised finetuning\ncan be found in Section 2.5.\n2.1 RL Prompt Set Curation\nThrough our preliminary experiments, we found that the quality and diversity of the RL prompt set play a critical role in\nensuring the effectiveness of reinforcement learning. A well-constructed prompt set not only guides the model toward\nrobust reasoning but also mitigates the risk of reward hacking and overfitting to superficial patterns. Specifically, three\nkey properties define a high-quality RL prompt set:\n‚Ä¢Diverse Coverage : Prompts should span a wide array of disciplines, such as STEM, coding, and general reasoning,\nto enhance the model‚Äôs adaptability and ensure broad applicability across different domains.\n‚Ä¢Balanced Difficulty : The prompt set should include a well-distributed range of easy, moderate, and difficult\nquestions to facilitate gradual learning and prevent overfitting to specific complexity levels. ",
            " Kimi k1.5 TECHNICAL REPORT\nKimi k1.5 short-CoTOpenAI 4oClaude 3.5 Sonnet DeepSeek V3LLaMA-3.1 405B-Inst.\nGeneralCodeMath\n47.333.4\nLiveCodeBench v4 24.08-24.11 (Pass@1-COT)\n36.328.440.531.1Qwen2.5 72B-Inst.\n87.487.2\nMMLU (EM)\n88.388.688.585.3\n94.674.6\nMATH-500 (EM)\n78.373.890.280\n60.8\n9.3AIME 2024 (Pass@1)\n1623.339.223.3\n6869.1\nMMMU_val(Pass@1)\n66.464.5\n70.163.8\nMathVista_test(Pass@1)\n65.369.7\n87.284.3\nIF-Eval (Prompt Strict)\n86.58686.184.1\n91.787.9\nCLUEWSC (EM)\n85.484.790.991.4\n88.376\nC-Eval (EM)\n76.761.586.586.1Qwen2-VLVision\nFigure 2: Kimi k1.5 short-CoT results.\n1 Introduction\nLanguage model pretraining with next token prediction has been studied under the context of the scaling law, where\nproportionally scaling model parameters and data sizes leads to the continued improvement of intelligence. (Kaplan\net al. 2020; Hoffmann et al. 2022) However, this approach is limited to the amount of available high-quality training\ndata (Villalobos et al. 2024; Muennighoff et al. 2023). In this report, we present the training recipe of Kimi k1.5,\nour latest multi-modal LLM trained with reinforcement learning (RL). The goal is to explore a possible new axis\nfor continued scaling. Using RL with LLMs, the models learns to explore with rewards and thus is not limited to a\npre-existing static dataset.\nThere are a few key ingredients about the design and training of k1.5.\n‚Ä¢Long context scaling . We scale the context window of RL to 128k and observe continued improvement of ",
            " Kimi k1.5's design and training revolve around several key ingredients: Long Context Scaling: The researchers scaled Kimi k1.5's context window for RL to 128k and found that performance continued to improve as the context length increased."
        ],
        "response":"The key ingredients of Kimi k1.5's design and training are:\n\n‚Ä¢ Long context scaling, where the context window is scaled up to 128k.\n \nA high-quality prompt set, with diverse coverage across disciplines and balanced difficulty levels.\n \nI don't know any additional specific details provided.",
        "reference":"Key ingredients include long context scaling, improved policy optimization (deriving a formulation of RL with long-CoT and using a variant of online mirror descent), a simplistic framework, multimodal training (text and vision data), and effective long2short methods.",
        "faithfulness":0.8571428571,
        "answer_relevancy":0.0,
        "context_precision":0.5,
        "answer_correctness":0.2898557641
    },
    {
        "user_input":"How is the RL prompt set curated for Kimi k1.5?",
        "retrieved_contexts":[
            "Could not find the references, answered by LLM"
        ],
        "response":"I don't know",
        "reference":"The RL prompt set is curated to have diverse coverage (spanning STEM, coding, general reasoning), balanced difficulty (including easy, moderate, and difficult questions), and accurate evaluability. It uses automatic filters, a tagging system, a model-based difficulty assessment method, and a process to remove easy-to-hack prompts.",
        "faithfulness":1.0,
        "answer_relevancy":0.0,
        "context_precision":0.0,
        "answer_correctness":0.0814343994
    },
    {
        "user_input":"What is the process of long-CoT supervised fine-tuning in Kimi k1.5?",
        "retrieved_contexts":[
            " Kimi k1.5 TECHNICAL REPORT\nKimi k1.5 short-CoTOpenAI 4oClaude 3.5 Sonnet DeepSeek V3LLaMA-3.1 405B-Inst.\nGeneralCodeMath\n47.333.4\nLiveCodeBench v4 24.08-24.11 (Pass@1-COT)\n36.328.440.531.1Qwen2.5 72B-Inst.\n87.487.2\nMMLU (EM)\n88.388.688.585.3\n94.674.6\nMATH-500 (EM)\n78.373.890.280\n60.8\n9.3AIME 2024 (Pass@1)\n1623.339.223.3\n6869.1\nMMMU_val(Pass@1)\n66.464.5\n70.163.8\nMathVista_test(Pass@1)\n65.369.7\n87.284.3\nIF-Eval (Prompt Strict)\n86.58686.184.1\n91.787.9\nCLUEWSC (EM)\n85.484.790.991.4\n88.376\nC-Eval (EM)\n76.761.586.586.1Qwen2-VLVision\nFigure 2: Kimi k1.5 short-CoT results.\n1 Introduction\nLanguage model pretraining with next token prediction has been studied under the context of the scaling law, where\nproportionally scaling model parameters and data sizes leads to the continued improvement of intelligence. (Kaplan\net al. 2020; Hoffmann et al. 2022) However, this approach is limited to the amount of available high-quality training\ndata (Villalobos et al. 2024; Muennighoff et al. 2023). In this report, we present the training recipe of Kimi k1.5,\nour latest multi-modal LLM trained with reinforcement learning (RL). The goal is to explore a possible new axis\nfor continued scaling. Using RL with LLMs, the models learns to explore with rewards and thus is not limited to a\npre-existing static dataset.\nThere are a few key ingredients about the design and training of k1.5.\n‚Ä¢Long context scaling . We scale the context window of RL to 128k and observe continued improvement of ",
            " The development of Kimi k1.5 went through several stages: pre-training, supervised fine-tuning (SFT), long-CoT supervised fine-tuning, and reinforcement learning (RL). Kimi has always had an advantage in supporting long contexts, and this time they also introduced long-text RL during the reinforcement learning phase to enable models to provide ... ",
            " The performance of the Kimi k1.5 long-CoT model is presented in Table 2. Through long-CoT supervised fine-tuning (described in Section 2.2) and vision-text joint reinforcement learning (discussed in Section 2.3), the model's long-term reasoning capabilities are enhanced significantly. The test-time computation scaling further strengthens its ..."
        ],
        "response":"Using long-CoT supervised fine-tuning, Kimi k1.5 fine-tunes a pre-trained model on a specific task by conditioning it on long contexts, allowing it to continue improving and enhancing its generation capabilities. This process is done using RL (reinforcement learning) but also incorporating the CoT supervision mechanism which helps the model to make decisions in a certain context while generating text. The training aims to enhance long-term reasoning capabilities of the LLM.",
        "reference":"A small yet high-quality long-CoT warmup dataset is constructed using prompt engineering. It contains verified reasoning paths for text and image inputs. Then, a lightweight SFT is performed on this dataset to prime the model to internalize reasoning strategies and improve its performance in generating detailed and coherent responses.",
        "faithfulness":0.6363636364,
        "answer_relevancy":0.8163752473,
        "context_precision":0.0,
        "answer_correctness":0.4954319254
    },
    {
        "user_input":"What is the policy optimization algorithm used in Kimi k1.5's RL training?",
        "retrieved_contexts":[
            " KIMI K 1.5:\nSCALING REINFORCEMENT LEARNING WITH LLM S\nTECHNICAL REPORT OF KIMI K 1.5\nKimi Team\nABSTRACT\nLanguage model pretraining with next token prediction has proved effective for scaling compute but\nis limited to the amount of available training data. Scaling reinforcement learning (RL) unlocks a new\naxis for the continued improvement of artificial intelligence, with the promise that large language\nmodels (LLMs) can scale their training data by learning to explore with rewards. However, prior\npublished work has not produced competitive results. In light of this, we report on the training practice\nof Kimi k1.5, our latest multi-modal LLM trained with RL, including its RL training techniques,\nmulti-modal data recipes, and infrastructure optimization. Long context scaling and improved policy\noptimization methods are key ingredients of our approach, which establishes a simplistic, effective\nRL framework without relying on more complex techniques such as Monte Carlo tree search, value\nfunctions, and process reward models. Notably, our system achieves state-of-the-art reasoning\nperformance across multiple benchmarks and modalities‚Äîe.g., 77.5 on AIME, 96.2 on MATH\n500, 94-th percentile on Codeforces, 74.9 on MathVista‚Äîmatching OpenAI‚Äôs o1. Moreover, we\npresent effective long2short methods that use long-CoT techniques to improve short-CoT models,\nyielding state-of-the-art short-CoT reasoning results‚Äîe.g., 60.8 on AIME, 94.6 on MATH500, 47.3 ",
            " Kimi k1.5 TECHNICAL REPORT\n2 Approach: Reinforcement Learning with LLMs\nThe development of Kimi k1.5 consists of several stages: pretraining, vanilla supervised fine-tuning (SFT), long-CoT\nsupervised fine-turning, and reinforcement learning (RL). This report focuses on RL, beginning with an overview of\nthe RL prompt set curation (Section 2.1) and long-CoT supervised finetuning (Section 2.2), followed by an in-depth\ndiscussion of RL training strategies in Section 2.3. Additional details on pretraining and vanilla supervised finetuning\ncan be found in Section 2.5.\n2.1 RL Prompt Set Curation\nThrough our preliminary experiments, we found that the quality and diversity of the RL prompt set play a critical role in\nensuring the effectiveness of reinforcement learning. A well-constructed prompt set not only guides the model toward\nrobust reasoning but also mitigates the risk of reward hacking and overfitting to superficial patterns. Specifically, three\nkey properties define a high-quality RL prompt set:\n‚Ä¢Diverse Coverage : Prompts should span a wide array of disciplines, such as STEM, coding, and general reasoning,\nto enhance the model‚Äôs adaptability and ensure broad applicability across different domains.\n‚Ä¢Balanced Difficulty : The prompt set should include a well-distributed range of easy, moderate, and difficult\nquestions to facilitate gradual learning and prevent overfitting to specific complexity levels. ",
            " Kimi k1.5 TECHNICAL REPORT\n2.6.1 Large Scale Reinforcement Learning Training System for LLM\nIn the realm of artificial intelligence, reinforcement learning (RL) has emerged as a pivotal training methodology\nfor large language models (LLMs)(Ouyang et al. 2022)(Jaech et al. 2024), drawing inspiration from its success in\nmastering complex games like Go, StarCraft II, and Dota 2 through systems such as AlphaGo(Silver et al. 2017),\nAlphaStar(Vinyals et al. 2019), and OpenAI Dota Five (Berner et al. 2019). Following in this tradition, the Kimi\nk1.5 system adopts an iterative synchronous RL framework, meticulously designed to bolster the model‚Äôs reasoning\ncapabilities through persistent learning and adaptation. A key innovation in this system is the introduction of a Partial\nRollout technique, designed to optimize the handling of complex reasoning trajectories.\nThe RL training system as illustrated in Figure 3a operates through an iterative synchronous approach, with each\niteration encompassing a rollout phase and a training phase. During the rollout phase, rollout workers, coordinated\nby a central master, generate rollout trajectories by interacting with the model, producing sequences of responses to\nvarious inputs. These trajectories are then stored in a replay buffer, which ensures a diverse and unbiased dataset for\ntraining by disrupting temporal correlations. In the subsequent training phase, trainer workers access these experiences"
        ],
        "response":"I don't know the specific policy optimization algorithm used in Kimi k1.5's RL training. The retrieval context doesn't provide information on the algorithm used. However, it highlights the importance of reinforcement learning and its benefits for large language models. \n\nKimi k1.5 adopts an iterative synchronous approach to reinforcement learning but references a Partial Rollout technique designed to optimize complex reasoning trajectories without explicitly mentioning the underlying policy optimization algorithm.",
        "reference":"A variant of online policy mirror decent is used. At each iteration, it optimizes a relative entropy regularized policy optimization problem. The surrogate loss is calculated and gradients are taken using samples from the reference policy. It excludes the value network to encourage the model to explore diverse reasoning paths.",
        "faithfulness":1.0,
        "answer_relevancy":0.0,
        "context_precision":0.0,
        "answer_correctness":0.3469703105
    },
    {
        "user_input":"How does Kimi k1.5 address the overthinking phenomenon during RL training?",
        "retrieved_contexts":[
            "Could not find the references, answered by LLM"
        ],
        "response":"Sorry, I don't know the answer yet.",
        "reference":"Kimi k1.5 introduces a length reward to restrain the growth of token length. Given sampled responses, it calculates a length reward based on the response length and correctness. It also gradually warms up the length penalty during training to alleviate potential slowdowns.",
        "faithfulness":0.0,
        "answer_relevancy":0.0,
        "context_precision":0.0,
        "answer_correctness":0.1026515372
    },
    {
        "user_input":"What are the sampling strategies used in Kimi k1.5's RL training?",
        "retrieved_contexts":[
            " Kimi k1.5 TECHNICAL REPORT\nKimi k1.5 short-CoTOpenAI 4oClaude 3.5 Sonnet DeepSeek V3LLaMA-3.1 405B-Inst.\nGeneralCodeMath\n47.333.4\nLiveCodeBench v4 24.08-24.11 (Pass@1-COT)\n36.328.440.531.1Qwen2.5 72B-Inst.\n87.487.2\nMMLU (EM)\n88.388.688.585.3\n94.674.6\nMATH-500 (EM)\n78.373.890.280\n60.8\n9.3AIME 2024 (Pass@1)\n1623.339.223.3\n6869.1\nMMMU_val(Pass@1)\n66.464.5\n70.163.8\nMathVista_test(Pass@1)\n65.369.7\n87.284.3\nIF-Eval (Prompt Strict)\n86.58686.184.1\n91.787.9\nCLUEWSC (EM)\n85.484.790.991.4\n88.376\nC-Eval (EM)\n76.761.586.586.1Qwen2-VLVision\nFigure 2: Kimi k1.5 short-CoT results.\n1 Introduction\nLanguage model pretraining with next token prediction has been studied under the context of the scaling law, where\nproportionally scaling model parameters and data sizes leads to the continued improvement of intelligence. (Kaplan\net al. 2020; Hoffmann et al. 2022) However, this approach is limited to the amount of available high-quality training\ndata (Villalobos et al. 2024; Muennighoff et al. 2023). In this report, we present the training recipe of Kimi k1.5,\nour latest multi-modal LLM trained with reinforcement learning (RL). The goal is to explore a possible new axis\nfor continued scaling. Using RL with LLMs, the models learns to explore with rewards and thus is not limited to a\npre-existing static dataset.\nThere are a few key ingredients about the design and training of k1.5.\n‚Ä¢Long context scaling . We scale the context window of RL to 128k and observe continued improvement of ",
            " Contribute to MoonshotAI\/Kimi-k1.5 development by creating an account on GitHub. ... 94-th percentile on Codeforces, 74.9 on MathVista---matching OpenAI's o1. Moreover, we present effective long2short methods that use long-CoT techniques to improve short-CoT models, yielding state-of-the-art short-CoT reasoning results---e.g., 60.8 on AIME, 94. ... ",
            " In addition to curriculum sampling, we use a prioritized sampling strategy to focus on problems where the model underperforms. ... The Kimi k1.5 base model is trained on a diverse, high-quality multimodal corpus. The language data covers five domains: English, Chinese, Code, Mathematics Reasoning, and Knowledge. ..."
        ],
        "response":"I don't know the specific sampling strategies used in Kimi k1.5's RL training beyond prioritized sampling and curriculum sampling with additional methods such as long-coT techniques. Kimi k1.5 uses a combination of these strategies for its reinforcement learning model fine-tuning.",
        "reference":"Curriculum Sampling is used, where training starts with easier tasks and progresses to more challenging ones. Prioritized Sampling is also employed, which focuses on problems where the model underperforms by sampling proportional to 1 - success rate.",
        "faithfulness":0.75,
        "answer_relevancy":0.0,
        "context_precision":0.3333333333,
        "answer_correctness":0.4957156032
    },
    {
        "user_input":"What are the long2short methods used in Kimi k1.5?",
        "retrieved_contexts":[
            " Long2Short Methods: Kimi 1.5 introduces techniques to transfer the reasoning capabilities of long-chain-of-thought (CoT) models to short-Co T models, improving their performance with limited token ... ",
            " Moreover, we present effective long2short methods that use long-CoT techniques to improve short-CoT models, yielding state-of-the-art short-CoT reasoning results---e.g., 60.8 on AIME, 94.6 on MATH500, 47.3 on LiveCodeBench---outperforming existing short-CoT models such as GPT-4o and Claude Sonnet 3.5 by a large margin (up to +550%). ",
            " Kimi k1.5 TECHNICAL REPORT\nKimi k1.5 short-CoTOpenAI 4oClaude 3.5 Sonnet DeepSeek V3LLaMA-3.1 405B-Inst.\nGeneralCodeMath\n47.333.4\nLiveCodeBench v4 24.08-24.11 (Pass@1-COT)\n36.328.440.531.1Qwen2.5 72B-Inst.\n87.487.2\nMMLU (EM)\n88.388.688.585.3\n94.674.6\nMATH-500 (EM)\n78.373.890.280\n60.8\n9.3AIME 2024 (Pass@1)\n1623.339.223.3\n6869.1\nMMMU_val(Pass@1)\n66.464.5\n70.163.8\nMathVista_test(Pass@1)\n65.369.7\n87.284.3\nIF-Eval (Prompt Strict)\n86.58686.184.1\n91.787.9\nCLUEWSC (EM)\n85.484.790.991.4\n88.376\nC-Eval (EM)\n76.761.586.586.1Qwen2-VLVision\nFigure 2: Kimi k1.5 short-CoT results.\n1 Introduction\nLanguage model pretraining with next token prediction has been studied under the context of the scaling law, where\nproportionally scaling model parameters and data sizes leads to the continued improvement of intelligence. (Kaplan\net al. 2020; Hoffmann et al. 2022) However, this approach is limited to the amount of available high-quality training\ndata (Villalobos et al. 2024; Muennighoff et al. 2023). In this report, we present the training recipe of Kimi k1.5,\nour latest multi-modal LLM trained with reinforcement learning (RL). The goal is to explore a possible new axis\nfor continued scaling. Using RL with LLMs, the models learns to explore with rewards and thus is not limited to a\npre-existing static dataset.\nThere are a few key ingredients about the design and training of k1.5.\n‚Ä¢Long context scaling . We scale the context window of RL to 128k and observe continued improvement of"
        ],
        "response":"Kimi k1.5 introduces techniques for transferring the reasoning capabilities of long-chain-of-thought (CoT) models to short-CoT models, which improves their performance with limited tokens. These \"long2short\" methods use long-CoT techniques to enhance short-CoT models, achieving state-of-the-art results on certain benchmarks. The specific methods used are not explained in the provided context.",
        "reference":"The long2short methods include model merging (averaging weights of long-cot and short-cot models), shortest rejection sampling (selecting the shortest correct response for fine-tuning), DPO (using long CoT model to generate samples for pairwise preference training), and long2short RL (applying length penalty and reducing rollout length after a standard RL training phase).",
        "faithfulness":1.0,
        "answer_relevancy":0.0,
        "context_precision":0.0,
        "answer_correctness":0.4672035079
    },
    {
        "user_input":"What is the training process of Kimi k1.5 in terms of pretraining and vanilla supervised finetuning?",
        "retrieved_contexts":[
            "Could not find the references, answered by LLM"
        ],
        "response":"Kimi K1.5 is trained using a combination of pre-training and vanilla supervised finetuning. Pre-training involves training the model on large amounts of masked language modeling tasks. For the specific task, Kimi was pre-trained on a modified version of the CC-100M dataset which consists of 100 million tokens from various sources including but not limited to Wikipedia, BookCorpus and Reddit. Following pre-training, Kimi is fine-tuned on the task-specific dataset for a few thousand steps using a lower learning rate in vanilla supervised finetuning.",
        "reference":"In pretraining, it is trained on a diverse multimodal corpus in three stages: vision-language pretraining, cooldown, and long-context activation. In vanilla supervised finetuning, a corpus covering multiple domains is created. For non-reasoning tasks, a seed dataset is constructed through human annotation and refined. For reasoning tasks, rejection sampling is used. The model is trained at different sequence lengths with learning rate decay.",
        "faithfulness":0.0,
        "answer_relevancy":0.83271447,
        "context_precision":0.0,
        "answer_correctness":0.3014553446
    },
    {
        "user_input":"What are the main contributions of this research?",
        "retrieved_contexts":[
            " ‚Ä¢Others : DeepSeek-R1 also excels in a wide range of tasks, including creative writing,\ngeneral question answering, editing, summarization, and more. It achieves an impressive\nlength-controlled win-rate of 87.6% on AlpacaEval 2.0 and a win-rate of 92.3% on Are-\nnaHard, showcasing its strong ability to intelligently handle non-exam-oriented queries.\nAdditionally, DeepSeek-R1 demonstrates outstanding performance on tasks requiring\nlong-context understanding, substantially outperforming DeepSeek-V3 on long-context\nbenchmarks.\n2. Approach\n2.1. Overview\nPrevious work has heavily relied on large amounts of supervised data to enhance model\nperformance. In this study, we demonstrate that reasoning capabilities can be significantly\nimproved through large-scale reinforcement learning (RL), even without using supervised\nfine-tuning (SFT) as a cold start. Furthermore, performance can be further enhanced with\nthe inclusion of a small amount of cold-start data. In the following sections, we present: (1)\nDeepSeek-R1-Zero, which applies RL directly to the base model without any SFT data, and\n(2) DeepSeek-R1, which applies RL starting from a checkpoint fine-tuned with thousands of\nlong Chain-of-Thought (CoT) examples. 3) Distill the reasoning capability from DeepSeek-R1 to\nsmall dense models.\n2.2. DeepSeek-R1-Zero: Reinforcement Learning on the Base Model\nReinforcement learning has demonstrated significant effectiveness in reasoning tasks, as ev- ",
            " Beyond basic quality control, we carefully evaluate the educational value of each document through our scoring system.\nDocuments with high pedagogical relevance and knowledge depth are prioritized, while maintaining a balance between\ntheoretical depth and instructional clarity. This helps ensure that our training corpus contains high-quality educational\ncontent that can effectively contribute to the model‚Äôs knowledge acquisition.\nFinally, to optimize the overall composition of our training corpus, the sampling strategy for different document types\nis empirically determined through extensive experimentation. We conduct isolated evaluations to identify document\nsubsets that contribute most significantly to the model‚Äôs knowledge acquisition capabilities. These high-value subsets\nare upsampled in the final training corpus. However, to maintain data diversity and ensure model generalization, we\ncarefully preserve a balanced representation of other document types at appropriate ratios. This data-driven approach\nhelps us optimize the trade-off between focused knowledge acquisition and broad generalization capabilities.\nB.2 Multimodal Data\nOur multi-modal pretraining corpus is designed to provide high-quality data that enables models to process and\nunderstand information from multiple modalities, including text, images, and videos. To this end, we also have curated\nhigh-quality data from five categories‚Äîcaptioning, interleaving, OCR (Optical Character Recognition), knowledge, and ",
            " 1.1. Contributions\nPost-Training: Large-Scale Reinforcement Learning on the Base Model\n‚Ä¢We directly apply reinforcement learning (RL) to the base model without relying on super-\nvised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore\nchain-of-thought (CoT) for solving complex problems, resulting in the development of\nDeepSeek-R1-Zero. DeepSeek-R1-Zero demonstrates capabilities such as self-verification,\nreflection, and generating long CoTs, marking a significant milestone for the research\ncommunity. Notably, it is the first open research to validate that reasoning capabilities of\nLLMs can be incentivized purely through RL, without the need for SFT. This breakthrough\npaves the way for future advancements in this area.\n‚Ä¢We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL\nstages aimed at discovering improved reasoning patterns and aligning with human pref-\nerences, as well as two SFT stages that serve as the seed for the model‚Äôs reasoning and\nnon-reasoning capabilities. We believe the pipeline will benefit the industry by creating\nbetter models.\nDistillation: Smaller Models Can Be Powerful Too\n‚Ä¢We demonstrate that the reasoning patterns of larger models can be distilled into smaller\nmodels, resulting in better performance compared to the reasoning patterns discovered\nthrough RL on small models. The open source DeepSeek-R1, as well as its API, will benefit"
        ],
        "response":"The main contributions of this research include the development of a new approach for large-scale reinforcement learning (RL) and its application to improve model performance without supervised fine-tuning (SFT), and the demonstration that reasoning capabilities can be distilled into smaller models. Additionally, their pipeline has shown potential benefits in creating better models. The research also validates that reasoning capabilities of LLMs can be incentivized purely through RL.",
        "reference":"Directly applied RL to the base model without preliminary SFT, developing DeepSeek-R1-Zero which exhibits self-verification, reflection, etc.Introduced the training pipeline for DeepSeek-R1, involving two RL and two SFT stages.Introduced the training pipeline for DeepSeek-R1, involving two RL and two SFT stages.Demonstrated the distillation of reasoning patterns from larger to smaller models and open - sourced relevant models and API.",
        "faithfulness":1.0,
        "answer_relevancy":1.0,
        "context_precision":0.8333333333,
        "answer_correctness":0.5081909186
    },
    {
        "user_input":"How does DeepSeek-R1-Zero improve reasoning capabilities?",
        "retrieved_contexts":[
            " In a stunning development that's reshaping the AI landscape, DeepSeek, a Chinese AI startup, has recently made waves with their open-source R1 model. Released in January 2025, DeepSeek R1 has garnered attention not just for matching OpenAI's capabilities at a fraction of the cost (90-95% less), but also for being the first to successfully integrate web search into a reasoning-focused language ... ",
            " significant challenge.\n5. Conclusion, Limitation, and Future Work\nIn this work, we share our journey in enhancing model reasoning abilities through reinforcement\nlearning (RL). DeepSeek-R1-Zero represents a pure RL approach without relying on cold-start\ndata, achieving strong performance across various tasks. DeepSeek-R1 is more powerful,\nleveraging cold-start data alongside iterative RL fine-tuning. Ultimately, DeepSeek-R1 achieves\nperformance comparable to OpenAI-o1-1217 on a range of tasks.\nWe further explore distillation the reasoning capability to small dense models. We use\nDeepSeek-R1 as the teacher model to generate 800K data, and fine-tune several small dense\nmodels. The results are promising: DeepSeek-R1-Distill-Qwen-1.5B outperforms GPT-4o and\nClaude-3.5-Sonnet on math benchmarks with 28.9% on AIME and 83.9% on MATH. Other dense\nmodels also achieve impressive results, significantly outperforming other instruction-tuned\nmodels based on the same underlying checkpoints.\nIn the future, we plan to invest in research across the following directions for DeepSeek-R1.\n‚Ä¢General Capability: Currently, the capabilities of DeepSeek-R1 fall short of DeepSeek-\nV3 in tasks such as function calling, multi-turn, complex role-playing, and json output.\nMoving forward, we plan to explore how leveraging long CoT to enhance tasks in these\nfields.\n‚Ä¢Language Mixing: DeepSeek-R1 is currently optimized for Chinese and English, which ",
            " We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it ..."
        ],
        "response":"I don't know. According to the retrieved context, DeepSeek R1-Zero improves reasoning capabilities through reinforcement learning (RL) without relying on cold-start data. This approach allows it to achieve strong performance across various tasks compared to previous models like OpenAI-o1-1217.",
        "reference":"By using Group Relative Policy Optimization (GRPO) as the RL algorithm, a rule - based reward system (accuracy and format rewards), and a simple training template. Through RL training, it shows a self - evolution process with increasing thinking time and emerging sophisticated reasoning behaviors, like reflection and exploring alternative problem - solving approaches.",
        "faithfulness":0.6,
        "answer_relevancy":0.0,
        "context_precision":0.0,
        "answer_correctness":0.3725895405
    },
    {
        "user_input":"What is the difference between DeepSeek-R1-Zero and DeepSeek-R1?",
        "retrieved_contexts":[
            " DeepSeek-R1-Zero: This model is a groundbreaking achievement in AI research. It proves that reinforcement learning alone can teach reasoning without the need for human-guided lessons. This is a ... ",
            " DeepSeek R1-Zero: Reinforcement Learning Without Supervised Fine-Tuning. Real-World Example Input: Input Sentence: \"Explain the process of photosynthesis in plants.\" Expected Output: \"Photosynthesis is the process by which green plants use sunlight to synthesise foods with the help of chlorophyll in their leaves. They convert carbon dioxide from the air and water from the soil into ... ",
            " DeepSeek-V3, which could help developers in real world tasks.\n‚Ä¢Knowledge : On benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek-\nR1 achieves outstanding results, significantly outperforming DeepSeek-V3 with scores\nof 90.8% on MMLU, 84.0% on MMLU-Pro, and 71.5% on GPQA Diamond. While its\nperformance is slightly below that of OpenAI-o1-1217 on these benchmarks, DeepSeek-R1\nsurpasses other closed-source models, demonstrating its competitive edge in educational\ntasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3,\ndemonstrating its capability in handling fact-based queries. A similar trend is observed\nwhere OpenAI-o1 surpasses 4o on this benchmark.\n4"
        ],
        "response":"I don't know the specific difference between DeepSeek-R1-Zero and DeepSeek-R1, but I can tell you that DeepSeek-R1-Zero is a variant of the model that uses reinforcement learning alone to teach reasoning without human guidance. Unlike the original R1, which may require supervised fine-tuning, R1-Zero achieves similar results using only self-supervised methods. The key difference is in their approach to achieving performance on certain benchmarks.",
        "reference":"DeepSeek-R1-Zero applies RL directly to the base model without SFT data, while DeepSeek-R1 uses a small amount of cold - start data to fine - tune the model first. DeepSeek-R1-Zero has issues like poor readability and language mixing, which DeepSeek-R1 addresses through its multi - stage training pipeline.",
        "faithfulness":0.6666666667,
        "answer_relevancy":0.0,
        "context_precision":0.0,
        "answer_correctness":0.5484330941
    },
    {
        "user_input":"What is the cold start in the training of DeepSeek-R1?",
        "retrieved_contexts":[
            " ‚Ä¢Others : DeepSeek-R1 also excels in a wide range of tasks, including creative writing,\ngeneral question answering, editing, summarization, and more. It achieves an impressive\nlength-controlled win-rate of 87.6% on AlpacaEval 2.0 and a win-rate of 92.3% on Are-\nnaHard, showcasing its strong ability to intelligently handle non-exam-oriented queries.\nAdditionally, DeepSeek-R1 demonstrates outstanding performance on tasks requiring\nlong-context understanding, substantially outperforming DeepSeek-V3 on long-context\nbenchmarks.\n2. Approach\n2.1. Overview\nPrevious work has heavily relied on large amounts of supervised data to enhance model\nperformance. In this study, we demonstrate that reasoning capabilities can be significantly\nimproved through large-scale reinforcement learning (RL), even without using supervised\nfine-tuning (SFT) as a cold start. Furthermore, performance can be further enhanced with\nthe inclusion of a small amount of cold-start data. In the following sections, we present: (1)\nDeepSeek-R1-Zero, which applies RL directly to the base model without any SFT data, and\n(2) DeepSeek-R1, which applies RL starting from a checkpoint fine-tuned with thousands of\nlong Chain-of-Thought (CoT) examples. 3) Distill the reasoning capability from DeepSeek-R1 to\nsmall dense models.\n2.2. DeepSeek-R1-Zero: Reinforcement Learning on the Base Model\nReinforcement learning has demonstrated significant effectiveness in reasoning tasks, as ev- ",
            " Figure 3|The average response length of DeepSeek-R1-Zero on the training set during the RL\nprocess. DeepSeek-R1-Zero naturally learns to solve reasoning tasks with more thinking time.\nment throughout the training process. This improvement is not the result of external adjustments\nbut rather an intrinsic development within the model. DeepSeek-R1-Zero naturally acquires the\nability to solve increasingly complex reasoning tasks by leveraging extended test-time compu-\ntation. This computation ranges from generating hundreds to thousands of reasoning tokens,\nallowing the model to explore and refine its thought processes in greater depth.\nOne of the most remarkable aspects of this self-evolution is the emergence of sophisticated\nbehaviors as the test-time computation increases. Behaviors such as reflection‚Äîwhere the model\nrevisits and reevaluates its previous steps‚Äîand the exploration of alternative approaches to\nproblem-solving arise spontaneously. These behaviors are not explicitly programmed but instead\nemerge as a result of the model‚Äôs interaction with the reinforcement learning environment. This\nspontaneous development significantly enhances DeepSeek-R1-Zero‚Äôs reasoning capabilities,\nenabling it to tackle more challenging tasks with greater efficiency and accuracy.\nAha Moment of DeepSeek-R1-Zero A particularly intriguing phenomenon observed during\nthe training of DeepSeek-R1-Zero is the occurrence of an ‚Äúaha moment‚Äù. This moment, as ",
            " To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense ..."
        ],
        "response":"I don't know what the cold start in the training of DeepSeek-R1 is. It's described as using a small amount of data during the initial stages of the model's performance enhancement, but more information or context would be needed for a precise explanation. \n\nThis model utilizes reinforcement learning (RL) during its main development process.",
        "reference":"It constructs and collects a small amount of long Chain-of-Thought (CoT) data to fine - tune the DeepSeek-V3-Base as the initial RL actor. The cold - start data is designed to be more readable and shows better potential compared to DeepSeek-R1-Zero. Data collection methods include few - shot prompting, direct prompting for detailed answers, gathering DeepSeek-R1-Zero outputs, and post - processing by human annotators.",
        "faithfulness":0.6,
        "answer_relevancy":0.0,
        "context_precision":0.9999999999,
        "answer_correctness":0.3025640067
    },
    {
        "user_input":"How is the reward modeled in DeepSeek-R1-Zero?",
        "retrieved_contexts":[
            " The reward function assigns scores 8 and 6 to o1 and o2 respectively. The average reward (R) becomes (8+6)\/2 = 7. ... While training DeepSeek-R1-Zero, a rule based reward system is employed, which ... ",
            " ‚Ä¢Others : DeepSeek-R1 also excels in a wide range of tasks, including creative writing,\ngeneral question answering, editing, summarization, and more. It achieves an impressive\nlength-controlled win-rate of 87.6% on AlpacaEval 2.0 and a win-rate of 92.3% on Are-\nnaHard, showcasing its strong ability to intelligently handle non-exam-oriented queries.\nAdditionally, DeepSeek-R1 demonstrates outstanding performance on tasks requiring\nlong-context understanding, substantially outperforming DeepSeek-V3 on long-context\nbenchmarks.\n2. Approach\n2.1. Overview\nPrevious work has heavily relied on large amounts of supervised data to enhance model\nperformance. In this study, we demonstrate that reasoning capabilities can be significantly\nimproved through large-scale reinforcement learning (RL), even without using supervised\nfine-tuning (SFT) as a cold start. Furthermore, performance can be further enhanced with\nthe inclusion of a small amount of cold-start data. In the following sections, we present: (1)\nDeepSeek-R1-Zero, which applies RL directly to the base model without any SFT data, and\n(2) DeepSeek-R1, which applies RL starting from a checkpoint fine-tuned with thousands of\nlong Chain-of-Thought (CoT) examples. 3) Distill the reasoning capability from DeepSeek-R1 to\nsmall dense models.\n2.2. DeepSeek-R1-Zero: Reinforcement Learning on the Base Model\nReinforcement learning has demonstrated significant effectiveness in reasoning tasks, as ev- ",
            " tags.\nWe do not apply the outcome or process neural reward model in developing DeepSeek-R1-Zero,\nbecause we find that the neural reward model may suffer from reward hacking in the large-scale\nreinforcement learning process, and retraining the reward model needs additional training\nresources and it complicates the whole training pipeline.\n2.2.3. Training Template\nTo train DeepSeek-R1-Zero, we begin by designing a straightforward template that guides\nthe base model to adhere to our specified instructions. As depicted in Table 1, this template\nrequires DeepSeek-R1-Zero to first produce a reasoning process, followed by the final answer.\nWe intentionally limit our constraints to this structural format, avoiding any content-specific\nbiases‚Äîsuch as mandating reflective reasoning or promoting particular problem-solving strate-\ngies‚Äîto ensure that we can accurately observe the model‚Äôs natural progression during the\nreinforcement learning (RL) process.\n2.2.4. Performance, Self-evolution Process and Aha Moment of DeepSeek-R1-Zero\nPerformance of DeepSeek-R1-Zero Figure 2 depicts the performance trajectory of DeepSeek-\nR1-Zero on the AIME 2024 benchmark throughout the reinforcement learning (RL) training\nprocess. As illustrated, DeepSeek-R1-Zero demonstrates a steady and consistent enhancement\nin performance as the RL training advances. Notably, the average pass@1 score on AIME 2024\nshows a significant increase, jumping from an initial 15.6% to an impressive 71.0%, reaching"
        ],
        "response":"The reward function in DeepSeek-R1-Zero assigns scores of 8 and 6 to o1 and o2 respectively, resulting in an average reward of (8+6)\/2 = 7.",
        "reference":"It uses a rule - based reward system. The accuracy reward model evaluates whether the response is correct, for example, verifying math problem answers in a specified format or using a compiler for LeetCode problems. The format reward model enforces the model to put its thinking process between ‚Äò<think>‚Äô and ‚Äò<\/think>‚Äô tags. Neural reward models are not used due to potential reward hacking and high resource requirements.",
        "faithfulness":1.0,
        "answer_relevancy":0.8823117408,
        "context_precision":0.8333333333,
        "answer_correctness":0.1753373846
    },
    {
        "user_input":"What benchmarks are used to evaluate the models?",
        "retrieved_contexts":[
            " LLM Leaderboard - Comparison of GPT-4o, Llama 3, Mistral, Gemini and over 30 models . Comparison and ranking the performance of over 30 AI models (LLMs) across key metrics including quality, price, performance and speed (output speed - tokens per second & latency - TTFT), context window & others. ",
            " In spite of being repeatable and efficient, offline evaluation approaches cannot directly take the perception of users to the systems into consideration (Moffat et al., 2012).To address this issue, recently, substantial studies have focused on proposing effective evaluation metrics that model user behavior to bring human factors in the loop of Web search evaluation. ",
            " Note üèÜ This leaderboard is based on the following three benchmarks: Chatbot Arena - a crowdsourced, randomized battle platform. We use 70K+ user votes to compute Elo ratings. MT-Bench - a set of challenging multi-turn questions. We use GPT-4 to grade the model responses. MMLU (5-shot) - a test to measure a model's multitask accuracy on 57 ..."
        ],
        "response":"The benchmarks used to evaluate the models are:\n\n1. Chatbot Arena\n2. MT-Bench\n3. MMLU (5-shot)\n\nI don't know the details of each benchmark, but these three are mentioned as being used in The Big Benchmarks Collection.",
        "reference":"MMLU, MMLU - Redux, MMLU - Pro, C - Eval, CMMLU, IFEval, FRAMES, GPQA Diamond, SimpleQA, C - SimpleQA, SWE - Bench Verified, Aider 1, LiveCodeBench, Codeforces, Chinese National High School Mathematics Olympiad (CNMO 2024), American Invitational Mathematics Examination 2024 (AIME 2024), and open - ended generation tasks like AlpacaEval 2.0 and Arena - Hard.",
        "faithfulness":0.6666666667,
        "answer_relevancy":0.0,
        "context_precision":0.0,
        "answer_correctness":0.2349513354
    },
    {
        "user_input":"What are the baselines for model evaluation?",
        "retrieved_contexts":[
            " In spite of being repeatable and efficient, offline evaluation approaches cannot directly take the perception of users to the systems into consideration (Moffat et al., 2012).To address this issue, recently, substantial studies have focused on proposing effective evaluation metrics that model user behavior to bring human factors in the loop of Web search evaluation. ",
            " A Reference-Dependent Model for Web Search Evaluation WWW '23, May 1-5, 2023, Austin, TX, USA. ... inative power of ReDeMs and baselines on the NTCIR-WWW3. dataset [42]. 4.1 Datasets. The ... ",
            " 2.Search based on a few epochs of training. (Could be bolstered with short learning curves for different settings.) 3.Search based on subsets of the data. (However, some parameters will be very dependent on dataset size, so this can be risky.) 4.Via heuristic search, determine which hyperparameters matter less, and set them by hand."
        ],
        "response":"I don't know the specific baselines for model evaluation that bring human factors into consideration. The context suggests that researchers have focused on proposing new evaluation metrics that model user behavior, but it doesn't provide a clear answer to this question. More research is needed to identify effective baselines for model evaluation.",
        "reference":"DeepSeek-V3, Claude - Sonnet - 3.5 - 1022, GPT - 4o - 0513, OpenAI - o1 - mini, OpenAI - o1 - 1217. For distilled models, QwQ - 32B - Preview is also used as a baseline.",
        "faithfulness":0.5,
        "answer_relevancy":0.0,
        "context_precision":0.0,
        "answer_correctness":0.1306296679
    },
    {
        "user_input":"What is the performance of DeepSeek-R1 on math tasks?",
        "retrieved_contexts":[
            " On coding-related tasks, DeepSeek-R1 demonstrates expert level in code competition tasks, as it achieves 2,029 Elo rating on Codeforces outperforming 96.3% human participants in the competition. For engineering-related tasks, DeepSeek-R1 performs slightly better than DeepSeek-V3, which could help developers in real world tasks. ",
            " Benchmark (Metric)Claude-3.5- GPT-4o DeepSeek OpenAI OpenAI DeepSeek\nSonnet-1022 0513 V3 o1-mini o1-1217 R1\nArchitecture - - MoE - - MoE\n# Activated Params - - 37B - - 37B\n# Total Params - - 671B - - 671B\nEnglishMMLU (Pass@1) 88.3 87.2 88.5 85.2 91.8 90.8\nMMLU-Redux (EM) 88.9 88.0 89.1 86.7 - 92.9\nMMLU-Pro (EM) 78.0 72.6 75.9 80.3 - 84.0\nDROP (3-shot F1) 88.3 83.7 91.6 83.9 90.2 92.2\nIF-Eval (Prompt Strict) 86.5 84.3 86.1 84.8 - 83.3\nGPQA Diamond (Pass@1) 65.0 49.9 59.1 60.0 75.7 71.5\nSimpleQA (Correct) 28.4 38.2 24.9 7.0 47.0 30.1\nFRAMES (Acc.) 72.5 80.5 73.3 76.9 - 82.5\nAlpacaEval2.0 (LC-winrate) 52.0 51.1 70.0 57.8 - 87.6\nArenaHard (GPT-4-1106) 85.2 80.4 85.5 92.0 - 92.3\nCodeLiveCodeBench (Pass@1-COT) 38.9 32.9 36.2 53.8 63.4 65.9\nCodeforces (Percentile) 20.3 23.6 58.7 93.4 96.6 96.3\nCodeforces (Rating) 717 759 1134 1820 2061 2029\nSWE Verified (Resolved) 50.8 38.8 42.0 41.6 48.9 49.2\nAider-Polyglot (Acc.) 45.3 16.0 49.6 32.9 61.7 53.3\nMathAIME 2024 (Pass@1) 16.0 9.3 39.2 63.6 79.2 79.8\nMATH-500 (Pass@1) 78.3 74.6 90.2 90.0 96.4 97.3\nCNMO 2024 (Pass@1) 13.1 10.8 43.2 67.6 - 78.8\nChineseCLUEWSC (EM) 85.4 87.9 90.9 89.9 - 92.8\nC-Eval (EM) 76.7 76.0 86.5 68.9 - 91.8\nC-SimpleQA (Correct) 55.4 58.7 68.0 40.3 - 63.7\nTable 4|Comparison between DeepSeek-R1 and other representative models.\nsearch and data analysis tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms\nDeepSeek-V3, demonstrating its capability in handling fact-based queries. A similar trend ",
            " DeepSeek-R1 is an open-source reasoning model developed by DeepSeek, a Chinese AI company, to address tasks requiring logical inference, mathematical problem-solving, and real-time decision-making. What sets reasoning models like DeepSeek-R1 and OpenAI's o1 apart from traditional language models is their ability to show how they arrived at a ..."
        ],
        "response":"I don't know the performance of DeepSeek-R1 on math tasks specifically. The provided context only discusses its performance on various other benchmarks, but does mention that it performs well in dealing with fact-based queries and demonstrating capability in handling math-related questions, such as MathAIME 2024 and MATH-500.",
        "reference":"On AIME 2024, it has a Pass@1 score of 79.8%, slightly surpassing OpenAI - o1 - 1217. On MATH - 500, it attains a score of 97.3%, performing on par with OpenAI - o1 - 1217 and significantly outperforming other models.",
        "faithfulness":0.8,
        "answer_relevancy":0.0,
        "context_precision":0.5,
        "answer_correctness":0.4041568937
    },
    {
        "user_input":"Why is distillation of reasoning capabilities important?",
        "retrieved_contexts":[
            " After generating diverse CoTs and their corresponding self-evaluation outputs using the LLM, the process begin to train the SLM. The training methodology for SLMs first emphasises distilling self-evaluation capability to lay the foundation for reducing the impact of errors in CoTs on SLMs, followed by incorporating comprehensive reasoning capability through diverse CoTs distillation. ",
            " Large language models (LLMs) excel in complex reasoning tasks, and distilling their reasoning capabilities into smaller models has shown promise. However, we uncover an interesting phenomenon, which we term the Small Model Learnability Gap: small models ($\\\\leq$3B parameters) do not consistently benefit from long chain-of-thought (CoT) reasoning or distillation from larger models. Instead ... ",
            " RL training, achieves performance on par with QwQ-32B-Preview. However, DeepSeek-R1-\nDistill-Qwen-32B, which is distilled from DeepSeek-R1, performs significantly better than\nDeepSeek-R1-Zero-Qwen-32B across all benchmarks. Therefore, we can draw two conclusions:\nFirst, distilling more powerful models into smaller ones yields excellent results, whereas smaller\nmodels relying on the large-scale RL mentioned in this paper require enormous computational\npower and may not even achieve the performance of distillation. Second, while distillation\nstrategies are both economical and effective, advancing beyond the boundaries of intelligence\nmay still require more powerful base models and larger-scale reinforcement learning.\n4.2. Unsuccessful Attempts\nIn the early stages of developing DeepSeek-R1, we also encountered failures and setbacks along\nthe way. We share our failure experiences here to provide insights, but this does not imply that\nthese approaches are incapable of developing effective reasoning models.\nProcess Reward Model (PRM) PRM is a reasonable method to guide the model toward better\napproaches for solving reasoning tasks (Lightman et al., 2023; Uesato et al., 2022; Wang et al.,\n2023). However, in practice, PRM has three main limitations that may hinder its ultimate suc-\ncess. First, it is challenging to explicitly define a fine-grain step in general reasoning. Second,\ndetermining whether the current intermediate step is correct is a challenging task. Automated"
        ],
        "response":"Distillation of reasoning capabilities is important because it allows for the creation of smaller models that can perform complex tasks similar to larger models, making them more efficient and effective. This is particularly useful in applications where computational power is limited or resources are scarce. Effective distillation can help reduce errors and improve overall performance.",
        "reference":"It can empower smaller models with reasoning capabilities. Distilling from more powerful models like DeepSeek-R1 into smaller models can achieve better performance than RL training on small models alone. For example, DeepSeek-R1 - Distill - Qwen - 7B outperforms non - reasoning models like GPT - 4o - 0513 across the board.",
        "faithfulness":1.0,
        "answer_relevancy":0.8985097875,
        "context_precision":0.5833333333,
        "answer_correctness":0.506680849
    }
]